"""
CLI-based agent implementations that wrap external AI tools.

These agents invoke CLI tools (codex, claude, openai) as subprocesses,
enabling heterogeneous multi-model debates.

Supports automatic fallback to OpenRouter API when CLI commands fail due to
rate limits, timeouts, or other errors. Enable fallback by setting
enable_fallback=True (default) and providing OPENROUTER_API_KEY.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import re
from typing import TYPE_CHECKING, Any, Callable

from aragora.agents.base import MAX_CONTEXT_CHARS, MAX_MESSAGE_CHARS, CritiqueMixin
from aragora.agents.errors import (
    RATE_LIMIT_PATTERNS,
    AgentCircuitOpenError,
    CLISubprocessError,
    ErrorClassifier,
)
from aragora.agents.registry import AgentRegistry
from aragora.core import Agent, Critique, Message
from aragora.resilience import CircuitBreaker, get_circuit_breaker

if TYPE_CHECKING:
    from aragora.agents.api_agents import OpenRouterAgent

# Module-level semaphore to limit concurrent CLI subprocesses
# Prevents OS resource exhaustion when running many agents in parallel
# Configurable via ARAGORA_MAX_CLI_SUBPROCESSES environment variable
_MAX_CLI_SUBPROCESSES = int(os.environ.get("ARAGORA_MAX_CLI_SUBPROCESSES", "10"))
_subprocess_semaphore = asyncio.Semaphore(_MAX_CLI_SUBPROCESSES)

# Maximum prompt size to pass as CLI argument (avoids E2BIG error)
# Prompts larger than this should be passed via stdin where supported
# Configurable via ARAGORA_MAX_CLI_PROMPT_CHARS environment variable
MAX_CLI_PROMPT_CHARS = int(os.environ.get("ARAGORA_MAX_CLI_PROMPT_CHARS", "100000"))  # 100KB default

# Re-export constants for backward compatibility
__all__ = [
    "CLIAgent",
    "CodexAgent",
    "ClaudeAgent",
    "OpenAIAgent",
    "GeminiCLIAgent",
    "GrokCLIAgent",
    "QwenCLIAgent",
    "DeepseekCLIAgent",
    "KiloCodeAgent",
    "MAX_CLI_PROMPT_CHARS",
    "MAX_CONTEXT_CHARS",
    "MAX_MESSAGE_CHARS",
    "RATE_LIMIT_PATTERNS",  # Re-exported from errors.py
]

logger = logging.getLogger(__name__)


class CLIAgent(CritiqueMixin, Agent):
    """Base class for CLI-based agents.

    Supports automatic fallback to OpenRouter API when CLI commands fail.
    Enable with enable_fallback=True (default) and OPENROUTER_API_KEY env var.
    """

    # Map CLI agent models to OpenRouter model identifiers
    OPENROUTER_MODEL_MAP: dict[str, str] = {
        # Claude models
        "claude": "anthropic/claude-sonnet-4",  # Default claude CLI
        "claude-opus-4-5-20251101": "anthropic/claude-opus-4",
        "claude-sonnet-4-20250514": "anthropic/claude-sonnet-4",
        "claude-3-opus-20240229": "anthropic/claude-3-opus",
        "claude-3-sonnet-20240229": "anthropic/claude-3-sonnet",
        # OpenAI/Codex models
        "gpt-5.2-codex": "openai/gpt-4o",
        "gpt-4o": "openai/gpt-4o",
        "gpt-4-turbo": "openai/gpt-4-turbo",
        "gpt-4": "openai/gpt-4",
        # Gemini models
        "gemini-3-pro-preview": "google/gemini-2.0-flash-001",
        "gemini-3-pro": "google/gemini-2.0-flash-001",
        "gemini-2.0-flash": "google/gemini-2.0-flash-001",
        "gemini-1.5-pro": "google/gemini-pro-1.5",
        # Grok models
        "grok-4": "x-ai/grok-2-1212",  # Grok 4 -> latest available on OpenRouter
        "grok-3": "x-ai/grok-2-1212",
        "grok-2": "x-ai/grok-2-1212",
        # Deepseek models
        "deepseek-coder": "deepseek/deepseek-chat",
        "deepseek-v3": "deepseek/deepseek-chat",
        # Qwen models
        "qwen-2.5-coder": "qwen/qwen-2.5-coder-32b-instruct",
    }

    def __init__(
        self,
        name: str,
        model: str,
        role: str = "proposer",
        timeout: int = 300,  # Increased default for complex operations
        enable_fallback: bool = True,
        circuit_breaker: CircuitBreaker | None = None,
        enable_circuit_breaker: bool = True,
        prefer_api: bool = False,  # Skip CLI, use OpenRouter directly
    ):
        super().__init__(name, model, role)
        self.timeout = timeout
        self.enable_fallback = enable_fallback
        self.prefer_api = prefer_api
        self._fallback_agent: OpenRouterAgent | None = None
        self._fallback_used = False  # Track if fallback was triggered this session
        self.enable_circuit_breaker = enable_circuit_breaker

        # Use provided circuit breaker, global registry, or disable
        # Global registry ensures consistent state across agent instances
        if circuit_breaker is not None:
            self._circuit_breaker = circuit_breaker
        elif enable_circuit_breaker:
            # Use global registry with agent name for shared state
            self._circuit_breaker = get_circuit_breaker(
                f"cli_{name}",
                failure_threshold=15,  # High threshold for CLI flakiness
                cooldown_seconds=120.0,  # Longer cooldown for active debates
            )
        else:
            self._circuit_breaker = None

    @property
    def circuit_breaker(self) -> CircuitBreaker | None:
        """Get the circuit breaker for this agent."""
        return self._circuit_breaker

    def is_circuit_open(self) -> bool:
        """Check if the circuit breaker is open (blocking requests)."""
        if self._circuit_breaker is None:
            return False
        return not self._circuit_breaker.can_proceed()

    def _get_fallback_agent(self) -> OpenRouterAgent | None:
        """Get or create the OpenRouter fallback agent.

        Returns None if fallback is disabled or OPENROUTER_API_KEY is not set.
        """
        if not self.enable_fallback:
            return None

        if self._fallback_agent is None:
            api_key = os.environ.get("OPENROUTER_API_KEY")
            if not api_key:
                logger.warning(
                    f"[{self.name}] No OPENROUTER_API_KEY set, fallback disabled - rate limit errors will not have a fallback"
                )
                return None

            # Import here to avoid circular dependency
            from aragora.agents.api_agents import OpenRouterAgent

            # Map the model to OpenRouter format
            openrouter_model = self.OPENROUTER_MODEL_MAP.get(
                self.model,
                "anthropic/claude-sonnet-4",  # Default fallback model
            )

            self._fallback_agent = OpenRouterAgent(
                name=f"{self.name}_fallback",
                model=openrouter_model,
                role=self.role,
                timeout=self.timeout,
            )
            # Copy system prompt if set
            if self.system_prompt:
                self._fallback_agent.system_prompt = self.system_prompt
            logger.info(
                f"[{self.name}] Created OpenRouter fallback agent with model {openrouter_model}"
            )

        return self._fallback_agent

    def _is_fallback_error(self, error: Exception) -> bool:
        """Check if the error should trigger a fallback to OpenRouter.

        Uses centralized ErrorClassifier for consistent error classification
        across all agent types. Detects rate limits, timeouts, CLI-specific
        errors, and network issues.

        This method is intentionally permissive to maximize fallback opportunities.
        """
        should_fallback, category = ErrorClassifier.classify_error(error)
        if should_fallback:
            logger.debug(f"[{self.name}] Detected fallback error ({category}): {str(error)[:100]}")
        return should_fallback

    def _sanitize_cli_arg(self, arg: str) -> str:
        """Sanitize a string for use as a CLI argument.

        Removes null bytes and other control characters that can cause
        'embedded null byte' ValueError from subprocess calls.
        Command-line arguments are null-terminated in C, so null bytes
        in arguments are not allowed by the OS.
        """
        if not isinstance(arg, str):
            return str(arg)
        # Remove null bytes (cause 'embedded null byte' error)
        sanitized = arg.replace("\x00", "")
        # Remove other problematic control characters (except newlines/tabs)
        sanitized = re.sub(r"[\x01-\x08\x0b-\x0c\x0e-\x1f\x7f]", "", sanitized)
        return sanitized

    async def _run_cli(self, command: list[str], input_text: str | None = None) -> str:
        """Run a CLI command and return output.

        Integrates with circuit breaker to prevent cascading failures.
        Uses a module-level semaphore to limit concurrent subprocesses.
        """
        # Check circuit breaker before attempting the call
        if self._circuit_breaker is not None and not self._circuit_breaker.can_proceed():
            raise AgentCircuitOpenError(
                "Circuit breaker is open for CLI agent",
                agent_name=self.name,
                cooldown_seconds=self._circuit_breaker.cooldown_seconds,
            )

        # Sanitize all command arguments to prevent 'embedded null byte' errors
        sanitized_command = [self._sanitize_cli_arg(arg) for arg in command]
        proc = None

        # Use semaphore to limit concurrent subprocesses (prevents resource exhaustion)
        async with _subprocess_semaphore:
            try:
                proc = await asyncio.create_subprocess_exec(
                    *sanitized_command,
                    stdin=asyncio.subprocess.PIPE if input_text else None,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )

                # Also sanitize stdin input (used by ClaudeAgent)
                sanitized_input = self._sanitize_cli_arg(input_text) if input_text else None
                stdout, stderr = await asyncio.wait_for(
                    proc.communicate(input=sanitized_input.encode() if sanitized_input else None),
                    timeout=self.timeout,
                )

                if proc.returncode != 0:
                    # Record failure to circuit breaker
                    if self._circuit_breaker is not None:
                        self._circuit_breaker.record_failure()
                    # Build informative error message with return code
                    stderr_text = stderr.decode("utf-8", errors="replace").strip()
                    error_msg = f"CLI command failed with return code {proc.returncode}"
                    if stderr_text:
                        # For verbose CLIs (like Codex), extract last meaningful line
                        lines = [line.strip() for line in stderr_text.split("\n") if line.strip()]
                        if lines:
                            last_line = lines[-1][:200]
                            error_msg += f": {last_line}"
                    else:
                        error_msg += " (stderr empty)"
                    raise CLISubprocessError(
                        message=error_msg,
                        agent_name=self.name,
                        returncode=proc.returncode,
                        stderr=stderr_text or None,
                    )

                # Record success to circuit breaker
                if self._circuit_breaker is not None:
                    self._circuit_breaker.record_success()

                return stdout.decode("utf-8", errors="replace").strip()

            except asyncio.TimeoutError:
                # Record failure to circuit breaker
                if self._circuit_breaker is not None:
                    self._circuit_breaker.record_failure()
                if proc:
                    proc.kill()
                    await proc.wait()  # Ensure process is fully cleaned up
                raise TimeoutError(f"CLI command timed out after {self.timeout}s")
            except AgentCircuitOpenError:
                # Don't record circuit open errors as failures - just re-raise
                raise
            except (OSError, ValueError, RuntimeError, UnicodeDecodeError) as e:
                # Record failure to circuit breaker (subprocess/encoding errors)
                if self._circuit_breaker is not None:
                    self._circuit_breaker.record_failure()
                if proc and proc.returncode is None:
                    logger.debug(f"[cleanup] Killing subprocess after error: {e}")
                    proc.kill()
                    await proc.wait()  # Cleanup zombie processes
                raise

    def _build_context_prompt(
        self,
        context: list[Message] | None = None,
        truncate: bool = True,
        sanitize_fn: Callable[[str], str] | None = None,
    ) -> str:
        """Build context from previous messages with truncation for large contexts.

        Delegates to CritiqueMixin with CLI-specific settings.
        """
        # Use CLI sanitization by default if not specified
        if sanitize_fn is None:
            sanitize_fn = self._sanitize_cli_arg
        # Use mixin method with truncation and CLI sanitization
        return CritiqueMixin._build_context_prompt(
            self, context, truncate=truncate, sanitize_fn=sanitize_fn
        )

    # _parse_critique is inherited from CritiqueMixin

    def _build_full_prompt(self, prompt: str, context: list[Message] | None = None) -> str:
        """Build full prompt with context and system prompt.

        Consolidates the repeated pattern across all CLI agents.
        """
        full_prompt = prompt
        if context:
            full_prompt = self._build_context_prompt(context) + prompt
        if self.system_prompt:
            full_prompt = f"System context: {self.system_prompt}\n\n{full_prompt}"
        return full_prompt

    def _is_prompt_too_large_for_argv(self, prompt: str) -> bool:
        """Check if prompt is too large to pass as CLI argument.

        Large prompts can trigger E2BIG (argument list too long) errors.
        Returns True if prompt should be passed via stdin instead.
        """
        return len(prompt) > MAX_CLI_PROMPT_CHARS

    async def _generate_with_fallback(
        self,
        cli_command: list[str],
        prompt: str,
        context: list[Message] | None = None,
        input_text: str | None = None,
        response_extractor: Callable[[str], str] | None = None,
    ) -> str:
        """Execute CLI command with automatic fallback on errors.

        Consolidates the repeated try/except fallback pattern across all CLI agents.

        Args:
            cli_command: CLI command to execute
            prompt: Original prompt (for fallback)
            context: Message context (for fallback)
            input_text: Optional stdin input for CLI
            response_extractor: Optional function to extract response from CLI output

        Returns:
            Generated response string
        """
        # If prefer_api is set, skip CLI entirely and use OpenRouter directly
        if self.prefer_api:
            fallback = self._get_fallback_agent()
            if fallback:
                logger.debug(f"[{self.name}] prefer_api=True, using OpenRouter directly")
                self._fallback_used = True
                return await fallback.generate(prompt, context)
            # If no fallback available, fall through to CLI

        try:
            result = await self._run_cli(cli_command, input_text=input_text)
            if response_extractor:
                return response_extractor(result)
            return result

        except Exception as e:
            if self._is_fallback_error(e):
                fallback = self._get_fallback_agent()
                if fallback:
                    logger.warning(
                        f"[{self.name}] CLI failed ({type(e).__name__}: {str(e)[:100]}), "
                        f"falling back to OpenRouter"
                    )
                    self._fallback_used = True
                    try:
                        result = await fallback.generate(prompt, context)
                        # Record success when fallback works - prevents circuit from opening
                        if self._circuit_breaker is not None:
                            self._circuit_breaker.record_success()
                        return result
                    except Exception as fallback_error:
                        # Fallback also failed - record as failure
                        if self._circuit_breaker is not None:
                            self._circuit_breaker.record_failure()
                        raise fallback_error
            raise

    def _build_critique_prompt(self, proposal: str, task: str) -> str:
        """Build standard critique prompt.

        Subclasses can override for custom critique prompts.
        """
        return f"""Analyze this proposal critically for the given task.

Task: {task}

Proposal:
{proposal}

Provide structured feedback:
- ISSUES: Specific problems (bullet points)
- SUGGESTIONS: Improvements (bullet points)
- SEVERITY: 0-10 rating (0=trivial, 10=critical)
- REASONING: Brief explanation"""

    async def critique(
        self,
        proposal: str,
        task: str,
        context: list[Message] | None = None,
        target_agent: str | None = None,
    ) -> Critique:
        """Critique a proposal using this CLI agent.

        Default implementation uses _build_critique_prompt and generate.
        Subclasses can override for custom critique behavior.

        Args:
            proposal: The proposal content to critique
            task: The debate task/question
            context: Optional conversation context
            target_agent: Name of the agent whose proposal is being critiqued
        """
        critique_prompt = self._build_critique_prompt(proposal, task)
        response = await self.generate(critique_prompt, context)
        # Use target_agent if provided, otherwise fall back to generic "proposal"
        return self._parse_critique(response, target_agent or "proposal", proposal)


@AgentRegistry.register(
    "codex",
    default_model="gpt-5.2-codex",
    agent_type="CLI",
    requires="codex CLI (npm install -g @openai/codex)",
)
class CodexAgent(CLIAgent):
    """Agent that uses OpenAI Codex CLI.

    Falls back to OpenRouter (OpenAI GPT-4o) on CLI failures if enabled.
    """

    def _extract_codex_response(self, result: str) -> str:
        """Extract the actual response from codex output (skip header)."""
        lines = result.split("\n")
        response_lines = []
        in_response = False
        for line in lines:
            if line.strip() == "codex":
                in_response = True
                continue
            if in_response:
                if line.startswith("tokens used"):
                    continue
                response_lines.append(line)
        return "\n".join(response_lines).strip() if response_lines else result

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using codex exec.

        For large prompts (>10KB), uses stdin to avoid OS E2BIG error
        (argument list too long).
        """
        full_prompt = self._build_full_prompt(prompt, context)
        # Use stdin for large prompts to avoid E2BIG (arg list too long)
        if self._is_prompt_too_large_for_argv(full_prompt):
            return await self._generate_with_fallback(
                ["codex", "exec", "--skip-git-repo-check", "-"],
                prompt,
                context,
                input_text=full_prompt,
                response_extractor=self._extract_codex_response,
            )
        return await self._generate_with_fallback(
            ["codex", "exec", "--skip-git-repo-check", full_prompt],
            prompt,
            context,
            response_extractor=self._extract_codex_response,
        )

    def _build_critique_prompt(self, proposal: str, task: str) -> str:
        """Build critique prompt with codex-specific formatting."""
        return f"""You are a critical reviewer. Analyze this proposal for the given task.

Task: {task}

Proposal to critique:
{proposal}

Provide a structured critique with:
1. ISSUES: List specific problems, errors, or weaknesses (use bullet points)
2. SUGGESTIONS: List concrete improvements (use bullet points)
3. SEVERITY: Rate 0-10 (0=trivial, 10=critical)
4. REASONING: Brief explanation of your assessment

Be constructive but thorough. Identify both technical and conceptual issues."""


@AgentRegistry.register(
    "claude",
    default_model="claude-sonnet-4",
    agent_type="CLI",
    requires="claude CLI (npm install -g @anthropic-ai/claude-code)",
)
class ClaudeAgent(CLIAgent):
    """Agent that uses Claude CLI (claude-code).

    Falls back to OpenRouter (Anthropic Claude) on CLI failures if enabled.
    """

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using claude CLI via stdin."""
        full_prompt = self._build_full_prompt(prompt, context)
        # Pass prompt via stdin to avoid shell argument length limits
        return await self._generate_with_fallback(
            ["claude", "--print", "-p", "-"],
            prompt,
            context,
            input_text=full_prompt,
        )


@AgentRegistry.register(
    "gemini-cli",
    default_model="gemini-3-pro-preview",
    agent_type="CLI",
    requires="gemini CLI (npm install -g @google/gemini-cli)",
)
class GeminiCLIAgent(CLIAgent):
    """Agent that uses Google Gemini CLI (v0.22+).

    Falls back to OpenRouter (Google Gemini) on CLI failures if enabled.
    """

    def _extract_gemini_response(self, result: str) -> str:
        """Filter out YOLO mode message from gemini output."""
        lines = result.split("\n")
        filtered = [line for line in lines if not line.startswith("YOLO mode is enabled")]
        return "\n".join(filtered).strip()

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using gemini CLI.

        For large prompts (>100KB), uses stdin to avoid OS E2BIG error.
        """
        full_prompt = self._build_full_prompt(prompt, context)
        # Use stdin for large prompts to avoid E2BIG (arg list too long)
        if self._is_prompt_too_large_for_argv(full_prompt):
            logger.debug(f"[{self.name}] Using stdin for large prompt ({len(full_prompt)} chars)")
            return await self._generate_with_fallback(
                ["gemini", "--yolo", "-o", "text", "-"],
                prompt,
                context,
                input_text=full_prompt,
                response_extractor=self._extract_gemini_response,
            )
        return await self._generate_with_fallback(
            ["gemini", "--yolo", "-o", "text", full_prompt],
            prompt,
            context,
            response_extractor=self._extract_gemini_response,
        )


@AgentRegistry.register(
    "kilocode",
    default_model=None,
    default_name="kilocode",
    agent_type="CLI",
    requires="kilocode CLI",
)
class KiloCodeAgent(CLIAgent):
    """Agent that uses Kilo Code CLI for codebase exploration.

    Kilo Code is an agentic coding assistant that can explore codebases
    autonomously. It supports multiple AI providers including Gemini and Grok
    via direct API or OpenRouter.

    Provider IDs (configured in ~/.kilocode/cli/config.json):
    - gemini-explorer: Gemini 3 Pro via direct API
    - grok-explorer: Grok via xAI API
    - openrouter-gemini: Gemini via OpenRouter
    - openrouter-grok: Grok via OpenRouter
    """

    def __init__(
        self,
        name: str,
        provider_id: str = "gemini-explorer",
        model: str | None = None,
        role: str = "proposer",
        timeout: int = 600,
        mode: str = "architect",
    ):
        super().__init__(name, model or provider_id, role, timeout)
        self.provider_id = provider_id
        self.mode = mode  # architect, code, ask, debug

    def _extract_kilocode_response(self, output: str) -> str:
        """Extract the assistant response from Kilo Code JSON output."""
        lines = output.strip().split("\n")
        responses = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            try:
                msg = json.loads(line)
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if content:
                        responses.append(content)
                elif msg.get("type") == "text":
                    text = msg.get("text", "")
                    if text:
                        responses.append(text)
            except json.JSONDecodeError:
                continue
        return "\n\n".join(responses) if responses else output

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using kilocode CLI with codebase access."""
        full_prompt = self._build_full_prompt(prompt, context)
        cmd = [
            "kilocode",
            "--auto",
            "--yolo",
            "--json",
            "-pv",
            self.provider_id,
            "-m",
            self.mode,
            "-t",
            str(self.timeout),
            full_prompt,
        ]
        return await self._generate_with_fallback(
            cmd,
            prompt,
            context,
            response_extractor=self._extract_kilocode_response,
        )


@AgentRegistry.register(
    "grok-cli",
    default_model="grok-4",
    agent_type="CLI",
    requires="grok CLI (npm install -g grok-cli)",
)
class GrokCLIAgent(CLIAgent):
    """Agent that uses xAI Grok CLI.

    Falls back to OpenRouter (xAI Grok) on CLI failures if enabled.
    """

    def _extract_grok_response(self, output: str) -> str:
        """Extract the final assistant response from Grok CLI JSON output."""
        lines = output.strip().split("\n")
        final_content = None
        for line in lines:
            line = line.strip()
            if not line:
                continue
            try:
                msg = json.loads(line)
                if msg.get("role") == "assistant":
                    content = msg.get("content", "")
                    if content and not content.startswith("Using tools"):
                        final_content = content
            except json.JSONDecodeError:
                if not output.startswith('{"role":'):
                    return output
                continue
        return final_content if final_content else output

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using grok CLI.

        For large prompts (>100KB), uses stdin to avoid OS E2BIG error.
        """
        full_prompt = self._build_full_prompt(prompt, context)
        # Use stdin for large prompts to avoid E2BIG (arg list too long)
        if self._is_prompt_too_large_for_argv(full_prompt):
            logger.debug(f"[{self.name}] Using stdin for large prompt ({len(full_prompt)} chars)")
            return await self._generate_with_fallback(
                ["grok", "-p", "-"],
                prompt,
                context,
                input_text=full_prompt,
                response_extractor=self._extract_grok_response,
            )
        return await self._generate_with_fallback(
            ["grok", "-p", full_prompt],
            prompt,
            context,
            response_extractor=self._extract_grok_response,
        )


@AgentRegistry.register(
    "qwen-cli",
    default_model="qwen3-coder",
    agent_type="CLI",
    requires="qwen CLI (npm install -g @qwen-code/qwen-code)",
)
class QwenCLIAgent(CLIAgent):
    """Agent that uses Alibaba Qwen Code CLI.

    Falls back to OpenRouter (Qwen) on CLI failures if enabled.
    """

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using qwen CLI.

        For large prompts (>100KB), uses stdin to avoid OS E2BIG error.
        """
        full_prompt = self._build_full_prompt(prompt, context)
        # Use stdin for large prompts to avoid E2BIG (arg list too long)
        if self._is_prompt_too_large_for_argv(full_prompt):
            logger.debug(f"[{self.name}] Using stdin for large prompt ({len(full_prompt)} chars)")
            return await self._generate_with_fallback(
                ["qwen", "-p", "-"],
                prompt,
                context,
                input_text=full_prompt,
            )
        return await self._generate_with_fallback(
            ["qwen", "-p", full_prompt],
            prompt,
            context,
        )


@AgentRegistry.register(
    "deepseek-cli",
    default_model="deepseek-v3",
    agent_type="CLI",
    requires="deepseek CLI (pip install deepseek-cli)",
    env_vars="DEEPSEEK_API_KEY",
)
class DeepseekCLIAgent(CLIAgent):
    """Agent that uses Deepseek CLI.

    Falls back to OpenRouter (Deepseek) on CLI failures if enabled.
    """

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using deepseek CLI.

        For large prompts (>100KB), uses stdin to avoid OS E2BIG error.
        """
        full_prompt = self._build_full_prompt(prompt, context)
        # Use stdin for large prompts to avoid E2BIG (arg list too long)
        if self._is_prompt_too_large_for_argv(full_prompt):
            logger.debug(f"[{self.name}] Using stdin for large prompt ({len(full_prompt)} chars)")
            return await self._generate_with_fallback(
                ["deepseek", "-p", "-"],
                prompt,
                context,
                input_text=full_prompt,
            )
        return await self._generate_with_fallback(
            ["deepseek", "-p", full_prompt],
            prompt,
            context,
        )


@AgentRegistry.register(
    "openai",
    default_model="gpt-4o",
    agent_type="CLI",
    requires="openai CLI (pip install openai)",
    env_vars="OPENAI_API_KEY",
)
class OpenAIAgent(CLIAgent):
    """Agent that uses OpenAI CLI.

    Falls back to OpenRouter (OpenAI GPT) on CLI failures if enabled.
    """

    def __init__(
        self, name: str, model: str = "gpt-4o", role: str = "proposer", timeout: int = 120
    ) -> None:
        super().__init__(name, model, role, timeout)

    def _extract_openai_response(self, result: str) -> str:
        """Parse JSON response from OpenAI CLI."""
        try:
            data = json.loads(result)
            choices = data.get("choices", [])
            if choices:
                return choices[0].get("message", {}).get("content", result)
            return result
        except json.JSONDecodeError:
            return result

    async def generate(self, prompt: str, context: list[Message] | None = None) -> str:
        """Generate a response using openai CLI.

        For large prompts (>100KB), uses stdin to avoid OS E2BIG error.
        """
        full_prompt = self._build_full_prompt(prompt, context)
        # Use stdin for large prompts to avoid E2BIG (arg list too long)
        if self._is_prompt_too_large_for_argv(full_prompt):
            logger.debug(f"[{self.name}] Using stdin for large prompt ({len(full_prompt)} chars)")
            return await self._generate_with_fallback(
                [
                    "openai",
                    "api",
                    "chat.completions.create",
                    "-m",
                    self.model,
                    "-g",
                    "user",
                    "-",  # Read content from stdin
                ],
                prompt,
                context,
                input_text=full_prompt,
                response_extractor=self._extract_openai_response,
            )
        return await self._generate_with_fallback(
            [
                "openai",
                "api",
                "chat.completions.create",
                "-m",
                self.model,
                "-g",
                "user",
                full_prompt,
            ],
            prompt,
            context,
            response_extractor=self._extract_openai_response,
        )

    def _build_critique_prompt(self, proposal: str, task: str) -> str:
        """Build critique prompt with OpenAI-specific formatting."""
        return f"""Critically analyze this proposal:

Task: {task}
Proposal: {proposal}

Format your response as:
ISSUES:
- issue 1
- issue 2

SUGGESTIONS:
- suggestion 1
- suggestion 2

SEVERITY: X.X
REASONING: explanation"""


# Synchronous wrappers for convenience
def run_sync(coro: Any) -> Any:
    """Run an async function synchronously.

    Uses asyncio.run() which properly creates and closes the event loop,
    avoiding resource leaks and deprecation warnings from get_event_loop().
    """
    return asyncio.run(coro)
