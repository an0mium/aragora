"""
Implementation Step for executing code tasks in workflows.

Bridges the workflow engine to the HybridExecutor, enabling the
gold path: debate -> decision_plan -> workflow -> IMPLEMENTATION -> verify -> learn.

Config format (from DecisionPlan.to_workflow_definition):
    {
        "task_id": "task-1",
        "description": "Implement token bucket algorithm",
        "files": ["rate_limiter.py"],
        "complexity": "moderate",
        "repo_path": "/path/to/repo",  # optional, defaults to cwd
    }
"""

from __future__ import annotations

import logging
import time
from pathlib import Path
from typing import Any

from aragora.workflow.step import BaseStep, WorkflowContext

logger = logging.getLogger(__name__)


class ImplementationStep(BaseStep):
    """Execute a code implementation task using the HybridExecutor.

    This step type is the bridge between workflow DAGs generated by
    DecisionPlan and the actual code generation/editing system.

    It accepts the same config keys that DecisionPlan emits for
    implementation steps and delegates to HybridExecutor.execute_task().

    Config keys:
        task_id: str - Unique task identifier
        description: str - What to implement
        files: list[str] - Files to create or modify
        complexity: str - "simple", "moderate", or "complex"
        task_type: str - Optional task type ("code", "tests", "computer_use", etc.)
        capabilities: list[str] - Optional capability hints
        requires_approval: bool - Whether task required prior approval
        repo_path: str - Repository root (optional, defaults to cwd)
        stop_on_failure: bool - Whether to abort on task failure (default True)
    """

    def __init__(self, name: str, config: dict[str, Any] | None = None):
        super().__init__(name, config)

    async def execute(self, context: WorkflowContext) -> dict[str, Any]:
        """Execute an implementation task via HybridExecutor."""
        config = {**self._config, **context.current_step_config}

        task_id = config.get("task_id", "unknown")
        description = config.get("description", "")
        files = config.get("files", [])
        complexity = config.get("complexity", "moderate")
        task_type = config.get("task_type")
        capabilities = config.get("capabilities", []) or []
        requires_approval = bool(config.get("requires_approval", False))
        repo_path_str = config.get("repo_path") or context.get_state("repo_path", str(Path.cwd()))
        repo_path = Path(repo_path_str)

        logger.info(
            "ImplementationStep executing task %s: %s (%s complexity, %d files)",
            task_id,
            description[:80],
            complexity,
            len(files),
        )

        start = time.monotonic()

        try:
            from aragora.implement.executor import HybridExecutor
            from aragora.implement.types import ImplementTask

            metadata = context.metadata if isinstance(context.metadata, dict) else {}
            plan_meta = (
                metadata.get("plan_metadata")
                if isinstance(metadata.get("plan_metadata"), dict)
                else {}
            )
            profile_payload = (
                metadata.get("implementation_profile")
                or plan_meta.get("implementation_profile")
                or plan_meta.get("implementation")
            )
            profile = None
            if isinstance(profile_payload, dict):
                try:
                    from aragora.pipeline.decision_plan import ImplementationProfile

                    profile = ImplementationProfile.from_dict(profile_payload)
                except (ImportError, ValueError, TypeError, KeyError) as exc:
                    logger.debug("Failed to parse ImplementationProfile: %s", exc)
                    profile = None

            task = ImplementTask(
                id=task_id,
                description=description,
                files=files,
                complexity=complexity,
                task_type=str(task_type) if task_type is not None else None,
                capabilities=capabilities if isinstance(capabilities, list) else [],
                requires_approval=requires_approval,
            )

            executor = HybridExecutor(
                repo_path=repo_path,
                strategy=profile.strategy if profile and profile.strategy is not None else None,
                implementers=profile.implementers if profile and profile.implementers else None,
                critic=profile.critic if profile and profile.critic is not None else None,
                reviser=profile.reviser if profile and profile.reviser is not None else None,
                max_revisions=profile.max_revisions
                if profile and profile.max_revisions is not None
                else None,
                complexity_router=profile.complexity_router if profile else None,
                task_type_router=profile.task_type_router if profile else None,
                capability_router=profile.capability_router if profile else None,
            )
            result = await executor.execute_task_with_retry(task)

            elapsed = time.monotonic() - start

            output: dict[str, Any] = {
                "task_id": result.task_id,
                "success": result.success,
                "diff": result.diff,
                "error": result.error,
                "model_used": result.model_used,
                "duration_seconds": elapsed,
            }

            if result.success:
                logger.info(
                    "Task %s completed successfully in %.1fs (model: %s)",
                    task_id,
                    elapsed,
                    result.model_used,
                )
            else:
                logger.warning(
                    "Task %s failed in %.1fs: %s",
                    task_id,
                    elapsed,
                    result.error,
                )

            return output

        except ImportError as e:
            elapsed = time.monotonic() - start
            logger.error("HybridExecutor not available: %s", e)
            return {
                "task_id": task_id,
                "success": False,
                "diff": "",
                "error": f"Implementation system unavailable: {e}",
                "model_used": None,
                "duration_seconds": elapsed,
            }
        except (OSError, RuntimeError, ValueError, ConnectionError) as e:
            elapsed = time.monotonic() - start
            logger.error("Implementation task %s failed: %s", task_id, e)
            return {
                "task_id": task_id,
                "success": False,
                "diff": "",
                "error": "Implementation task failed",
                "model_used": None,
                "duration_seconds": elapsed,
            }


class VerificationStep(BaseStep):
    """Execute verification checks against implementation results.

    Runs the verification plan generated by DecisionPlanFactory,
    checking that implementation meets the debate's conclusions.

    Config keys:
        action: str - "verify" (default)
        test_count: int - Number of test cases
        critical_count: int - Number of P0 test cases
        verification_plan: dict - Serialized VerificationPlan (optional)
        run_tests: bool - Whether to run pytest (default True)
        test_paths: list[str] - Specific test paths to run
    """

    def __init__(self, name: str, config: dict[str, Any] | None = None):
        super().__init__(name, config)

    async def execute(self, context: WorkflowContext) -> dict[str, Any]:
        """Execute verification checks using TestRunner for structured results."""
        config = {**self._config, **context.current_step_config}

        test_count = config.get("test_count", 0)
        critical_count = config.get("critical_count", 0)
        run_tests = config.get("run_tests", True)
        test_paths = config.get("test_paths", [])

        logger.info(
            "VerificationStep: %d test cases (%d critical), run_tests=%s",
            test_count,
            critical_count,
            run_tests,
        )

        results: dict[str, Any] = {
            "success": True,
            "test_count": test_count,
            "tests_passed": 0,
            "tests_failed": 0,
            "errors": [],
        }

        if run_tests and test_paths:
            repo_path = context.get_state("repo_path", str(Path.cwd()))
            try:
                from aragora.nomic.testfixer.runner import TestRunner

                test_command = f"python -m pytest {' '.join(test_paths)} -v --tb=short"
                runner = TestRunner(
                    repo_path=Path(repo_path),
                    test_command=test_command,
                    timeout_seconds=300,
                )
                test_result = await runner.run()

                results["exit_code"] = test_result.exit_code
                results["success"] = test_result.success
                results["tests_passed"] = test_result.passed
                results["tests_failed"] = test_result.failed
                results["num_passed"] = test_result.passed
                results["num_failed"] = test_result.failed
                results["stdout"] = test_result.stdout[-2000:] if test_result.stdout else ""
                results["stderr"] = test_result.stderr[-1000:] if test_result.stderr else ""
                results["test_result"] = test_result

                # Serialize failure details for downstream use
                results["failure_details"] = [
                    {
                        "test_name": f.test_name,
                        "test_file": f.test_file,
                        "line_number": f.line_number,
                        "error_type": f.error_type,
                        "error_message": f.error_message,
                        "stack_trace": f.stack_trace[:500],
                        "involved_files": f.involved_files,
                    }
                    for f in test_result.failures[:10]
                ]

            except ImportError:
                logger.debug("TestRunner unavailable, falling back to subprocess")
                results = await self._run_tests_subprocess(test_paths, repo_path, results)
            except (OSError, FileNotFoundError) as e:
                results["success"] = False
                results["errors"].append(f"Failed to run tests: {e}")
        else:
            # No test paths specified - mark as passed (verification plan only)
            results["tests_passed"] = test_count
            results["success"] = True

        return results

    async def _run_tests_subprocess(
        self,
        test_paths: list[str],
        repo_path: str,
        results: dict[str, Any],
    ) -> dict[str, Any]:
        """Fallback: run tests via raw subprocess when TestRunner is unavailable."""
        import re
        import subprocess

        try:
            cmd = ["python", "-m", "pytest", *test_paths, "-v", "--tb=short"]
            proc = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300,
                cwd=repo_path,
            )
            results["exit_code"] = proc.returncode
            results["success"] = proc.returncode == 0
            results["stdout"] = proc.stdout[-2000:] if proc.stdout else ""
            results["stderr"] = proc.stderr[-1000:] if proc.stderr else ""

            for line in proc.stdout.splitlines():
                if "passed" in line:
                    m = re.search(r"(\d+) passed", line)
                    if m:
                        results["tests_passed"] = int(m.group(1))
                if "failed" in line:
                    m = re.search(r"(\d+) failed", line)
                    if m:
                        results["tests_failed"] = int(m.group(1))

        except subprocess.TimeoutExpired:
            results["success"] = False
            results["errors"].append("Test suite timed out after 300s")
        except (OSError, FileNotFoundError) as e:
            results["success"] = False
            results["errors"].append(f"Failed to run tests: {e}")

        return results
