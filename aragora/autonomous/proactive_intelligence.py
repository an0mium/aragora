"""
Proactive Intelligence (Phase 5.3).

Provides:
- Scheduled debate triggers
- Alert-based analysis
- Trend monitoring
- Anomaly detection
"""

import asyncio
import json
import logging
import statistics
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Coroutine, Dict, List, Optional, Set

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Severity levels for alerts."""

    INFO = "info"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class TrendDirection(Enum):
    """Direction of a trend."""

    INCREASING = "increasing"
    DECREASING = "decreasing"
    STABLE = "stable"
    VOLATILE = "volatile"


@dataclass
class ScheduledTriggerConfig:
    """Configuration for a scheduled trigger."""

    id: str
    name: str
    cron_expression: Optional[str] = None
    interval_seconds: Optional[int] = None
    enabled: bool = True
    last_run: Optional[datetime] = None
    next_run: Optional[datetime] = None
    run_count: int = 0
    max_runs: Optional[int] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Alert:
    """An alert generated by the system."""

    id: str
    severity: AlertSeverity
    title: str
    description: str
    source: str
    timestamp: datetime
    acknowledged: bool = False
    acknowledged_by: Optional[str] = None
    acknowledged_at: Optional[datetime] = None
    resolved: bool = False
    resolved_at: Optional[datetime] = None
    debate_triggered: bool = False
    debate_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TrendData:
    """Data about a monitored trend."""

    metric_name: str
    direction: TrendDirection
    current_value: float
    previous_value: float
    change_percent: float
    period_start: datetime
    period_end: datetime
    data_points: int
    confidence: float
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class Anomaly:
    """A detected anomaly."""

    id: str
    metric_name: str
    value: float
    expected_value: float
    deviation: float  # Standard deviations from expected
    timestamp: datetime
    severity: AlertSeverity
    description: str
    metadata: Dict[str, Any] = field(default_factory=dict)


class ScheduledTrigger:
    """
    Manages scheduled debate triggers.

    Supports:
    - Cron-like scheduling
    - Interval-based scheduling
    - Dynamic topic generation
    - Conditional execution
    """

    def __init__(
        self,
        storage_path: Optional[Path] = None,
        debate_creator: Optional[Callable[..., Coroutine]] = None,
    ):
        """
        Initialize scheduled trigger manager.

        Args:
            storage_path: Path to store trigger state
            debate_creator: Async function to create debates
        """
        self.storage_path = storage_path
        self.debate_creator = debate_creator
        self._triggers: Dict[str, ScheduledTriggerConfig] = {}
        self._running = False
        self._task: Optional[asyncio.Task] = None

    def add_trigger(
        self,
        trigger_id: str,
        name: str,
        interval_seconds: Optional[int] = None,
        cron_expression: Optional[str] = None,
        enabled: bool = True,
        max_runs: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> ScheduledTriggerConfig:
        """
        Add a scheduled trigger.

        Args:
            trigger_id: Unique identifier
            name: Human-readable name
            interval_seconds: Run every N seconds
            cron_expression: Cron expression (future)
            enabled: Whether trigger is active
            max_runs: Maximum number of times to run
            metadata: Additional metadata

        Returns:
            Created trigger config
        """
        trigger = ScheduledTriggerConfig(
            id=trigger_id,
            name=name,
            interval_seconds=interval_seconds,
            cron_expression=cron_expression,
            enabled=enabled,
            max_runs=max_runs,
            metadata=metadata or {},
        )

        if interval_seconds:
            trigger.next_run = datetime.now() + timedelta(seconds=interval_seconds)

        self._triggers[trigger_id] = trigger
        self._save_state()

        logger.info(f"Added trigger: {trigger_id} - {name}")

        return trigger

    def remove_trigger(self, trigger_id: str) -> bool:
        """Remove a scheduled trigger."""
        if trigger_id in self._triggers:
            del self._triggers[trigger_id]
            self._save_state()
            return True
        return False

    def enable_trigger(self, trigger_id: str) -> bool:
        """Enable a trigger."""
        if trigger_id in self._triggers:
            self._triggers[trigger_id].enabled = True
            self._save_state()
            return True
        return False

    def disable_trigger(self, trigger_id: str) -> bool:
        """Disable a trigger."""
        if trigger_id in self._triggers:
            self._triggers[trigger_id].enabled = False
            self._save_state()
            return True
        return False

    async def start(self) -> None:
        """Start the trigger scheduler."""
        if self._running:
            return

        self._running = True
        self._task = asyncio.create_task(self._run_scheduler())
        logger.info("Scheduled trigger manager started")

    async def stop(self) -> None:
        """Stop the trigger scheduler."""
        self._running = False
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        logger.info("Scheduled trigger manager stopped")

    async def _run_scheduler(self) -> None:
        """Main scheduler loop."""
        while self._running:
            now = datetime.now()

            for trigger_id, trigger in list(self._triggers.items()):
                if not trigger.enabled:
                    continue

                if trigger.max_runs and trigger.run_count >= trigger.max_runs:
                    continue

                if trigger.next_run and now >= trigger.next_run:
                    await self._execute_trigger(trigger)

            await asyncio.sleep(1)  # Check every second

    async def _execute_trigger(self, trigger: ScheduledTriggerConfig) -> None:
        """Execute a trigger."""
        logger.info(f"Executing trigger: {trigger.id} - {trigger.name}")

        trigger.last_run = datetime.now()
        trigger.run_count += 1

        # Schedule next run
        if trigger.interval_seconds:
            trigger.next_run = datetime.now() + timedelta(seconds=trigger.interval_seconds)

        # Create debate if handler is set
        if self.debate_creator:
            try:
                topic = trigger.metadata.get("topic", f"Scheduled analysis: {trigger.name}")
                agents = trigger.metadata.get("agents", ["anthropic-api", "openai-api"])
                rounds = trigger.metadata.get("rounds", 3)

                await self.debate_creator(topic, agents, rounds)

            except Exception as e:
                logger.error(f"Trigger {trigger.id} debate creation failed: {e}")

        self._save_state()

    def list_triggers(self) -> List[ScheduledTriggerConfig]:
        """List all triggers."""
        return list(self._triggers.values())

    def _save_state(self) -> None:
        """Save trigger state to storage."""
        if not self.storage_path:
            return

        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            tid: {
                "id": t.id,
                "name": t.name,
                "interval_seconds": t.interval_seconds,
                "cron_expression": t.cron_expression,
                "enabled": t.enabled,
                "last_run": t.last_run.isoformat() if t.last_run else None,
                "next_run": t.next_run.isoformat() if t.next_run else None,
                "run_count": t.run_count,
                "max_runs": t.max_runs,
                "metadata": t.metadata,
            }
            for tid, t in self._triggers.items()
        }
        self.storage_path.write_text(json.dumps(data, indent=2))


class AlertAnalyzer:
    """
    Analyzes conditions and generates alerts that may trigger debates.

    Features:
    - Threshold-based alerts
    - Rate-of-change alerts
    - Pattern-based alerts
    - Auto-debate triggering for critical alerts
    """

    def __init__(
        self,
        storage_path: Optional[Path] = None,
        alert_callback: Optional[Callable[[Alert], None]] = None,
        debate_creator: Optional[Callable[..., Coroutine]] = None,
        auto_debate_severities: Optional[Set[AlertSeverity]] = None,
    ):
        """
        Initialize alert analyzer.

        Args:
            storage_path: Path to store alerts
            alert_callback: Called when alert is generated
            debate_creator: Function to create debates
            auto_debate_severities: Severities that auto-trigger debates
        """
        self.storage_path = storage_path
        self.alert_callback = alert_callback
        self.debate_creator = debate_creator
        self.auto_debate_severities = auto_debate_severities or {
            AlertSeverity.HIGH,
            AlertSeverity.CRITICAL,
        }
        self._alerts: Dict[str, Alert] = {}
        self._thresholds: Dict[str, Dict[str, Any]] = {}
        self._metric_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))

    def set_threshold(
        self,
        metric_name: str,
        warning_threshold: Optional[float] = None,
        critical_threshold: Optional[float] = None,
        comparison: str = "gt",  # gt, lt, eq, ne
        enabled: bool = True,
    ) -> None:
        """
        Set alert thresholds for a metric.

        Args:
            metric_name: Name of the metric
            warning_threshold: Warning level threshold
            critical_threshold: Critical level threshold
            comparison: Comparison type (gt, lt, eq, ne)
            enabled: Whether threshold is active
        """
        self._thresholds[metric_name] = {
            "warning": warning_threshold,
            "critical": critical_threshold,
            "comparison": comparison,
            "enabled": enabled,
        }

    async def check_metric(
        self,
        metric_name: str,
        value: float,
        source: str = "system",
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Optional[Alert]:
        """
        Check a metric against thresholds and generate alert if needed.

        Args:
            metric_name: Name of the metric
            value: Current value
            source: Source of the metric
            metadata: Additional metadata

        Returns:
            Alert if threshold exceeded, None otherwise
        """
        # Record in history
        self._metric_history[metric_name].append(
            {
                "value": value,
                "timestamp": datetime.now(),
            }
        )

        # Check thresholds
        threshold = self._thresholds.get(metric_name)
        if not threshold or not threshold["enabled"]:
            return None

        comparison = threshold["comparison"]
        severity = None
        threshold_value = None

        # Check critical first
        if threshold["critical"] is not None:
            if self._compare(value, threshold["critical"], comparison):
                severity = AlertSeverity.CRITICAL
                threshold_value = threshold["critical"]

        # Check warning if not critical
        if severity is None and threshold["warning"] is not None:
            if self._compare(value, threshold["warning"], comparison):
                severity = AlertSeverity.HIGH
                threshold_value = threshold["warning"]

        if severity is None:
            return None

        # Generate alert
        alert = await self._create_alert(
            severity=severity,
            title=f"{metric_name} threshold exceeded",
            description=f"{metric_name} = {value} ({comparison} {threshold_value})",
            source=source,
            metadata={
                "metric_name": metric_name,
                "value": value,
                "threshold": threshold_value,
                "comparison": comparison,
                **(metadata or {}),
            },
        )

        return alert

    async def _create_alert(
        self,
        severity: AlertSeverity,
        title: str,
        description: str,
        source: str,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Alert:
        """Create and store an alert."""
        import hashlib

        alert_id = hashlib.sha256(f"{title}{datetime.now().isoformat()}".encode()).hexdigest()[:16]

        alert = Alert(
            id=alert_id,
            severity=severity,
            title=title,
            description=description,
            source=source,
            timestamp=datetime.now(),
            metadata=metadata or {},
        )

        self._alerts[alert_id] = alert

        # Notify callback
        if self.alert_callback:
            self.alert_callback(alert)

        # Auto-trigger debate for high severity
        if severity in self.auto_debate_severities and self.debate_creator:
            try:
                topic = f"Analyze alert: {title}. {description}"
                result = await self.debate_creator(topic)
                if result:
                    alert.debate_triggered = True
                    alert.debate_id = result.get("debate_id")
            except Exception as e:
                logger.error(f"Auto-debate for alert failed: {e}")

        self._save_alerts()

        logger.info(f"Alert created: [{severity.value}] {title}")

        return alert

    def _compare(self, value: float, threshold: float, comparison: str) -> bool:
        """Compare value to threshold."""
        if comparison == "gt":
            return value > threshold
        elif comparison == "lt":
            return value < threshold
        elif comparison == "eq":
            return abs(value - threshold) < 0.0001
        elif comparison == "ne":
            return abs(value - threshold) >= 0.0001
        elif comparison == "gte":
            return value >= threshold
        elif comparison == "lte":
            return value <= threshold
        return False

    def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:
        """Acknowledge an alert."""
        if alert_id not in self._alerts:
            return False

        self._alerts[alert_id].acknowledged = True
        self._alerts[alert_id].acknowledged_by = acknowledged_by
        self._alerts[alert_id].acknowledged_at = datetime.now()
        self._save_alerts()

        return True

    def resolve_alert(self, alert_id: str) -> bool:
        """Resolve an alert."""
        if alert_id not in self._alerts:
            return False

        self._alerts[alert_id].resolved = True
        self._alerts[alert_id].resolved_at = datetime.now()
        self._save_alerts()

        return True

    def get_active_alerts(self) -> List[Alert]:
        """Get all unresolved alerts."""
        return [a for a in self._alerts.values() if not a.resolved]

    def _save_alerts(self) -> None:
        """Save alerts to storage."""
        if not self.storage_path:
            return

        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        data = {
            aid: {
                "id": a.id,
                "severity": a.severity.value,
                "title": a.title,
                "description": a.description,
                "source": a.source,
                "timestamp": a.timestamp.isoformat(),
                "acknowledged": a.acknowledged,
                "acknowledged_by": a.acknowledged_by,
                "acknowledged_at": a.acknowledged_at.isoformat() if a.acknowledged_at else None,
                "resolved": a.resolved,
                "resolved_at": a.resolved_at.isoformat() if a.resolved_at else None,
                "debate_triggered": a.debate_triggered,
                "debate_id": a.debate_id,
                "metadata": a.metadata,
            }
            for aid, a in self._alerts.items()
        }
        self.storage_path.write_text(json.dumps(data, indent=2))


class TrendMonitor:
    """
    Monitors metrics for trends over time.

    Detects:
    - Increasing/decreasing trends
    - Rate of change
    - Volatility
    - Seasonal patterns
    """

    def __init__(
        self,
        window_size: int = 100,
        min_data_points: int = 10,
        trend_threshold: float = 0.1,  # 10% change to detect trend
    ):
        """
        Initialize trend monitor.

        Args:
            window_size: Number of data points to keep
            min_data_points: Minimum points needed to detect trend
            trend_threshold: Minimum change to consider a trend
        """
        self.window_size = window_size
        self.min_data_points = min_data_points
        self.trend_threshold = trend_threshold
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))

    def record(
        self,
        metric_name: str,
        value: float,
        timestamp: Optional[datetime] = None,
    ) -> None:
        """Record a metric value."""
        self._metrics[metric_name].append(
            {
                "value": value,
                "timestamp": timestamp or datetime.now(),
            }
        )

    def get_trend(
        self,
        metric_name: str,
        period_seconds: Optional[int] = None,
    ) -> Optional[TrendData]:
        """
        Get trend for a metric.

        Args:
            metric_name: Name of the metric
            period_seconds: Time period to analyze (None = all data)

        Returns:
            TrendData if enough data, None otherwise
        """
        data = list(self._metrics.get(metric_name, []))

        if len(data) < self.min_data_points:
            return None

        # Filter by period if specified
        if period_seconds:
            cutoff = datetime.now() - timedelta(seconds=period_seconds)
            data = [d for d in data if d["timestamp"] >= cutoff]

            if len(data) < self.min_data_points:
                return None

        values = [d["value"] for d in data]
        timestamps = [d["timestamp"] for d in data]

        # Calculate trend
        first_half_avg = statistics.mean(values[: len(values) // 2])
        second_half_avg = statistics.mean(values[len(values) // 2 :])

        if first_half_avg == 0:
            change_percent = 0 if second_half_avg == 0 else float("inf")
        else:
            change_percent = (second_half_avg - first_half_avg) / first_half_avg

        # Determine direction
        if abs(change_percent) < self.trend_threshold:
            direction = TrendDirection.STABLE
        elif change_percent > 0:
            direction = TrendDirection.INCREASING
        else:
            direction = TrendDirection.DECREASING

        # Check for volatility
        if len(values) > 2:
            stdev = statistics.stdev(values)
            mean = statistics.mean(values)
            cv = stdev / mean if mean != 0 else 0
            if cv > 0.3:  # Coefficient of variation > 30%
                direction = TrendDirection.VOLATILE

        # Calculate confidence based on data points and consistency
        confidence = min(1.0, len(data) / 50)  # Max confidence at 50 points

        return TrendData(
            metric_name=metric_name,
            direction=direction,
            current_value=values[-1],
            previous_value=values[0],
            change_percent=change_percent * 100,
            period_start=timestamps[0],
            period_end=timestamps[-1],
            data_points=len(data),
            confidence=confidence,
        )

    def get_all_trends(self) -> Dict[str, TrendData]:
        """Get trends for all monitored metrics."""
        trends = {}
        for metric_name in self._metrics.keys():
            trend = self.get_trend(metric_name)
            if trend:
                trends[metric_name] = trend
        return trends


class AnomalyDetector:
    """
    Detects anomalies in metric data.

    Uses:
    - Statistical methods (z-score, IQR)
    - Moving average deviation
    - Pattern-based detection
    """

    def __init__(
        self,
        window_size: int = 100,
        z_threshold: float = 3.0,
        min_data_points: int = 20,
        alert_callback: Optional[Callable[[Anomaly], None]] = None,
    ):
        """
        Initialize anomaly detector.

        Args:
            window_size: Number of data points for baseline
            z_threshold: Z-score threshold for anomaly
            min_data_points: Minimum points before detecting
            alert_callback: Called when anomaly detected
        """
        self.window_size = window_size
        self.z_threshold = z_threshold
        self.min_data_points = min_data_points
        self.alert_callback = alert_callback
        self._metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))
        self._anomalies: List[Anomaly] = []

    def record(
        self,
        metric_name: str,
        value: float,
        timestamp: Optional[datetime] = None,
    ) -> Optional[Anomaly]:
        """
        Record a metric value and check for anomaly.

        Args:
            metric_name: Name of the metric
            value: Current value
            timestamp: Timestamp (default: now)

        Returns:
            Anomaly if detected, None otherwise
        """
        data = self._metrics[metric_name]

        # Need baseline before detecting
        if len(data) < self.min_data_points:
            data.append({"value": value, "timestamp": timestamp or datetime.now()})
            return None

        values = [d["value"] for d in data]
        mean = statistics.mean(values)
        stdev = statistics.stdev(values) if len(values) > 1 else 0

        # Record the new value
        data.append({"value": value, "timestamp": timestamp or datetime.now()})

        # Check for anomaly using z-score
        if stdev == 0:
            return None

        z_score = (value - mean) / stdev

        if abs(z_score) >= self.z_threshold:
            # Determine severity based on z-score
            if abs(z_score) >= 4.0:
                severity = AlertSeverity.CRITICAL
            elif abs(z_score) >= 3.5:
                severity = AlertSeverity.HIGH
            else:
                severity = AlertSeverity.MEDIUM

            direction = "above" if z_score > 0 else "below"

            anomaly = Anomaly(
                id=f"anomaly_{metric_name}_{datetime.now().isoformat()}",
                metric_name=metric_name,
                value=value,
                expected_value=mean,
                deviation=abs(z_score),
                timestamp=datetime.now(),
                severity=severity,
                description=(
                    f"{metric_name} is {abs(z_score):.1f} standard deviations "
                    f"{direction} expected ({value:.2f} vs {mean:.2f})"
                ),
            )

            self._anomalies.append(anomaly)

            if self.alert_callback:
                self.alert_callback(anomaly)

            logger.info(f"Anomaly detected: {anomaly.description}")

            return anomaly

        return None

    def get_recent_anomalies(
        self,
        hours: int = 24,
        metric_name: Optional[str] = None,
    ) -> List[Anomaly]:
        """Get anomalies from the last N hours."""
        cutoff = datetime.now() - timedelta(hours=hours)
        anomalies = [a for a in self._anomalies if a.timestamp >= cutoff]

        if metric_name:
            anomalies = [a for a in anomalies if a.metric_name == metric_name]

        return anomalies

    def get_baseline_stats(
        self,
        metric_name: str,
    ) -> Optional[Dict[str, float]]:
        """Get baseline statistics for a metric."""
        data = self._metrics.get(metric_name)
        if not data or len(data) < self.min_data_points:
            return None

        values = [d["value"] for d in data]

        return {
            "mean": statistics.mean(values),
            "stdev": statistics.stdev(values) if len(values) > 1 else 0,
            "min": min(values),
            "max": max(values),
            "median": statistics.median(values),
            "count": len(values),
        }


__all__ = [
    "AlertSeverity",
    "TrendDirection",
    "ScheduledTriggerConfig",
    "Alert",
    "TrendData",
    "Anomaly",
    "ScheduledTrigger",
    "AlertAnalyzer",
    "TrendMonitor",
    "AnomalyDetector",
]
