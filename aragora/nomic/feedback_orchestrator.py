"""Self-Improvement Feedback Orchestrator.

Runs after each self-improvement cycle to bridge 7 audit subsystems
into active feedback that informs the next improvement cycle.

Steps:
1. Gauntlet: validate execution receipts
2. Introspection: capture agent performance snapshots
3. Genesis: auto-breed improved agent configs when quality < threshold
4. Learning: adjust hyperparameters from cycle outcomes
5. Workspace: deduplicate work via bead fingerprints
6. Pulse: inject trending topics into improvement queue
7. Knowledge Contradiction: detect conflicting knowledge in KnowledgeMound

Results are persisted to the ImprovementQueue (SQLite-backed) which
MetaPlanner._scan_prioritize() reads as Signal 8 on the next cycle.

Usage:
    orchestrator = SelfImproveFeedbackOrchestrator()
    feedback = orchestrator.run(cycle_id, execution_results)
"""

from __future__ import annotations

import json
import logging
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)

# Default path for the improvement queue SQLite database
_DEFAULT_QUEUE_PATH = Path(".aragora_beads") / "improvement_queue.json"


@dataclass
class FeedbackGoal:
    """A goal generated by the feedback orchestrator."""

    description: str
    source: str  # e.g., "gauntlet_finding", "introspection_gap", "pulse_trending"
    track: str = "core"
    priority: int = 5
    estimated_impact: str = "medium"
    created_at: float = field(default_factory=time.time)
    metadata: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        return {
            "description": self.description,
            "source": self.source,
            "track": self.track,
            "priority": self.priority,
            "estimated_impact": self.estimated_impact,
            "created_at": self.created_at,
            "metadata": self.metadata,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> FeedbackGoal:
        return cls(
            description=data.get("description", ""),
            source=data.get("source", "unknown"),
            track=data.get("track", "core"),
            priority=data.get("priority", 5),
            estimated_impact=data.get("estimated_impact", "medium"),
            created_at=data.get("created_at", 0.0),
            metadata=data.get("metadata", {}),
        )


@dataclass
class ImprovementQueue:
    """Persisted queue of feedback-generated improvement goals.

    MetaPlanner reads from this queue as an additional signal source
    during scan mode. Goals have a source field indicating which
    subsystem generated them.
    """

    goals: list[FeedbackGoal] = field(default_factory=list)
    max_goals: int = 50

    def add(self, goal: FeedbackGoal) -> None:
        """Add a goal, evicting oldest if at capacity."""
        self.goals.append(goal)
        if len(self.goals) > self.max_goals:
            self.goals = self.goals[-self.max_goals :]

    def save(self, path: Path | None = None) -> None:
        """Persist queue to disk."""
        target = path or _DEFAULT_QUEUE_PATH
        target.parent.mkdir(parents=True, exist_ok=True)
        data = [g.to_dict() for g in self.goals]
        target.write_text(json.dumps(data, indent=2), encoding="utf-8")

    @classmethod
    def load(cls, path: Path | None = None) -> ImprovementQueue:
        """Load queue from disk, returning empty queue if file doesn't exist."""
        target = path or _DEFAULT_QUEUE_PATH
        queue = cls()
        if target.exists():
            try:
                data = json.loads(target.read_text(encoding="utf-8"))
                queue.goals = [FeedbackGoal.from_dict(g) for g in data]
            except (json.JSONDecodeError, KeyError, TypeError) as e:
                logger.warning("Failed to load improvement queue: %s", e)
        return queue


@dataclass
class FeedbackResult:
    """Result from running the feedback orchestrator."""

    cycle_id: str
    goals_generated: int = 0
    steps_completed: int = 0
    steps_failed: int = 0
    gauntlet_findings: int = 0
    introspection_snapshots: int = 0
    genesis_breeds: int = 0
    learning_adjustments: int = 0
    workspace_deduped: int = 0
    pulse_injections: int = 0
    contradiction_detections: int = 0


class SelfImproveFeedbackOrchestrator:
    """7-step feedback pipeline bridging audit subsystems to active improvement.

    Each step is wrapped in try/except for graceful degradation — any
    subsystem failure logs a warning but doesn't block the pipeline.
    """

    def __init__(self, queue_path: Path | None = None):
        self._queue_path = queue_path
        self._queue = ImprovementQueue.load(queue_path)

    def run(
        self,
        cycle_id: str,
        execution_results: list[dict[str, Any]],
    ) -> FeedbackResult:
        """Run the 6-step feedback pipeline.

        Args:
            cycle_id: The self-improvement cycle ID.
            execution_results: List of per-subtask execution result dicts.

        Returns:
            FeedbackResult with counts of generated goals and step outcomes.
        """
        result = FeedbackResult(cycle_id=cycle_id)

        # Step 1: Gauntlet — validate execution receipts
        try:
            goals = self._step_gauntlet(execution_results)
            result.gauntlet_findings = len(goals)
            for g in goals:
                self._queue.add(g)
                result.goals_generated += 1
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_gauntlet_failed: %s", e)
            result.steps_failed += 1

        # Step 2: Introspection — capture agent performance snapshots
        try:
            goals = self._step_introspection(execution_results)
            result.introspection_snapshots = len(goals)
            for g in goals:
                self._queue.add(g)
                result.goals_generated += 1
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_introspection_failed: %s", e)
            result.steps_failed += 1

        # Step 3: Genesis — auto-breed new agent configs when quality < threshold
        try:
            breeds = self._step_genesis(execution_results)
            result.genesis_breeds = breeds
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_genesis_failed: %s", e)
            result.steps_failed += 1

        # Step 4: Learning — hyperparameter adjustments from cycle outcomes
        try:
            adjustments = self._step_learning(cycle_id, execution_results)
            result.learning_adjustments = adjustments
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_learning_failed: %s", e)
            result.steps_failed += 1

        # Step 5: Workspace — deduplicate via bead fingerprints
        try:
            deduped = self._step_workspace(execution_results)
            result.workspace_deduped = deduped
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_workspace_failed: %s", e)
            result.steps_failed += 1

        # Step 6: Pulse — inject trending topics as priorities
        try:
            goals = self._step_pulse()
            result.pulse_injections = len(goals)
            for g in goals:
                self._queue.add(g)
                result.goals_generated += 1
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_pulse_failed: %s", e)
            result.steps_failed += 1

        # Step 7: Knowledge Contradiction — detect conflicting knowledge in KM
        try:
            goals = self._step_knowledge_contradiction(cycle_id)
            result.contradiction_detections = len(goals)
            for g in goals:
                self._queue.add(g)
                result.goals_generated += 1
            result.steps_completed += 1
        except (ImportError, RuntimeError, ValueError, OSError) as e:
            logger.warning("feedback_step_knowledge_contradiction_failed: %s", e)
            result.steps_failed += 1

        # Persist the queue
        self._queue.save(self._queue_path)

        logger.info(
            "feedback_orchestrator_complete cycle=%s goals=%d steps=%d/%d",
            cycle_id,
            result.goals_generated,
            result.steps_completed,
            result.steps_completed + result.steps_failed,
        )

        return result

    # --- Step implementations ---

    def _step_gauntlet(
        self,
        execution_results: list[dict[str, Any]],
    ) -> list[FeedbackGoal]:
        """Step 1: Run Gauntlet validation on execution receipts."""
        goals: list[FeedbackGoal] = []

        from aragora.gauntlet.runner import GauntletRunner

        runner = GauntletRunner()

        for er in execution_results:
            receipt_hash = er.get("receipt_hash")
            if not receipt_hash:
                continue

            files_changed = er.get("files_changed", [])
            if not files_changed:
                continue

            try:
                findings = runner.scan_files(files_changed)
                critical = [f for f in findings if f.severity in ("critical", "high")]

                if critical:
                    desc = f"Gauntlet found {len(critical)} critical findings in {', '.join(files_changed[:3])}"
                    goals.append(FeedbackGoal(
                        description=desc,
                        source="gauntlet_finding",
                        track="security",
                        priority=1,
                        estimated_impact="high",
                        metadata={"receipt_hash": receipt_hash, "finding_count": len(critical)},
                    ))
            except (RuntimeError, ValueError, OSError, AttributeError):
                continue

        return goals

    def _step_introspection(
        self,
        execution_results: list[dict[str, Any]],
    ) -> list[FeedbackGoal]:
        """Step 2: Capture agent performance and identify capability gaps."""
        goals: list[FeedbackGoal] = []

        from aragora.introspection.tracker import PerformanceTracker

        tracker = PerformanceTracker()
        snapshots = tracker.get_recent_snapshots(limit=10)

        for snapshot in snapshots:
            # Detect agents with low success rates
            success_rate = getattr(snapshot, "success_rate", 1.0)
            agent_name = getattr(snapshot, "agent_name", "unknown")

            if success_rate < 0.5:
                goals.append(FeedbackGoal(
                    description=(
                        f"Agent '{agent_name}' has low success rate ({success_rate:.0%}). "
                        f"Consider retraining or replacing for this domain."
                    ),
                    source="introspection_gap",
                    track="core",
                    priority=3,
                    estimated_impact="medium",
                    metadata={"agent_name": agent_name, "success_rate": success_rate},
                ))

        return goals

    def _step_genesis(
        self,
        execution_results: list[dict[str, Any]],
    ) -> int:
        """Step 3: Auto-breed improved agent configs when quality < threshold."""
        from aragora.genesis.evolution import AgentEvolver

        evolver = AgentEvolver()

        # Check if any execution results show consistently poor quality
        failed_count = sum(1 for er in execution_results if not er.get("success"))
        total = len(execution_results)

        if total > 0 and failed_count / total > 0.5:
            bred = evolver.breed_from_failures(
                failure_contexts=[er for er in execution_results if not er.get("success")]
            )
            return bred

        return 0

    def _step_learning(
        self,
        cycle_id: str,
        execution_results: list[dict[str, Any]],
    ) -> int:
        """Step 4: Adjust hyperparameters based on cycle outcomes."""
        from aragora.learning.meta_optimizer import MetaOptimizer

        optimizer = MetaOptimizer()

        # Feed cycle outcomes as training signal
        outcomes = []
        for er in execution_results:
            outcomes.append({
                "success": er.get("success", False),
                "tests_passed": er.get("tests_passed", 0),
                "tests_failed": er.get("tests_failed", 0),
                "files_changed": len(er.get("files_changed", [])),
            })

        adjustments = optimizer.adjust_from_outcomes(cycle_id, outcomes)
        return adjustments

    def _step_workspace(
        self,
        execution_results: list[dict[str, Any]],
    ) -> int:
        """Step 5: Deduplicate executed work via bead fingerprints."""
        from aragora.workspace.bead import BeadStore

        store = BeadStore()
        deduped = 0

        for er in execution_results:
            desc = er.get("subtask", "")
            files = er.get("files_changed", [])
            if desc and files:
                fingerprint = f"{desc}:{','.join(sorted(files))}"
                if store.has_fingerprint(fingerprint):
                    deduped += 1
                else:
                    store.record_fingerprint(fingerprint)

        return deduped

    def _step_pulse(self) -> list[FeedbackGoal]:
        """Step 6: Inject trending topics from Pulse as timely priorities."""
        goals: list[FeedbackGoal] = []

        from aragora.pulse.scheduler import PulseScheduler

        scheduler = PulseScheduler()
        trending = scheduler.get_trending(limit=5)

        for topic in trending:
            title = getattr(topic, "title", str(topic))
            category = getattr(topic, "category", "general")

            # Map Pulse categories to tracks
            track_map = {
                "security": "security",
                "ai": "core",
                "testing": "qa",
                "frontend": "sme",
                "devtools": "developer",
            }
            track = track_map.get(category, "core")

            goals.append(FeedbackGoal(
                description=f"Trending: {title[:100]}",
                source="pulse_trending",
                track=track,
                priority=4,
                estimated_impact="low",
                metadata={"category": category},
            ))

        return goals

    def _step_knowledge_contradiction(
        self,
        cycle_id: str,
    ) -> list[FeedbackGoal]:
        """Step 7: Detect contradictions in KnowledgeMound.

        Scans the 'nomic' workspace for conflicting knowledge items and
        generates feedback goals for high-severity contradictions so they
        can be resolved in the next improvement cycle.
        """
        import asyncio

        from aragora.knowledge.mound import get_knowledge_mound
        from aragora.knowledge.mound.ops.contradiction import (
            ContradictionDetector,
        )

        goals: list[FeedbackGoal] = []

        mound = get_knowledge_mound(workspace_id="nomic")
        detector = ContradictionDetector()

        # Run the async detection in a sync context
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop and loop.is_running():
            # Already in an async context — create a task via nest_asyncio
            # or fall back to a new thread. Use thread for safety.
            import concurrent.futures

            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
                report = pool.submit(
                    asyncio.run,
                    detector.detect_contradictions(mound, workspace_id="nomic"),
                ).result(timeout=30)
        else:
            report = asyncio.run(
                detector.detect_contradictions(mound, workspace_id="nomic")
            )

        if report.contradictions_found == 0:
            logger.debug(
                "feedback_contradiction_scan cycle=%s clean", cycle_id,
            )
            return goals

        logger.info(
            "feedback_contradiction_scan cycle=%s found=%d by_severity=%s",
            cycle_id,
            report.contradictions_found,
            report.by_severity,
        )

        # Generate goals for medium+ severity contradictions
        for contradiction in report.contradictions:
            if contradiction.severity in ("low",):
                continue

            priority_map = {"critical": 1, "high": 2, "medium": 3}
            impact_map = {"critical": "high", "high": "high", "medium": "medium"}

            goals.append(FeedbackGoal(
                description=(
                    f"KM contradiction ({contradiction.severity}): "
                    f"{contradiction.contradiction_type.value} conflict "
                    f"between items {contradiction.item_a_id} and "
                    f"{contradiction.item_b_id} "
                    f"(score={contradiction.conflict_score:.2f})"
                ),
                source="km_contradiction",
                track="core",
                priority=priority_map.get(contradiction.severity, 3),
                estimated_impact=impact_map.get(contradiction.severity, "medium"),
                metadata={
                    "contradiction_type": contradiction.contradiction_type.value,
                    "severity": contradiction.severity,
                    "conflict_score": contradiction.conflict_score,
                    "item_a_id": contradiction.item_a_id,
                    "item_b_id": contradiction.item_b_id,
                    "cycle_id": cycle_id,
                },
            ))

        return goals[:10]  # Cap at 10 to avoid flooding the queue


__all__ = [
    "FeedbackGoal",
    "FeedbackResult",
    "ImprovementQueue",
    "SelfImproveFeedbackOrchestrator",
]
