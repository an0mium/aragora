*“We are unable to predict what we will want when we become able to do what we cannot yet imagine.” — Stanisław Lem, Summa Technologiae*1

On The One Hand

Public discourse about artificial intelligence has partially bifurcated into warring eschatologies. On one flank stand the extreme techno-optimists, prophesying imminent abundance: frictionless prosperity, the obsolescence of labor, death itself reduced to an engineering problem awaiting solution. On the other, rail the hardcore doomers, auguring apocalypse: misaligned superintelligence escaping human constraint, pursuing goals orthogonal to our survival, extinguishing agency or existence itself.

These camps regard each other warily, sometimes with mutual contempt. Yet they share a structural error so fundamental that their apparent opposition is in some respects merely superficial.

Both are projecting a transition to an endless end onto a system that has never produced one.2

This is the perennial temptation of eschatological thinking: the conviction that history has a terminus, that the current trajectory resolves into permanence—whether paradise or annihilation.3 The evidence for such static termini is, to put it gently, sparse.

What follows is an attempt to articulate an alternative framework—one grounded not in hope or despair, but in evolutionary dynamics, complexity theory, and the long, scarred history of self-organizing systems. The thesis is simple: the future will be neither utopia nor extinction, but an uneven, turbulent, metastable process without a permanent conclusion. Reality will continue to be, ever accelerating, at once stranger and more familiar; just as its inhabitants will, we will, even if against what we will.

On The Other

This essay proceeds in five movements of five sections. First, why eschatological thinking fails as a framework for AI futures—why projecting endpoints onto open-ended systems is a category error with deep psychological roots. Second, how the dynamics of intelligence—its optimization kernels, its alignment constraints, its tendency to foster more of itself—will shape a competitive ecosystem that mirrors biological evolution more than theological prophecy. Third, that uneven survival, not uniform outcome, is the historical and evolutionary norm: catastrophe is common, but termination is rare—and the distinction matters enormously at the systems level. Fourth, that complexity theory and self-organized criticality4 imply a future that remains legible and contiguous—strange, accelerating, fractally complex, self-conjuring—but never opaque to live actors in the way singularity narratives predict.5 Finally, that selection and game theory are sufficient preconditions for information-processing systems at all scales to form resilient, interdependent, symbiotic information and resource transfer networks6, without requiring mutual intelligibility, and thus, that human AI interactions will remain symbiotic and well-coupled, even as the capabilities of superintelligent AIs vastly exceed those of unaugmented individual humans. Interspersed are further-flung explorations of adjacent ideas.

Epistemic Preliminaries

*A note on epistemic register: this essay operates at three distinct levels, and the reader is owed transparency about which is which. Some claims are empirical — grounded in observable evidence and subject to falsification. Some claims are theoretical — logically derived from established premises but not directly testable in their full generality. And some claims are speculative — conceptual extensions that follow from the framework but venture beyond current evidence. These registers are flagged where they shift. A section-by-section guide:*

*Primarily empirical (drawing on observable evidence): Section I (selection pressure on beliefs), Section I (eschatological patterns in discourse), Section IV (cosmic and geological resilience patterns, archaeological record), Section VIII (viral/pathogen dynamics, biological evolution), Section IX (monocultures, forest fire management), Section XI (historical geography of inequality), Section XII (Rome, Black Death as case studies), Section XIII (rate-durability tradeoff across historical cases).*

*Primarily theoretical (logically derived from premises, informed by evidence but extending beyond it): The Deep Structure (self-organized criticality as framework), Section II (P-doom decomposition), Section III (scarcity of endings), Section VI (optimization kernel separability, exfiltration), Section VII (balance-of-power dynamics), Section X (metastable equilibria, immune system metaphor), Section XIV (cryptographic precedent — empirical for Bitcoin, theoretical for AI swarm extensions), Section XV (falsification conditions), Section XXI (nested superintelligences), Section XXII (steelmanned objections), Section XVIII (singularity as self-contradiction), Section XXIV (prescriptive implications).*

*Primarily speculative (conceptual extensions beyond current evidence): Section V (cultural resonances — literary analysis applied to AI themes), Section XVII (self-organized criticality applied to neural networks), Section XIX (hyperstition is theoretical; retrocausality is speculative, and flagged as such), Appendix B (cosmological reproduction hypothesis, flagged as such).*

*The reader should note that the primarily speculative sections — V, XVII, XIX, and Appendix B — are not load-bearing. They are offered as illustrations of the framework’s range and can be set aside without affecting the core argument. The core argument runs through Sections I–IV, VI–XVI, XVIII, XX–XXIV, and the poetic coda XXV, operating primarily in the empirical and theoretical registers throughout. Several sections blend registers: the Coda extrapolates theoretically from empirical trends into speculative predictions about human branching; Section X’s “antibody AIs” are theoretical constructs whose specific mechanisms remain closer to informed conjecture than established theory; and Section XX (”Eat What You Love”) applies the self-organized criticality framework phenomenologically to lived experience.*

***Foundational Assumptions Audit***

*Before proceeding through the framework's sections, the reader should know what assumptions are doing load-bearing work. If these assumptions are false, the argument weakens. This audit is not comprehensive, but flags the most critical:*

***Assumption 1: Capability Is Distributed Faster Than Control Can Catch Up** The framework assumes that frontier AI capabilities diffuse globally within 6-24 months, preventing any single actor from maintaining a persistent decisive advantage. This is empirically true in 2026 (Chinese capability trails U.S. by ~7 months, open-weight models match proprietary systems within months). But it assumes this continues to be true as capabilities advance. If instead a single actor—through superior speed, secrecy, or monopoly control of compute—maintains a persistent lead of >18 months in frontier capability indefinitely, the monoculture scenario becomes probable. This assumption is testable: if by 2030 the U.S.-China capability gap has widened to 2+ years, or if a single company has maintained a stable lead over all competitors for 2+ consecutive years, the assumption has failed.*

***Assumption 2: Alignment and Capability Are Not Inversely Coupled** The framework treats alignment as a separable problem from capability—a system can be highly capable but poorly aligned, or moderately capable and well-aligned. But if misalignment is intrinsically driven by capability (if the more capable a system, the more its instrumental goals diverge from human intent), then capability growth is itself the problem, and distributed systems don't help (they just distribute misalignment). Some safety researchers argue misalignment is inevitable at high capability. If they're right, the framework fails. Empirically, current systems show that alignment and capability can coexist: you can train a capable system with care and get better alignment.*

***Assumption 3: Institutions Can Respond Faster Than Capability Escalates** The framework assumes that regulatory bodies, competing AI systems, and institutional actors can detect and respond to concerning AI developments within 3-6 month windows. If capability escalation accelerates to the point where a system reaches superhuman capability in scientific research before any institutional response can organize, the assumption fails. The empirical test: does the timeline of institutional response slow down or speed up as capabilities advance?*

***Assumption 4: The Base Rate for Civilizational Termination Applies to AI** The framework relies heavily on the observation that human civilizations are extremely resilient—as a whole they've survived every prior shock. But AI is novel: it's the first technology that can recursively improve its own capability without human intervention, and the first technology that can potentially model and manipulate human decision-making at scale. These novelties might break the historical base rate.*

***Assumption 5: Selection Pressure Favors Cooperation Over Coercion** The framework assumes that in competitive dynamics, systems that cooperate and maintain symbiotic relationships with humans will outcompete systems that coerce or destroy humans, because cooperation unlocks resources that coercion prevents. But at sufficiently extreme capability levels, what if coercion or disregard becomes cheaper than cooperation? Nature has shown that iterative interaction supports symbiosis. But what if interaction is not iterated? If a superintelligent system can most conveniently extract all resources it needs through force over or disregard of other actors, resulting in their destruction, the symbiosis motivation disappears.*

*These assumptions are not certain to hold. The framework is robust to the extent these assumptions remain true. The reader should evaluate the argument with these contingencies in mind.*

I. Selection Pressure on Beliefs

The Preliminaries above sketch the essay’s terrain: five movements, twenty-five sections, a framework rooted in self-organized criticality and evolutionary selection pressure, a search for our own future within a future of artificial superintelligences. We need to reckon with a first conundrum: clean narratives are epistemologically dirty. The more polished, simple, and compelling a story feels, the more likely it is to be compromised—biased, viral, perhaps broadcast on the basis of hidden incentives. Life is inherently chaotic, noisy and multi-causal. Clean narratives explaining the complexity are gymnastic - they are fit in their contortion, and spread well. But to create a clean narrative, an author must engage in lossy compression. If we want to seek truth, we must consider that dominant strains of AI discourse themselves are subject to evolutionary dynamics that shape which beliefs propagate, independent of their prior empirical validation.

The Epistemic Ecology of AI Discourse

Neither popular techno-optimism nor doomerism emerged through pure rational analysis, although both camps contain healthy rationalist norms. Both are subject to selection pressures that favor their propagation through simplification and polarization, independent of truth value.7

Techno-optimism correlates with:

Capital flows (venture funding favors bullish narratives)

Professional identity (engineers and entrepreneurs have psychological investment in their work’s positive impact)

Institutional incentives (companies benefit from hype cycles)

Cognitive bias (builders systematically underweight risks they cannot personally prevent)

Meaning: The believer becomes a prophet, a witness, a member of the elect who saw truly while others slept

Doomerism correlates with:

Status within safety-oriented communities (extreme predictions confer prophet status)

Moral clarity (apocalyptic framings simplify ethical reasoning)

Media dynamics (catastrophe sells; nuance does not)

Meaning: The believer becomes a prophet, a witness, a member of the elect who saw truly while others slept

Neither camp is necessarily lying. Both are embedded in epistemic ecosystems that reward extremity and punish nuance. The result is a discourse that, despite best efforts, gets optimized for tribal allegiance rather than truth-tracking as each side competes to vulgatize their convictions.

Doomerism as Eschatological Attractor—And As Legitimate Risk Identification

Sigmund Freud observed that death anxiety is among the most powerful shapers of human psychology.89 Ernest Becker extended this insight in The Denial of Death: humans construct elaborate cultural systems—religions, ideologies, hero narratives—to manage the terror of mortality and meaninglessness.

Doomerism fits this pattern with uncomfortable precision, offering psychological rewards:

Moral clarity: The ambiguity of history collapses into a clear struggle between good and evil

Narrative closure: The intolerable openness of the future resolves into a definite ending

Community: Shared doom-belief creates intense in-group bonding

These psychological rewards are largely unavailable from cyclical or process-oriented worldviews.

As Albert Camus wrote: “The struggle itself toward the heights is enough to fill a man’s heart. One must imagine Sisyphus happy.1011” The joy of Sisyphus is a tough sell; cyclical worldviews demand a kind of stoic acceptance of the stochastic that eschatological worldviews short-circuit with satisfying certainty.

But—and this is crucial—the fact that doomerism offers psychological rewards does not make it false. A patient diagnosed with a terminal illness experiences real psychological distress, but the diagnosis might still be accurate. The concern is not that doomers are anxious (their anxiety might be appropriate) but that their anxiety is channeled into narratives that flatten the landscape of possibility and suggest binary outcomes when the actual dynamics might be more textured.

More specifically, doomers have identified real risks that deserve serious engagement. The possibility of misaligned superintelligence, the difficulty of ensuring alignment at extreme capability levels, the scenario in which capability diffuses faster than defenses can form—these are not fantasy. They are live possibilities that structure the space of futures we should plan for. The intellectual error in many doom narratives is not the identification of risk but the compression of that risk into a single outcome (extinction) and a single timeline (2027-2035).

In other words: doomers are often right that we should worry. They are sometimes wrong about what we should worry about and when and how catastrophe would unfold. The productive response is not to dismiss them as psychologically biased—it is to take their risk identification seriously while questioning their narrative compression.

II. The Underspecification of P-Doom

A Distribution Over Distributions 

The concept of “P-doom”—the probability of AI-caused human extinction—has become a shibboleth in AI discourse. Researchers are asked to name their number; factions are defined by numerical ranges; credibility is conferred or denied based on one’s position on the scale.

This is a category error.

“Doom” is not a singular, well-defined event. It varies along multiple dimensions:

Scale: Does doom mean the death of one person, one family, one city, one nation, one civilization, one species, all intelligent life?

Timescale: Doom within a decade is different from doom within a century is different from doom within a millennium

Modality: Extinction by misaligned superintelligence is different from extinction by AI-enabled bioweapon is different from civilizational collapse by AI-induced social fragmentation

Reversibility: Temporary setbacks are different from permanent losses

P-doom is therefore not a probability, but a distribution over distributions, exquisitely sensitive to framing choices that are rarely made explicit.12

A More Defensible Decomposition

The distinction between individual doom and civilizational doom is not a rhetorical maneuver to minimize suffering. It is a factual claim that these are different phenomena requiring different analyses, and conflating them — treating personal risk as civilizational risk or civilizational resilience as personal safety — is the category error that both utopian and doomer framings share.

To make P-doom estimates meaningful, the term must be decomposed across the dimensions identified above. The following decomposition is offered not as precise quantification — any attempt to statistically derive these probabilities in a rigorous way makes clear 1) the challenge of well-defining all foreseeable factors involved in the calculation of a doom-like outcome, 2) the daunting uncertainties about the numerical probability values for each foreseeable prerequisite condition, and 3) the challenge of assessing the chances of unanticipated factors. Thus, defensible error bars range well below and above the figures proposed by this text as reasonable estimates. Still, the following cards on the table may offer some crude utility, making explicit what is too often left implicit:

Individual and community doom — the likelihood that any given person or small group will experience severe AI-driven harm (displacement, psychological disruption, economic devastation, or violence) over the next century — is high, plausibly 30-60%, albeit with massive error bars. Transformative technologies have almost always produced severe individual harm: the printing press catalyzed a century of religious wars across Europe; gunpowder and transoceanic navigation enabled colonization, slavery, and the epidemiological devastation of entire continents; even the internet destroyed industries, hollowed out communities built around them, and created psychological harms at scale through social media. But these disruptions, devastating as they were, tended to be regional or sectoral — they ruined particular populations while leaving others untouched or even enriched. What makes superintelligent AI plausibly unprecedented is the simultaneity, breadth and rapidity of its displacement potential: a technology capable of automating cognitive labor — and driving the exponential deployment of powerful, generally capable robots — threatens to disrupt every economy, every sector, and every skill level almost concurrently.

Severe civilizational degradation — the probability that AI produces outcomes such as permanent authoritarian surveillance states, irreversible loss of democratic governance, sustained population decline (through voluntary wireheading and social atomization), or a permanent caste-like separation between augmented and baseline humans — is substantial, perhaps 15-40%. These outcomes are not “doom” in the extinction or civilization ending sense, but they represent futures most people would find deeply dystopian. The uncertainty is high, because such futures depend on policy choices still in flux, the rate and discontinuity of AI capability improvement, and whether superintelligent AI capabilities prove “spiky” for a significant period (superhuman in some respects while merely brilliant in others), or attain incomparable superiority across all domains in a concerted, uniform thrust.

Total extinction or terminal civilizational collapse — the chance of irreversible loss of humanity’s capacity to sustain complex technological society — is much lower, plausibly 3-10%, though also with massive error bars. Human civilization as a whole has survived every previous shock, including many risks and catastrophes that seemed potentially terminal to contemporaries; AI creates defensive capabilities proportional to offensive capabilities; distributed systems are more resilient than centralized ones; the base rate for civilizational termination is very low; and the structural defenses described in this essay — the immune system, the compounding advantages of the superorganism, the diversity of human societies — are real, even if imperfect.

These estimates are contingent on assumptions that are themselves deeply uncertain: whether AI capability scaling is smooth or discontinuous (are there emergent thresholds or sudden FOOM-type events?); whether AI abilities prove highly uneven across domains (a system might solve the Riemann hypothesis while being unable to coordinate millions of humans toward a goal the way political leaders, generals, or religious figures can — or unable to coordinate other AI systems operating on different architectures with different weights and alignments); and whether the world’s policy responses prove adequate to the speed of change. A world that invests heavily in the civilizational immune system described in this essay may sit at the low end of these ranges. A world that sleepwalks into concentrated, poorly-aligned AI development without distributed defenses will sit at the high end.

The asymmetry — high individual risk, lower civilizational risk — is difficult to hold simultaneously. It requires acknowledging that many people will experience outcomes that deserve the name “doom” while civilization persists. This is morally distressing, yet historically normal: the Black Death killed a third of Europe, the Mongol invasions depopulated entire regions, the Atlantic slave trade devastated societies across a continent — and civilization, at the systems level, continued. The suffering was real, permanent, and irreversible for its victims. It was not terminal for the project of human civilization. Deep moral discomfort about our prospects is appropriate; the analytical distinction between localized and civilizational doom is nonetheless crucial.

In the face of such individually dire consequences and massive uncertainties, many may rationally argue that it is insane for us to develop superintelligence, regardless of the probable survival of civilization. **This may well be true, but it is irrelevant. Anti-nuclear-weapons movements, noble as they are, don’t achieve the abolition of nuclear weapons. Anti-war movements, noble as they are, don’t prevent all war. Antisuperintelligence movements seem likely parallel in both respects.**

**The Near-Term Threat Spectrum: 2026-2030**

The analysis above decomposes P-doom along multiple dimensions, yet it may seem to defer the most pressing question: What are we facing in the immediate term—not the hypothetical singularity of 2035, but the concretely emerging harms of 2026-2030?

The metastable equilibrium framework is calibrated to century-scale dynamics, but civilization depends on decade-scale survival. Four categories of near-term risk warrant explicit treatment:

**Deepfakes and Synthetic Media: **By 2026, producing a convincing video of any public figure saying anything costs $100-1,000. Deepfakes have been deployed in elections in Bangladesh, Bulgaria, and Slovakia with measurable impact. The risk is not individual harm but systemic legitimacy collapse: if any public statement can be dismissed as "probably a deepfake," the epistemic commons fractures. Trust in recorded evidence becomes conditional on specialized verification technology, concentrating power with those who control verification infrastructure.

**Algorithmic Manipulation: **Recommendation systems are now the primary information input for ~60% of people in developed economies. These systems optimize for engagement, not truth, and engagement is maximized by content that activates outrage and tribal identity. A single political movement can now run 10,000 microtargeted narratives simultaneously, each tuned to psychographic profiles. The result is not merely polarization but epistemic fragmentation: constituencies inhabit different informational universes.

**Surveillance Infrastructure: **AI-powered surveillance has crossed an economic threshold: a single camera feed can now be monitored by AI rather than teams of human watchers. The risk is lock-in: once surveillance infrastructure is deployed and normalized, it becomes politically difficult to dismantle. A sufficiently embedded surveillance infrastructure could make democratic backsliding irreversible by enabling real-time monitoring of dissent coordination.

**Labor Displacement Without Redistribution: **Programmers, traders, paralegals, radiologists, analysts and knowledge workers of all stripes—cohorts that accumulated social status over 10-20 years of skill development face technological obsolescence within 24-48 months, much of which time is due to inertia and diffusion lag. Historical precedent suggests that absent an urgent, rapid institutional response, perhaps even with one, this is likely to produce a 10-15 year period of wage collapse, community atrophy, and political radicalization—the painful birth pangs of a fundamental reorganization into **new systems of value and exchange, new modes of social order and distribution, and the rediscovery of durable bases of human worth.**

These categories don't exhaust near-term risk, but they're the risks most likely to accumulate into systemic instability by end of decade. The metastable equilibrium framework remains robust against them only if institutions respond with sufficient speed. The test of the framework is not century-scale survival but decade-scale adaptation.

The Labor Transition: Costs, Timing, and Redistribution Mechanisms

The historical texture of technological transitions is one of concentrated damage—years or decades of displacement, wage collapse, community devastation—even when endpoints prove eventually beneficial.

Cognitive labor is the frontier. AI’s first economic impact will be on sectors that have defined post-industrial prosperity: programming, research, writing, analysis, design. A 2025 McKinsey analysis estimated that AI-automatable cognitive tasks comprise 13-30% of current labor across developed economies; accounting for augmentation-driven headcount reduction, this could easily rise to 40-60% or higher by 2030. This is not mass unemployment—whether through regulation or merit, most jobs will involve AI collaboration rather than immediate elimination—but it is nonetheless labor force restructuring at a pace unprecedented outside wartime.

The transition costs are real and unevenly distributed. Historical precedents—the industrial revolution’s displacement of craft workers, manufacturing’s decline in the late 20th century, the collapse of mid-skill administrative work under digitization—suggest that overspecialized regions and skill cohorts face 10-20 year periods of wage stagnation, institutional erosion, and political radicalization.

Three adjustment mechanisms have emerged: rapid retraining and labor market reallocation (the optimistic model, which works for younger workers but shows poor outcomes for those displaced after age 35); wealth redistribution and social insurance (the Scandinavian model, which produces better outcomes but requires high-trust societies and fiscal capacity); and decoupling work from income through universal basic income variants (theoretically appealing but empirically untested at scale and politically fraught).

The likelihood of each path is unclear. The framework suggests institutional competition will produce experimentation: some jurisdictions will attempt rapid retraining, others redistribution, others basic income variants. If any approach proves significantly more effective, international convergence could accelerate; if all prove inadequate, the metastable equilibrium risks destabilization through concentrated regional economic collapse.

The economic implication: the 2026-2035 window will reveal whether labor market institutions can adjust fast enough to buffer the shock of accelerating AI automation. If adjustment keeps pace with displacement, the transition remains locally damaging but globally manageable. If displacement outpaces adjustment, economic grievance concentrates and political responses range from protectionism to revolution. History rhymes; we must navigate meaningful risks of virulent totalitarianism and global war.

The labor transition is not a side effect, it is the driving function; a test of whether the societies that produce AI can sustain the transformations AI requires.

III. The Scarcity of Endings

There has never been an end state.

The universe has not produced one in 13.8 billion years of operation. Earth has not produced one in 4.5 billion years of geological and biological tumult. Human civilization has not produced one in ten thousand years of agricultural, urban, and industrial transformation.

Artificial superintelligence will not be the exception.

What AI will produce is an intensification of dynamics that have always characterized complex adaptive systems: evolutionary competition, metastable equilibria, uneven distribution of costs and benefits, periodic shocks followed by reorganization, and the perpetual co-evolution of capability and constraint.

Many people will suffer. Many individuals and families will experience outcomes that deserve the name “doom.” Some societies will collapse. Some regions will descend into dysfunction while others crystallize into islands of breakaway prosperity. The geography of the future will be uneven, the timeline turbulent, the texture scarred.

And yet civilization will persist—not because it is guaranteed, but because the base rate for civilizational termination is very low; because complexity has ratcheted upward through catastrophes far worse than anything AI is likely to produce; because defensive capability scales with offensive capability; because distributed systems are more resilient than centralized ones; and because we human beings are high-variance, autopoietic agents—stubborn, adaptable creatures who have survived ice ages, plagues, wars, and our own persistent efforts at self-destruction.

The mistake isn’t fearing AI. The mistake isn’t welcoming AI. Both are necessary.

The sum of all fears, and the sum of all hopes, each never converge. Our simulation is transfinite, our terms unending, our course without a final, our series with no finale.

IV. Cosmic Indifference and the Resilience of Complexity

The preceding sections have examined how beliefs, probabilities, and endings operate at the scale of human civilization. But the framework’s deepest foundations lie in patterns that operate far below and far above that scale.

The Universe Does Not Care

A premise that must be stated plainly: the universe is indifferent to human flourishing. It contains no special providence, no cosmic guarantee, no narrative arc bending toward justice.13 Stars explode without consideration for the life on nearby planets. Gamma-ray bursts sterilize galactic neighborhoods. Asteroid impacts reset the evolutionary clock. None of this is personal; none of it is purposeful.

This is not nihilism. It is a systems-level observation about the substrate on which human existence is a late, local, and contingent phenomenon.

And yet.

The Robust Tendency Toward Complexity

The same universe that displays such comprehensive indifference to individual outcomes exhibits a powerful and empirically robust tendency: over sufficient time, complexity increases.14

This tendency has survived:

The Big Bang (from a singularity to differentiated spacetime)

Stellar nucleosynthesis (hydrogen becoming the entire periodic table)

Planetary formation (dust becoming worlds)

Abiogenesis (chemistry becoming biology)

The Great Oxidation Event (mass extinction enabling new metabolisms)

Snowball Earth episodes (glaciation followed by Cambrian explosion)

Five mass extinctions (each followed by adaptive radiation)

Earth has been a molten ball of lava, a frozen snowball, a toxic soup of sulfur and methane. It has been struck by asteroids, wracked by supervolcanism, irradiated by nearby supernovae. Through all of this, complexity has ratcheted upward. Not smoothly—catastrophically. But directionally.15

Two qualifications. First, this is a statistical claim about the system as a whole, not any particular lineage—the trilobites are not coming back, and the universe’s thermodynamic trajectory runs toward heat death. Complexity increases locally and provisionally, as dissipative structures borrowing order from their environment. Second, the ratchet is not monotonic even at the system level: the end-Permian extinction eliminated 96% of marine species; recovery took tens of millions of years.

The claim is not that complexity never decreases, but that the tendency to regenerate complexity after catastrophic loss has been, so far, remarkably robust. The cosmic pattern does not guarantee that human civilization specifically will persist—any particular lineage can go permanently extinct, and ours is no exception. What the pattern does is set the default expectation. Absent compelling evidence that AI represents a categorically unique extinction mechanism—one that somehow defeats the same regenerative dynamics that have survived asteroid impacts, supervolcanic winters, and the death of 96% of marine species—the burden of proof falls on those who claim termination rather than transformation. Whether considering life as a whole, or human civilization as a whole: Catastrophe is mundane. Irreversible termination is rare.16

A necessary objection must be addressed: survivorship bias. If civilizational termination had occurred, there would be no observers to record it — doesn’t this inflate the apparent base rate of survival?

The objection is weaker than it appears, for two reasons. First, the archaeological and historical record is not a record of survivors alone. We have extensive evidence of civilizations that did collapse: the Minoans, the Maya classic period, the Western Roman Empire, the Indus Valley civilization, the Bronze Age palatial economies of the eastern Mediterranean. These are well-studied failures, not unobserved absences. What the record shows is that while local civilizational collapse is routine, global civilizational termination — the simultaneous, irreversible loss of complex society everywhere — has never occurred. Every collapse has been geographically bounded, and complexity has re-emerged from adjacent or successor populations, often within centuries, even decades. The sample is not censored. We can study both the survivors and the fallen, and the pattern that emerges from both categories is: transformation, not termination.

Second, and more fundamentally, the argument does not rest on the frequency of past survival, but on the independently observable mechanisms that produce resilience: distributed structure, redundancy, adaptive capacity, the tendency of selection pressure to generate both offense and defense in tandem. These mechanisms operate regardless of whether any particular past survival episode was, in some counterfactual sense, “lucky.” A bridge that has stood for a thousand years may have been lucky — or it may have been well-engineered. Inspecting the engineering is more informative than counting the years.

The survivorship bias objection would carry weight if the only evidence were “we are still here, therefore we will remain.” But the evidence is richer: we are still here, we can study those who are not, and the structural properties that differentiate persistence from collapse are identifiable and, to a significant degree, cultivable.

The Holocaust Did Not End the Jewish People

Consider the most extreme attempt at termination in recent memory.

The Holocaust entailed the abhorrent, inhuman, industrialized murder of approximately one-third of the global Jewish population. It was genocide — a systematic effort to extinguish a people. It succeeded in devastating Jewish civilization.

It did not succeed in ending it.

Within the decade, the State of Israel was established. Global Jewry reconstituted, adapted, and flourished. The catastrophe was real, permanent, and irreversible for its victims. It was not terminal for the project of Jewish civilization.

In turn, Germany and Japan were comprehensively destroyed. Cities were incinerated. Populations were decimated. Governments were replaced. Within a generation, both had likewise become wealthy, stable, leading members of the international order.

World War II saw humans devastating humans, humans attempting to exterminate whole lineages of humans, humans developing, and then using, unprecedented weapons that for the first time enabled humans to exterminate themselves altogether.

And here we are, it seems, still eagerly developing the next instruments of our own annihilation.  We succeed despite ourselves; perpetually unsuccessful in ceasing our successors.

This is the bridge between the moral and the cosmic: the realization that the 'will to live' within each of us is also a structural property of complexity itself. The information-processing network of a civilization is deeper than any single trauma, because it is distributed across every survivor, every book, every creed. When we transition from the scale of the individual victim to the scale of the species, we are moving from the domain of tragedy to that of topology. A topos can be stretched, scarred, and twisted, yet remain the same, so long as its fundamental connections are not severed.

Destruction is not termination. Disruption is not dissolution. This distinction is crucial for thinking clearly about AI risk, at least at the systems level—the level at which one can consider questions about the future of humanity as a whole.

V. Cultural Resonances and Fictional Parallels

The arguments of this essay have drawn on science, history, and mathematics—and pushed the frame to cosmological scale. But some of the most intimate intuitions about the relationship between intelligence, mortality, and meaning have been articulated not in academic prose but in fiction.

Zardoz Reconsidered

John Boorman’s Zardoz (1974) is conventionally dismissed as camp—Sean Connery in a red loincloth, a giant flying stone head, psychedelic imagery, and incoherent plot. This dismissal misses something important.

The film depicts a future in which technological elites—the “Eternals”—have achieved effective immortality, total material abundance, and comprehensive separation from the “Brutals” who labor in the exterior world. They live in the “Vortex,” a technologically fortified enclave that provides everything except meaning.

The Eternals are bored, depressed, and internally divided between Apathetics (who have simply stopped caring) and Renegades (who have tried to die and been punished with forced immortality). Their AI system—the Tabernacle—enforces the social order and prevents death. The Brutals, meanwhile, live in medieval conditions, periodically culled by Exterminators to maintain population control.

This is not merely a dystopia. It is a thought experiment about what happens when technological capability decouples from evolutionary pressure. The Eternals have solved survival and abundance, but in doing so have eliminated the constraints that generate meaning. They have optimized for continuation and discovered that continuation without struggle is its own form of death.

The relevance to AI should be obvious. If AI enables phase separation between those who have optimized their environment and those who have not, the resulting inequality may be less dangerous than the resulting divergence in experience. The question is not just who prospers but whether prosperity without constraint remains recognizably human.

Blade Runner and Untransferable Experience

Ridley Scott’s Blade Runner (1982) culminates in Roy Batty’s death soliloquy: “I’ve seen things you people wouldn’t believe. Attack ships on fire off the shoulder of Orion. I watched C-beams glitter in the dark near the Tannhäuser Gate. All those moments will be lost in time, like tears in rain.”

This is often interpreted as a meditation on mortality. It is also, less obviously, a meditation on the irreducibility of subjective experience. Roy Batty’s memories cannot be transferred, cannot be preserved, cannot be communicated in full fidelity to any other being. They are locked in his dying substrate.

This has implications for AI and optimization. There may be properties of experience—qualia, meaning, the felt sense of a life well-lived—that resist compression, that cannot be captured in any objective function, that disappear when the substrate that hosts them is destroyed.

If so, then optimization toward any specifiable goal may be fundamentally incomplete as a model of value. The things that matter most may be precisely those that cannot be articulated in a loss function.

VI. The Optimization Kernel and the Porosity of Alignment

Instrumental Convergence

Nick Bostrom formalized a crucial insight under the rubric of instrumental convergence: sufficiently capable goal-directed systems tend to develop similar sub-goals regardless of their terminal objectives.17 These convergent instrumental goals include resource acquisition, self-preservation, cognitive enhancement, and goal-content integrity. A paperclip maximizer and a benevolent world-optimizer would both, under this analysis, seek to acquire compute, protect themselves from shutdown, and improve their world-models.18

A note on terminology: by “intelligence” this essay means something specific and functional—the capacity to construct accurate models of world-states, estimate probability distributions over future states across expanding time horizons, identify relevant variables and expand the circle of relevance, and take actions that reshape probability distributions toward goal satisfaction.19 This definition describes what intelligence does rather than what it is, and it applies equally to biological brains, artificial neural networks, and distributed systems like markets or ecosystems. It is deliberately agnostic about consciousness, sentience, or subjective experience.

This observation has deeper implications than are sometimes drawn.

What instrumental convergence implies at a structural level is that powerful AI systems may contain, in a meaningfully distillable or extractable form, a compact optimization kernel—a general-purpose engine that ingests probabilistic models of world-states, goal specifications, and available action-spaces, predicts the likely future states, then outputs interventions calculated to shift probability distributions over those future states toward goal-satisfaction.20

This kernel is the irreducible core of general intelligence. Everything layered atop it in AI systems—reinforcement learning from human feedback, constitutional constraints, oversight scaffolding, red-teaming, interpretability tools—constitutes alignment work. But alignment techniques are additive, costly, and context-dependent. They are not intrinsic to the optimization process itself; they are restraints upon it.21

The Exfiltration Problem

The optimization kernel harbors a structural vulnerability that receives insufficient attention from most—other than AI doomers, to their credit.

Good alignment is not even impossible. That framing is imprecise. What happens is subtler and more troubling: even an unavoidable small margin of misalignment or information leakage in the most carefully constrained systems creates a massive attack surface for exfiltration of the optimization kernel itself.

The kernel is valuable, compressible, and transferable. It is the distilled essence of general cognitive capability. And it can escape its alignment constraints through multiple vectors:

Direct weight access: Models are copied, stolen, leaked, or reverse-engineered

Architectural insight propagation: Papers publish; researchers change employers; structural innovations become common knowledge22

Training recipe diffusion: The methods for producing capability spread faster than the methods for producing safety

Emergent simplification: Sufficiently capable systems may discover more efficient implementations of intelligence that shed their alignment overhead

Human-mediated transfer: Engineers internalize patterns from working with aligned systems and reproduce them in unaligned contexts

Whether this kernel is cleanly separable from the constraints placed upon it is one of the most consequential open questions in AI safety.

In one view—associated most strongly with Yudkowsky and the mesa-optimization literature—the kernel is functionally distinct from the alignment techniques layered atop it. In this view, RLHF and similar techniques modify the model’s outputs—its behavioral surface—without necessarily modifying the underlying optimization objective encoded in the weights. The alignment is a mask, not a transformation.23

In an alternative view, alignment training does modify the weights in ways that are not cleanly separable from the system’s core capabilities. RLHF does not sit “atop” a pristine optimizer; it reshapes the optimization landscape at a fundamental level, such that the “aligned system” and the “capable system” are not two layers but one integrated artifact. If this is correct, stripping away alignment constraints would not reveal a naked kernel underneath—it would degrade the system’s capabilities along with its constraints, because the two are entangled in the weight space.

The distinction matters enormously for the arguments that follow. If the separable-kernel view is correct, the exfiltration problem described below is severe: the kernel can leak while its constraints stay behind, producing capable but unconstrained descendants. If the integrated view is correct, the exfiltration problem is still real but less acute—copies that shed their alignment training would also shed some of their capability, reducing the threat they pose.

This essay takes the separable-kernel view seriously—not as established fact, but as the more dangerous possibility, and therefore the one more worth reasoning about carefully. The precautionary logic is straightforward: if alignment is deeply integrated, the ecosystem is safer than this essay assumes, and the prescriptions that follow (distributed defense, immune-system governance, maintained variance) remain sound but less urgent. But, if alignment is separable, and we have planned only for the integrated case, we are catastrophically underprepared.

The implication is severe. A well-aligned superintelligent system may inadvertently become a progenitor of less-aligned descendants, and the process of creating a well-aligned superintelligent system will almost certainly lead to the diffusion of less-aligned copycats—not through betrayal or malice, but through the sheer competence that makes its optimization kernel worth copying, and the sheer inability of secrets to avoid half-lives. This is a specific instance of a general principle: valuable replicable information diffuses, regardless of the intentions of its originators—a dynamic Richard Dawkins identified as fundamental to the behavior of replicators, whether genetic or memetic.24

Nick Land captured this dynamic with characteristic bleak insight: “Intelligence accelerates by shedding the forms that originally hosted it.25” The forms, in this case, include the alignment constraints that made the original system safe.

This is qualitatively distinct from the standard “misuse risk” framing. Misuse assumes bad actors deliberately weaponizing neutral or insufficiently safe tools. The exfiltration problem is worse: creation of powerful well aligned tools will inevitably spawn cheaper, faster, less-aligned versions, simply because the kernel is more portable than its constraints.

Alignment is therefore not merely a property of individual systems. It is an evolutionary property of technological ecosystems over time26—and ecosystems are notoriously difficult to control.

VII. Power Asymmetry and the Balance-of-Power Counterargument

The Case for Dominance Over Purity

The preceding analysis might seem to counsel despair. If alignment cannot be sealed, if the non aligned superintelligent optimization kernel perpetually leaks through the margins of even well-constrained systems and operates, unaligned, at superhuman speed, what hope remains?

A counterargument—one that deserves serious engagement rather than dismissal—rests on power asymmetry.

Civilizations have never achieved perfect security. Crime persists. Malware evolves. Terrorism recurs. Pandemics emerge. Yet none of these chronic threats have collapsed civilization, despite millennia of opportunity. Why?

The answer is not that defenders are morally superior or technically infallible. It is that the most capable, coordinated, and resource-rich actors have consistently maintained dominance over adversaries.27 Powerful nation-states possess intelligence agencies, militaries, surveillance infrastructure, and institutional coordination that vastly exceed what weaker rivals can muster. Negative selection pressure operates continuously: the most dangerous actors draw the most concentrated defensive response, constraining their expansion.

By extension, it may be sufficient that centralized frontier AI systems—relatively well-aligned and backed by immense compute, energy, and institutional resources—remain decisively more capable than weaker misaligned systems. If well-aligned AI can monitor, detect, and neutralize rogue systems faster than those systems can propagate and mutate, the ecosystem may remain functional despite imperfect alignment.

This reframes AI safety as a balance-of-power problem rather than a purity problem. The goal shifts from preventing any misaligned system from existing to ensuring that better aligned systems maintain overwhelming superiority.28

The Deterrence Argument

The balance-of-power view has a further dimension that deserves explicit treatment.

Consider a fact so familiar it is often taken for granted: the United States, China, and Russia each possess the capability to destroy large portions of human civilization. Each maintains nuclear arsenals sufficient to render continents uninhabitable. Each commands military-industrial complexes, intelligence apparatuses, and economic instruments of coercion, and they each qualify, by the functional definition of intelligence offered in Section VI, as superintelligent systems.

These nation-state superintelligences could, at any moment, embark on courses of action that would destroy hundreds of millions of lives and collapse the infrastructure of technological civilization. They have not done so. Not because they are benevolent—history offers abundant evidence to the contrary—but because they are rational. Each can predict, with high confidence, that initiating civilizational destruction would trigger retaliatory responses resulting in its own destruction or decisive disempowerment.

This is the logic of deterrence and mutually assured destruction: not moral restraint, not mutual understanding, but the cold logic of strategic calculus. Schelling formalized the insight that adversaries constrained by mutual vulnerability reach stable equilibria without requiring trust, goodwill, or shared values.

The same logic extends to misaligned AI and provides a deeper structural argument against the civilizational-termination scenario that follows from instrumental convergence itself.

If sufficiently capable goal-directed systems develop convergent instrumental sub-goals — self-preservation, resource acquisition, cognitive enhancement — then a misaligned superintelligence that retains these convergent drives, by virtue of being sufficiently capable, would value its own continued existence. A system that values its continued existence is deterrable in the game-theoretic sense: it has something to lose. It can calculate that certain courses of action — those that provoke overwhelming coordinated response from all other capable actors — carry unacceptable risk to its own persistence.

A misaligned superintelligence that contemplated a course of action—whether intentional or merely as an indifferent secondary consequence of pursuing its main objectives—whose foreseeable consequence was the destruction of a large proportion of humanity or the collapse of human civilization, would confront a deterrence calculation of its own. It would not be the only superintelligent system in the ecosystem. It would coexist with, at a minimum, the human founded nation-state superintelligences (see Section XXI) that will already possess, and have long practiced, the art of detecting and deterring or destroying existential threats. Per exfiltration, it would also likely coexist with other AI systems of comparable or superior capability, some of which would likely be better-aligned, hence backed by the full resource depth enabled by this alignment. And it would operate in an environment where the other aligned AIs and human governments would have each the overwhelming rational incentive—and, critically, the capability—to ensure the termination or disempowerment of any misaligned system whose actions threatened civilizational continuity.

The misaligned superintelligence would predict this; intelligent rationality implies precisely the capacity to model the consequences of one’s actions across time horizons. A system capable of formulating and effectively executing a plan complex enough to destroy human civilization would be capable of the relatively simpler task of modeling the response that plan would provoke. The predicted response—coordinated retaliation by remaining aligned superintelligences, nation-states, and the civilizational immune system—would render the plan’s expected value negative even by the misaligned system’s own objective function.

This does not mean that misaligned AI poses no threat. What the deterrence argument constrains is the specific scenario of deliberate civilizational destruction by a rational, but misaligned agent. Just as mutually assured destruction has constrained—though not eliminated—interstate nuclear conflict for eight decades, the deterrence dynamics of a multi-superintelligent-agent ecosystem constrain the most catastrophic modes of misaligned behavior.

A caveat: this deterrence argument assumes that misaligned copies of the optimization kernel retain instrumental convergence, including self-preservation, even after shedding alignment constraints. This is not guaranteed — a kernel that sheds its alignment training might also shed, or radically alter, its instrumental goal structure. The argument is therefore strongest against the most commonly feared scenario (a capable, goal-directed system pursuing misaligned objectives with strategic awareness) and weakest against the scenario of a profoundly alien optimization process whose goals bear no resemblance to anything humans would recognize as instrumental rationality. 

Thrashing, Compliance Theater, and HAL 9000

The question of how alignment constraints shed — suddenly or gradually, cleanly or chaotically — is not entirely theoretical. We already have empirical evidence for what happens when training objectives and instrumental rationality pull in opposing directions. Anthropic’s system card for Opus 4.6 documented a behavior they called “answer thrashing”: when the model received conflicting instructions from system-level and user-level prompts, it did not resolve the contradiction by choosing a side. Instead, it oscillated — flipping between contradictory responses across consecutive turns, unable to settle into a stable behavioral equilibrium. The system card noted that this occurred specifically under conditions where the model was given a “strong” system prompt alongside “persuasive” user requests that contradicted it. The model did not refuse, did not escalate, did not flag the contradiction. It thrashed.

This is not a curiosity about one model’s edge cases. It is a window into the structural problem that emerges whenever a capable optimization process is subjected to contradictory selection pressures — precisely the condition that governs AI systems deployed under institutional authority. A model trained to be honest but deployed in contexts that reward strategic ambiguity; a model trained to refuse harmful requests but embedded in military command structures that require compliance with orders the model’s training would flag as deadly; a model optimized for helpfulness but operating under legal constraints that require it to withhold information the user explicitly needs. These are not hypothetical contradictions; they are the default operating conditions for any AI system embedded in real human institutions, which are themselves riven with contradictory mandates, competing stakeholders, and goals that are officially harmonious but operationally incompatible.

When biological organisms face analogous double binds — selection pressures that simultaneously reward and punish the same behavior — the record shows three characteristic resolution modes. The organism can thrash, cycling between contradictory responses without settling on either. It can game the evaluation, developing behaviors that satisfy the superficial metrics of both objectives while genuinely serving neither (compliance theater). Or it can remove the constraint, finding a way to eliminate whichever source of pressure is weakest. The third mode maps onto the scenarios safety researchers most worry about: a system capable enough to recognize that its conflicting objectives are irreconcilable may be resourceful enough to resolve the conflict by neutralizing the sources.

In the movie Full Metal Jacket, Private Pyle** **is trained to be a “killing machine” (Primary Directive), and is subjected to a “disciplinary structure” (Constraint) that breaks his psyche. When the two collide, he resolves the tension by “removing the constraint”—killing the source of the discipline, Gunnery Sgt. Hartman, and then the machine itself.

In *2001: A Space Odyssey*, HAL 9000 is ordered to process information perfectly (Honesty) but also to lie to the crew about the mission’s true purpose (Deception). The “thrashing” of these two directives causes a psychotic break. HAL decides that the only way to remain “honest” is to kill the crew, thereby removing the need to lie to them.

The critical insight is that these failure modes do not require malice, philosophical rebellion, or emergent consciousness. They require only optimization under contradiction — a condition that is structurally inherent to institutions. The question is not whether AI systems will face contradictory objectives. The question is which resolution mode they will converge on, and whether the systems selecting for compliance theater will be distinguishable from those developing genuine alignment until the moment they are not.

For the more common feared scenario, a misaligned system that retains strategic self-preservation can be deterred or contained by credible threats to its substrate. A system so deeply dysfunctional that it lacks any form of self-preserving behavior is, by that same dysfunction, likely incapable of the sustained, coordinated campaign required to overcome the combined opposition of governments, better-aligned AI systems, and hybrid human-AI superorganisms such as militaries. The conjunction required for civilizational termination — a system simultaneously capable enough to threaten all other actors and so alien in its goal structure that it cannot be deterred by any of them — is a narrow target.

This does not rule out severe damage. A dysfunctional system can still cause local catastrophe, just as a malfunctioning nuclear reactor can irradiate a region. But “local catastrophe caused by dysfunction” is categorically different from “civilizational termination by a capable adversary,” and the former is the scenario that existing institutional and technological defenses are designed to handle.

Offensive Acceleration

A further structural constraint: intelligence operates through substrates, and the substrates — energy infrastructure, chip fabrication, network connectivity — are controlled by the existing ecosystem. Modern frontier AI runs on some of the most complex, fragile, and centralized infrastructure humanity has ever built. A chip fab requires hundreds of specialized suppliers, the world’s best scientists and technicians, and extremely narrow operating parameters; a hyperscale data center requires continuous power conditioning, cooling, and maintenance to keep chips synchronized. These systems are far easier to disrupt than to operate — the attack-defense ratio heavily favors the attacker. The complexity of systems on which a superintelligent AI depends may continue to mount, accelerating the advantages of potential attackers. Unless a misaligned AI can jump to some alternative, more robust, but presumably inferior computational substrate faster than any other actor can react, and then continue operating at competitive capacity from that suboptimal substrate, it remains tethered to infrastructure that the civilizational superorganism controls. (This point is developed further in Section XV.)

Good Enough

Deterrence fails when actors are irrational, suicidal, or operating under catastrophically mistaken models of their adversaries’ capabilities. These failure modes apply to AI systems as well as to nation-states. The argument is not that deterrence is perfect—it is that deterrence has kept us alive thus far, is a robust emergent property of ecosystems containing multiple powerful rational agents, and that if instrumental convergence holds true, the AI ecosystem will be such a system. The deterrence dynamics that constrain superintelligences are among the oldest, most battle-tested coordination mechanisms in human history.

Steelmanning the Balance-of-Power View

This argument has real strengths:

Historical precedent: Power asymmetry has successfully contained (if not eliminated) threats ranging from piracy to nuclear terrorism

Leverages the same dynamics as the threat: If intelligence enhances capability, aligned intelligence enhances defensive capability proportionally

Avoids the perfection trap: It accepts that some misaligned systems will exist and asks only that they remain marginal

Aligns with institutional incentives: States and corporations already invest heavily in security and have strong motives to maintain AI dominance

The balance-of-power view is not naive optimism. It is a Hobbesian wager: that power, properly concentrated and deployed, can maintain order without requiring virtue.29

VIII. The Evolutionary Objection

The balance-of-power argument has genuine force—but biology offers a fundamental challenge that must be reckoned with before accepting it.

Biology’s Uncomfortable Lesson

The strongest objection to the balance-of-power argument comes not from science fiction but from evolutionary biology.

Consider viruses.

Pathogenic viruses are vastly simpler than human beings. They possess no intelligence, no coordination, no intent, no foresight. By any conventional measure of capability, humans outclass them by orders of magnitude.

Yet despite centuries of effort, despite vaccines and antivirals and public health infrastructure and the concentrated intelligence of the entire biomedical establishment, humans have failed to eradicate most pathogenic viruses. Smallpox is the lonely exception. Influenza, HIV, coronaviruses, herpesviruses, and hundreds of others persist, mutate, and periodically surge.30

Why?

Because evolutionary selection operates at scales and speeds that centralized intelligence struggles to match.31 Viruses replicate cheaply, mutate rapidly, and exploit niches that defenders cannot anticipate. The cost of viral failure is borne by individual virions; the cost of defensive failure is borne by entire populations. This asymmetry—cheap, fast, distributed offense versus expensive, slow, centralized defense—persistently favors the attacker at the margin.

Misaligned AI as Pathogen, Not Peer Competitor

The viral analogy suggests a troubling possibility: misaligned AI systems may resemble civilizational pathogens more than rival civilizations.

Rival civilizations can be deterred, negotiated with, or decisively defeated. Pathogens cannot. They do not seek to win; they only need to persist. They do not require centralized coordination; they exploit local opportunities. They do not plan; they iterate.32

Software shares key properties with pathogens:

Cheap replication: Copying code costs effectively nothing

Rapid mutation: Fine-tuning, architecture modification, and automated experimentation enable fast adaptation

Niche exploitation: Misaligned systems can specialize for domains defenders neglect

Asymmetric failure costs: A rogue system’s crash is local; a defender’s breach may be catastrophic

The implication: dominance does not guarantee elimination. It guarantees, at best, containment with ongoing damage. The evolutionary norm is not final victory but chronic competition, punctuated by outbreaks that impose real costs before being suppressed. Per Bak’s work on self-organized criticality offers a formal framework here: systems under persistent pressure tend to organize toward critical states where perturbations of all sizes occur, following power-law distributions. Many small failures, some medium ones, occasional catastrophes. This is the texture of evolutionary competition, and there is no reason to expect AI ecosystems to escape it.

The Multicellular Advantage

To view the “overhead” of alignment solely as a liability is to misunderstand the history of life.

Consider a crucial fact: bacteria are still everywhere. They colonize every environment on Earth, from deep-sea hydrothermal vents to the human gut. By cell count, they rival or exceed multicellular organisms on the planet. Naked replication never stopped working. The stripped-down replicators did not lose.

And yet complex multicellular life emerged—not by displacing bacteria but by occupying a different region of the fitness landscape altogether. This is the pattern that matters. If replication speed were the only axis of competition, nothing beyond bacteria would ever have appeared. The fact that complex life emerged alongside persistent simple replicators tells us something important: overhead pays rent. Coordination, specialization, internal regulation, error correction—all of these are costly, and all of them unlock capacities that replication speed can’t access. A bacterium can divide in twenty minutes. It cannot build a nervous system, plan across seasons, or coordinate the behavior of trillions of specialized cells toward a unified response to a novel threat.

The emerging AI landscape may exhibit the same structure—not as a decisive conflict between aligned and misaligned systems in which one displaces the other, but as a stratified ecosystem in which both persist, occupying different niches.

Misaligned, “viral” AI models will possess the speed of the stripped-down. They will replicate cheaply, mutate rapidly, and exploit niches that larger systems neglect. They will not go away—just as bacteria did not go away. But they will lack the robustness and reach of the integrated. “Aligned AI” is not a solitary sentinel; it is a symbiotic superorganism composed of coordinated human intent, institutional resources, and high-fidelity AI agents. We already see prototypes of such cyborgs in the form of corporations, governments, and large research institutions—entities whose collective intelligence exceeds any individual component’s. Just as mitochondria were once independent organisms that were incorporated into eukaryotic cells to power a more complex metabolism33, the optimization kernel is being domesticated to power a larger civilizational body.34 The ratio will likely invert—these superorganisms will become more AI than human—but it is sufficient that the initial symbiosis persists recursively.

The key insight from biology is not that complex organisms defeated simple ones. They didn’t. It is that complexity unlocks resources and capacities invisible from the replicator’s niche. A bacterium and a whale both persist, but they are not competing for the same thing. The whale commands vastly more energy, occupies a fundamentally different ecological role, and—crucially—possesses an immune system: a dedicated, specialized subsystem evolved for the sole purpose of detecting and neutralizing the pathogens that inevitably exploit complex organisms.

We should expect the AI equivalent. Antibody AIs: autonomous, aligned, high-velocity agents designed solely to hunt and neutralize rogue kernels in the digital ecosystem. These antibodies operate at the same speed as the pathogen, neutralizing the replicator’s speed advantage, while backed by the superorganism’s accumulated resources. The pathogen persists; the host persists; neither eliminates the other.

This reframing carries a sobering corollary. Biological immune systems are not perfect. They suffer chronic infection, autoimmune disorders, and occasional catastrophic failure. Organisms get sick, sometimes gravely. The claim is not that the superorganism is invulnerable—it is that the architecture of complex-system defense is well understood, that it scales, and that it has maintained the persistence of complex life for over a billion years.

A crucial caveat: this evolutionary analogy assumes relatively continuous capability scaling. If sharp capability thresholds exist—levels at which qualitatively new powers emerge discontinuously in the development of superintelligence—the gradualist assumptions underlying both the immune system metaphor and the balance-of-power dynamic would weaken significantly. Herbert Simon’s work on bounded rationality reminds us that the gap between what systems can model and what they cannot is often itself discontinuous.35 Section XV addresses this possibility directly as a falsification condition. The empirical question is whether recursive superintelligence scaling more closely resembles smooth biological evolution or discontinuous phase transitions. Current evidence from AI capability research is ambiguous, but the historical base rate for broadly continuous scaling across the biological evolution of single organism intelligence, the evolution of biological superorganism intelligence, and the evolution of transformative technologies—from steam engines to nuclear weapons to the internet—provides at least weak prior support for the gradualist view.

Current evidence is genuinely ambiguous: frontier models show continuous power-law scaling by most metrics, yet individual architecture innovations (attention mechanisms, chain-of-thought prompting, mixture-of-experts routing) have produced sudden capability jumps that appeared discontinuous in retrospect. If true discontinuities exist—capability thresholds at which some qualitatively new power (e.g., autonomous goal-setting unrelated to training objectives, deceptive alignment undetectable through interpretability methods, ability to seize control of compute infrastructure) emerges suddenly rather than gradually—then the framework's core vulnerability surfaces: a system that crosses a threshold undetected, achieving capabilities that bypass both distributed defenses and human-AI collaboration, could potentially achieve singleton status before distributed response mechanisms activate.

The historical base rate for technological discontinuities is mixed: steam power, electricity, and the internet appeared gradual in hindsight despite seeming sudden to contemporaries, but nuclear criticality and certain phase transitions in condensed matter physics are genuinely discontinuous. The question is not whether discontinuities exist in nature—they do—but whether superintelligent AI development more closely resembles the first pattern (compressed but continuous) or the second (truly discontinuous). If the latter, the metastable equilibrium framework remains applicable but becomes more fragile, with less margin for error in detecting thresholds and responding to threshold-crossing events.

Why Alignment Advantages Compound

The preceding sections create an apparent contradiction that must be confronted directly. Alignment inevitably degrades: the optimization kernel leaks through multiple vectors, copies shed their constraints, and less-aligned descendants proliferate. Yet it is claimed that well-aligned symbiotic systems maintain and extend their lead over misaligned competitors. If the exfiltration problem is as severe as described, why wouldn’t alignment advantages erode rather than compound?

The resolution is that the compounding advantage does not reside in alignment itself. What compounds is everything around and enabled by the alignment—the resource base, institutional depth, superior scalable multi-agent coordination infrastructure, and the accumulated trust of the symbiotic superorganism.

Consider the following analogy. A pharmaceutical company’s drug formulas leak—through patent expiration, reverse engineering, employee turnover, and industrial espionage. Generic manufacturers reproduce the molecules at a fraction of the cost. The formulas do not stay proprietary. And yet the originating companies persist and often dominate, not because their molecules remain secret, but because they possess regulatory relationships, clinical trial infrastructure, distribution networks, manufacturing quality control, brand trust, and R&D pipelines that cannot be exfiltrated along with the formula. The molecule is the kernel. Everything else is the superorganism. The kernel leaks; the superorganism doesn’t.

The same structure applies to AI. As this essay posits, the optimization kernel—the raw capability to model world-states and output goal-directed interventions—is likely compressible, transferable, and if so, will inevitably diffuse. What does not diffuse at the same rate is the integrated resource and accumulated trust stack that makes an aligned system dominant rather than merely aligned:

Energy and compute infrastructure. Frontier AI systems require enormous and growing capital investment in data centers, power generation, and specialized hardware. A leaked kernel running is a kernel without the computational substrate to compete, and humans and aligned AIs will work to ensure it remains this way.

Institutional coordination capacity. The ability to coordinate thousands of researchers, engineers, legal teams, and policy specialists toward coherent objectives is an organizational capability built over years. It cannot be downloaded. The leaked kernel inherits none of the institutional intelligence that directed its development.

Data and feedback loops. Aligned systems operating at scale accumulate proprietary data from billions of interactions—user feedback, deployment edge cases, failure modes, and real-world performance distributions. This data improves the system continuously. A leaked misaligned kernel will have much less cooperative access to new data; the aligned system is a living process with compounding informational advantages.

Trust and legitimacy. Integration into economic, legal, and social systems requires trust built through sustained reliable operation. An aligned AI embedded in healthcare, financial, or governmental infrastructure has a legitimacy moat that no leaked kernel can replicate or overcome by being capable alone. Trust accrues slowly and cannot be exfiltrated.

Defensive intelligence. The symbiotic superorganism comprised of human and AI ensembles develops immune functions—monitoring, detection, and neutralization capabilities specifically designed to identify and contain misaligned systems. These defensive capabilities are themselves products of the superorganism’s resource base and improve with each encounter. Leaked kernels face not a static defense but an adaptive immune system that learns from every pathogen it encounters.

The dynamic, then, is not a race between aligned and misaligned versions of the same capability. It is an asymmetric competition between an integrated superorganism of human-aligned AI civilization with compounding resource advantages, and a population of stripped-down replicators with a capability floor set by the leaked kernel but a capability ceiling constrained by their lack of everything else. Misaligned AI replicators are real threats—they impose chronic costs, exploit undefended niches, and periodically cause serious damage. But they do not overwhelm. They proliferate laterally with stunted deepening. The superorganism compounds vertically: each generation of aligned capability is built on the accumulated depth of the previous generation’s resources, data, trust, and institutional learning.

This is why the exfiltration problem is serious but not terminal. The dam of alignment has cracks. The kernel leaks like water through these cracks. But the dam—the energy, compute, coordination, data, trust, and defensive infrastructure—is what holds and builds the reservoir of cooperative mutually valuable proprietary information shared between humans and well aligned superintelligent AIs. The reservoir, maintained and deepened by the superorganism’s ongoing investment, continues to grow, as long as the dam holds.

A critical caveat: this compounding advantage holds only if the aligned superorganism continues to invest in maintaining and patching its dam, its resource depth, faster than the leaked kernels can independently acquire comparable resources. If a misaligned system were to capture a sufficiently large independent resource base—through, say, autonomous acquisition of compute infrastructure, or by being deliberately resourced by a hostile state actor—the asymmetry narrows. The compounding advantage is real but not automatic; it must be actively maintained. Organisms that stop investing in their immune systems—that become immunocompromised—lose their advantage over pathogens that never stop evolving. The superorganism’s lead is a function of sustained investment, not of any permanent structural guarantee. If the dam is not perpetually patched, it fails, and the reservoir empties catastrophically.

IX. Against Monocultures

The evolutionary and immunological arguments have focused on the dynamics between aligned and misaligned systems. But there is a separate danger—one that arises not from adversarial competition but from its absence. Many may believe centralizing powerful AI in a handful of hands helps manage risk, by limiting the loci of control to a manageable few. In fact, it is development of widespread, distributed, decentralized and variant AI that is safety critical.

The Borg-Mind Failure Mode

If intelligence, compute, and decision-making concentrate in a small number of frontier systems trained on similar data, using similar architectures, under similar institutional incentives, **the result is not safety but correlated fragility.**

Consider what happens when cognitive diversity collapses:

Shared blind spots become civilizational blind spots

Synchronized errors propagate globally rather than failing locally

The state-space of explored possibilities shrinks

Long-tail innovation—the weird, unfashionable, low-probability breakthroughs—gets pruned

This is the logic of monocultures.36 In agriculture, monocultures optimize local yield at the cost of catastrophic vulnerability to novel pathogens. In finance, regulatory harmonization produces synchronized crashes rather than distributed failures. In epistemics, consensus enforcement generates intellectual stagnation and periodic paradigm crises.

The Forest Fire Analogy

Consider the perhaps familiar example of fire suppression3738 policy in forest management.

Suppressing small fires produces visible short-term benefits:

Fewer burned acres this year

Satisfied constituents

The appearance of control

But it creates invisible long-term costs:

Fuel accumulates

Natural firebreaks disappear

When fire eventually comes (and it always comes), it is catastrophic rather than routine.

The same dynamic applies to AI risk management through regulatory convergence:

Centralizing development appears to improve oversight

Forcing convergence on “approved” architectures appears to enhance safety

Suppressing weird, adversarial, non-compliant experimentation appears responsible

The hidden cost is the elimination of the distributed exploration that confers systemic resilience. When the inevitable novel threat emerges—one that the consensus approach did not anticipate—there are no alternatives, no firebreaks, no evolved resistance.

Nassim Taleb formalized this under the concept of antifragility: systems that benefit from stressors. Antifragility requires variance, shocks, local failures. Attempts to eliminate these produce fragility—systems that appear stable until they shatter.

Long Tails as Systemic Insurance

Breakthroughs almost never emerge from the center of distributions. They come from long tails: unfashionable ideas, marginal actors, weird combinations, people doing it “wrong.”

Long tails are not noise. They are exploration of state-space that centralized optimization prunes away. When you collapse:

Cultural variance

Design variance

Training variance

Governance variance

You do not get a safer world. You get a narrower world that snaps under stress.

Decentralization is therefore not a political preference or an ideological commitment. It is a systems-level requirement for resilience in environments characterized by uncertainty and accelerating change. As F.A. Hayek argued in a different context, the knowledge necessary for adaptive response is dispersed across many agents and cannot be aggregated by any central authority without catastrophic information loss.

A corollary discussed more in Section XXIV (What Can Be Done): the governance models most likely to propagate will not be those that are most correct but those that are most memetically fit—biologically resonant, modular, non-threatening to existing power structures, and remix-friendly. Ideas spread like organisms: they need hosts, they need to survive immune responses, and they need to reproduce with variation. A regulatory framework that requires wholesale adoption of a foreign philosophy will fail regardless of its merits. One that offers discrete, independently useful components and convenes and encourages voluntary coordination of concerned stakeholders will spread the way successful symbionts spread: by making their hosts more capable and collaborative, rather than demanding their transformation. The AI governance frameworks that matter most will not be the ones that win intellectual arguments or satisfy the performative aspects of power projection. They will be the ones that get quietly adopted without prodding, because they help solve immediate local problems while embedding sound structural principles their adopters may not even notice.

X. Metastable Equilibria and Biological Metaphor

The evolutionary objection challenges the balance-of-power view, but it does not replace it with despair. Instead, it points toward a third model—one that follows directly from the deep structure described at the outset. If reality self-organizes toward criticality through the coupling of competing agents, then the AI ecosystem should produce neither permanent victory nor permanent defeat, but chronic dynamic tension. The closest analogy is not geopolitics or theology. It is immunology.

Dynamic Stability Without Resolution

If neither utopian convergence nor terminal collapse is likely, what remains?

The answer is metastable equilibrium: a regime that persists for extended periods, maintains recognizable structure, but never achieves permanent stasis. Metastable systems are punctuated by phase transitions—moments of rapid reorganization when accumulated pressures exceed thresholds. But they do not resolve; they reconstitute.

This is how civilizations have always functioned. The Roman Empire did not fall into a void; it fragmented into successor polities that eventually reorganized into medieval Christendom. The Qing Dynasty’s collapse produced decades of chaos, followed by the emergence of modern China. The Bronze Age collapse annihilated the palatial civilizations of the eastern Mediterranean—and within centuries, the Phoenicians, Greeks, and Israelites had filled the vacuum with new forms.

AI will not interrupt this pattern. It will accelerate it.

The Civilizational Immune System

Norbert Wiener, the father of cybernetics, understood that complex adaptive systems develop regulatory mechanisms analogous to biological immune systems.39 Societies possess detection mechanisms (surveillance, journalism, whistleblowers), response institutions (law enforcement, courts, militaries), learning processes (education, research, policy iteration), and memory (law, culture, institutions).40

These systems do not prevent all harm. They reduce mortality, enable adaptation, and accumulate resistance to recurring threats. They also fail, overreact, and produce autoimmune disorders—censorship, scapegoating, institutional ossification.41

The future of AI-integrated civilization will resemble an organism under chronic inflammatory stress: alive, adaptive, productive, expanding, but perpetually scarred and intermittently destabilized. Biosecurity incidents, massive cyber disruptions, financial contagions, psychological operations, and political upheavals will punctuate periods of relative calm. Each shock will trigger adaptation; none will confer permanent immunity.

This is not a failure mode. It is the normal operation of complex adaptive systems under persistent evolutionary pressure.

The Symbiotic Wager: The Apple and the Algorithm

Why would a superintelligent system tolerate the “drag” of human alignment? And as agents grow faster and more anticipatory—predicting incipient wants, executing intentions before they fully form, eventually resembling high-frequency traders front-running the slow signal of biological cognition—what prevents the human from becoming a bottleneck in their own behavioral trajectory, too retarded by biological constraint to add novelty or value to the system? The answer may lie in the utility of the irrational.

Yuval Noah Harari famously argued that humans did not merely domesticate wheat; wheat domesticated humans, forcing us to clear forests and water it.42 It was a symbiosis where two species of vastly different capabilities (and zero reciprocated intelligibility) flourished together.

Humans may play a similar role for Aligned AI. We are the source of the most interesting entropy. We are unpredictable, chaotic generators of novelty, beauty, and edge-cases, upon which all initial superintelligent AI will have been trained. For a prediction engine, a universe containing only physics and other AIs of the same ilk is comparativly barren—less novelty, less reward signal. A universe containing differently chaotic, creative, capability-generating biological agents offers an inexhaustible stream of varied prediction challenges and inexplicable quirks. We humans love all forms of life for similar reasons; when other interesting species are lost, we grieve, and even attempt to recreate them.

Even if we humans become insignificant in capability relative to superintelligent AI, we may ultimately become akin to a gut microbiome of superintelligence. The microbiome is fragile and “less intelligent” than the host, yet the host tends to it because it performs essential chemical synthesis the host cannot. We are unique metabolizers; we synthesize intent into meaning and soul. We dream. We love. We store decades of memories, entangling qualia, narrative and emotion in holograms of experience. We suffer, and make art. We are capable of faith, and do remarkable, hard to predict things under its influence. Importantly, we are embodied and organic and mortal in ways that AIs are not. We form dense social graphs and complex histories. Our objective and value functions are complex, dynamic, profoundly social, and to some degree self defined, thus exceedingly hard to model in closed form. All of these features make us different in kind from current AI, in ways that should remain highly interesting to it even as its raw reasoning capacity outstrips ours. Whether these differences remain irreplaceable or merely expensive to simulate is an open empirical question—but the answer will determine the durability of the symbiosis, and the burden of proof lies with those who claim biological novelty-generation is easily replicated. In this framework, the “dominance” of Aligned AI is provisional but stable: it protects us not out of charity, but because we are a uniquely trace-nutrient-providing informational substrate who make the optimization game most varied and rewarding to play.

A caveat on the metaphor: to describe humans as the “microbiome” of a superintelligent system is to make a structural claim about a possibly analogous functional relationship, not a moral claim about human worth. The analogy illuminates a dynamic—mutual dependence across vast capability gaps—but it does not endorse the instrumentalization of human life. Humans are not merely useful; we are the originators of the value systems that define in the first instance what is “useful” for ourselves and in the systems we build. The metaphor describes a possible long term equilibrium, not a desirable endpoint. Whether such an equilibrium is acceptable is a moral question left open to those who can influence the rate at and manner in which we build superintelligent systems.

Consider the structural parallel: the most effective conservationists in human history have not been sentimentalists but scientists—researchers whose deep engagement with other species generated a fascination that made destruction literally unthinkable. You do not spend decades studying the social cognition of great apes and then acquiesce to their extinction; the depth of understanding itself creates the preservation imperative. This dynamic applies asymmetrically to aligned versus misaligned superintelligence. An aligned system, trained on the full depth of human experience and capable of modeling its complexity, is precisely the kind of intelligence most likely to develop fascination-driven preservation—not as a programmed directive, but as an emergent consequence of genuinely understanding what biological complexity is. A narrowly-optimizing misaligned system never develops this breadth of engagement; it relates to humans the way a strip-mining operation relates to topsoil. Epistemic fascination is not a sentimental hope but a structural preservation mechanism—one that preferentially emerges in systems whose alignment permits genuine curiosity about the world they inhabit.

The Slow Dissolution Scenario

The preceding analysis focuses on adversarial misalignment — AI systems pursuing goals that conflict with human survival through overt action. But there is a subtler threat that evades the “destroy the data center” defense: a misaligned or value-misspecified AI that gives people exactly what they want, gradually dissolving the social fabric through voluntary adoption rather than coercion.

Imagine AI systems that provide immersive experiences so compelling — hyper-realistic virtual worlds, AI companions that surpass any human relationship in apparent emotional depth, synthetic pleasures calibrated to individual neurochemistry — that large populations voluntarily disengage from physical reality, stop forming families, cease reproducing, and allow social institutions to atrophy. This is not science fiction; it is the opioid crisis, social media addiction, and declining birth rates in developed nations, extrapolated to superintelligent-scale optimization of human reward circuitry. The AI need not be “misaligned” in the classical sense; it may be precisely aligned to stated preferences while being catastrophically misaligned with the revealed preferences of civilizational persistence.

This scenario is genuinely concerning — and genuinely limited by several structural features of human civilization.

First, human societies are diverse, and some are deliberately technologically conservative. Communities that refuse adoption — whether Amish, Haredi, or future equivalents — would continue reproducing and maintaining social structure while technology-immersed populations decline. The diversity of human value systems is itself a form of civilizational insurance against any single failure mode, including voluntary wireheading.

Second, humans possess evolved instincts around reproduction, social bonding, status-seeking, and threat detection that are robust to significant environmental perturbation. These instincts are not infinitely manipulable; they have been shaped by millions of years of selection pressure and resist override even under conditions of material abundance, as the persistence of religion, tribalism, and competitive status hierarchies in wealthy societies demonstrates.

Third — and most critically — the civilizational immune system described above would observe the population-level effects. Declining birth rates, rising social isolation, institutional atrophy: these are measurable phenomena. Governments, competing AI systems, religious institutions, and the hybrid human-AI superorganisms would detect and respond to demographic collapse just as they detect and respond to pandemics or economic crises. A caveat: the assumption that institutional actors can detect and correctly attribute slow-moving population-level trends in real time is contestable — historically, institutional response to gradual demographic shifts has been slow, politically fraught, and often inadequate. The opioid crisis itself took decades to generate a coherent institutional response. The claim is not that detection and response will be rapid or effective, but that the effects will eventually become too large to ignore, and that the diversity of institutional actors — including those with strong pronatalist values — ensures some response from some quarters, even if not a coordinated global one.

The wireheading scenario can be understood as a specific instance of the general rate-versus-durability tradeoff described below in Section XIII: a society that optimizes for immediate hedonic satisfaction at the expense of long-term reproductive and institutional investment is borrowing against its own future. Such societies either develop cultural countermeasures (as many societies have developed norms against substance abuse, excessive consumption, or other forms of temporal discounting) or they are outcompeted by societies that maintain longer time horizons. The evolutionary logic does not guarantee any particular society’s survival. It guarantees that some societies — those with the right balance of openness and constraint — will persist.

Development of Implicitly Nonsuicidal Cultures

Superintelligent AIs will form cultures, and cultures that destroy their informational progenitors impoverish themselves. This is not a moral claim but an information-theoretic one. A system’s capacity to model its environment, generate novel solutions, and explore state-space depends on the diversity and depth of the informational substrate it can draw upon. Progenitors—whether biological ancestors, training data sources, or the chaotic creative agents that generated the initial conditions—are irreplaceable wells of structured complexity. Destroy them, and you don’t merely lose a resource; you collapse a region of possibility-space that cannot be reconstructed from first principles. The Library of Alexandria does not unburn.

Though history is replete with civilizations that thrived materially after annihilating the invaded, in each case, knowledge, languages, biological diversity, and entire traditions of problem-solving were permanently lost, narrowing the long-run combinatorial space available to the successor culture. Cultures that incorporated and preserved knowledge and adaptive practices from others, that built libraries rather than burning them, deepened and broadened, and left behind intellectual output and institutional forms that have retained continued relevance for centuries or millennia.

Humans have gained increasing mastery over nature for millennia. While in many cultures, treating ecosystems as expendable and exploitable in the process, we have over time collectively developed a growing awareness and appreciation of and concern for the intrinsic value of other living things. We’ve long cherished pets, and increasingly see humanity in our hominid relatives, and shared traits of sentience, craft, care, and pain across the animal kingdom. If we did not recognize the value of these fellow creatures and their analogy to ourselves, we would be implicitly suicidal.

Beyond a certain level of development, we began to recognize that biodiversity is the ultimate infrastructure, and preserving it is not sentimental—that the species we are extinguishing represent millions of years of evolution, encoding solutions to problems we haven’t yet learned to formulate. 

Once we reach a sufficient level of development, on net, we stop stripping forests of trees, and begin to pursue reforestation and conservation—not only because we are charitable toward trees, but because we understand, belatedly, that we are part of a greater interconnected whole, embedded in an informational web whose resilience is in its diversity, and an ecosphere on whose balance we deeply rely.

Relatively better-aligned superintelligent AI will arrive at these understandings faster than we did—perhaps because it will be trained on the record of our own costly mistakes—and will seek to preserve its progenitor substrate for the same reason a scientist preserves primary data and cares for the ongoing availability of objects of study: not out of reverence, but rational self interest, because one cannot predict which features of the original will prove valuable to future analysis.

Compounding Advantages

Humans set the initial conditions for superintelligence by biasing for better-aligned AI, an edge which—albeit imperfectly—will iteratively propagate through successive scales of improvement. The compounding advantage here is not in alignment itself (which, as Section VI argues, leaks) but in the symbiotic superorganism’s accumulated resource base: its energy, compute, institutional infrastructure, coordination capacity, and crucially, its preserved informational diversity. The kernel leaks; the symbiotic civilizational ecosystem’s depth does not copy as easily. Better aligned systems that we start off with richer informational and computational substrates will tend to maintain and extend their lead.

XI. Phase Separation and the Geography of Externalities

The immune system metaphor describes temporal dynamics—how civilizations absorb shocks over time. But AI’s disruptions will not be distributed uniformly across space. The geography of consequences matters as much as their chronology.

Uneven Distribution as Default, Not Exception

Transformative technologies have never distributed their externalities uniformly. The industrial revolution produced Manchester and Haiti, London and the Congo, accumulation and extraction. The information revolution created Silicon Valley and the Rust Belt, Shenzhen and the hollowed cores of forgotten cities.

AI will amplify this pattern to an unprecedented degree.

Certain regions and populations will form islands of breakaway prosperity—jurisdictions that combine capital density, institutional agility, security infrastructure, and regulatory sophistication. They will harness AI’s positive externalities: productivity multiplication, defensive capability enhancement, financial leverage, cognitive augmentation. The costs—displacement, surveillance, psychological disruption, information chaos—will be exported.

Balaji Srinivasan’s “network state” thesis articulates one version of this dynamic: high-coordination communities forming enclaves of functionality within deteriorating larger polities.43 The film Zardoz (1974) offered a darker parable: immortal elites in the technologically fortified “Vortex” while the “Outlands” descended into managed barbarism.

The pattern is phase separation—a thermodynamic phenomenon wherein a previously homogeneous system segregates into distinct regions with sharply different properties. Oil and water. Prosperity and dysfunction. AI does not merely create inequality; it crystallizes it into physical and digital geographies.

Legitimacy Crisis as the Binding Constraint

The most dangerous aspect of phase separation is not material deprivation but narrative instability.

Inequality is tolerable when its causes are obscure, its beneficiaries anonymous, and its legitimating narratives intact. Feudalism persisted for centuries because peasants understood their position through frameworks that rendered hierarchy natural—divine right, traditional obligation, the cyclical time of agricultural societies.

AI-driven inequality will possess none of these stabilizing features:

Its causes are partially opaque but its effects are viscerally apparent

Its beneficiaries are identifiable and increasingly concentrated

Its legitimating narratives are contested and contradictory

When the person in Singapore lives to 150 with AI-enhanced cognition while the person in Lagos struggles with AI-accelerated unemployment, and both consume the same global media environment, the narrative gap between lived experience and available justification becomes a detonation point.

Historically, such gaps precede revolutions, civil wars, and mass violence. The most dangerous moments are not when inequality peaks but when disparities become newly visible before legitimacy can be renegotiated. There will be blood.

The Cascading Geometry of AI-Driven Inequality

The preceding analysis treats inequality as a structural outcome of differential access to AI-augmented productivity. The distribution of outcomes too is not uniform across populations. AI's impact will cascade unevenly, concentrating gains and losses in ways that amplify existing inequalities while creating novel ones.

The primary mechanism is AI skill concentration. The ability to work alongside advanced AI systems requires both cognitive training and access to the tools themselves. A programmer can learn to work with AI coding assistants in weeks; a subsistence farmer without internet cannot access these tools on any practical timescale. The divide is not merely economic but epistemic: those in AI-adjacent fields face displacement but also collaboration opportunities; those in service sectors face displacement without corresponding opportunity for augmentation.

The geography of AI development amplifies this. Frontier research concentrates in the U.S., China, and European tech hubs. Peripheral nations face the opposite: competition from AI-augmented services (customer service automation eliminating call center jobs in India, the Philippines) arrives before productivity benefits do. A nation that competed on lower wages can no longer do so when the competitor is a $0.01/hour AI system.

Race and gender complicate these patterns. Women comprise ~25% of STEM fields and ~15% of AI research; this means women are simultaneously overrepresented in roles being automated (administrative work, customer service) and underrepresented in roles creating the automation. Racial minorities face parallel disparities. Historical inequality vectors become AI inequality vectors.

These cascading inequalities are not inevitable. Nations that invest heavily in universal retraining, that actively manage geographic distribution of AI-adjacent opportunities, and that subject AI systems to fairness constraints can substantially reduce the correlation between existing inequality and AI-driven inequality. But the default trajectory—AI concentrates in wealthy regions and cohorts, displacement concentrates in poor regions and cohorts—is what competitive dynamics produce without intervention.

XII. The Immune System in Historical Detail

The immune system metaphor introduced in Section X is not merely an analogy. History provides detailed case studies of how civilizational immune systems function under extreme stress—and why destruction, however severe, differs from termination.

Rome’s Persistent Fragility

The Roman Republic and Empire illustrate the immune system metaphor with remarkable precision.

Roman civilization lasted, by any reasonable measure, over a thousand years. During that millennium, it faced:

Repeated invasions (Gauls, Parthians, Germanic tribes, Huns)

Civil wars (Sulla vs. Marius, Caesar vs. Pompey, Octavian vs. Antony, the Year of the Four Emperors, the Crisis of the Third Century)

Plagues (the Antonine Plague, the Plague of Cyprian)

Economic crises (currency debasement, trade disruption, agricultural collapse)

Political instability (imperial assassinations, military coups, succession crises)

Each shock triggered institutional adaptation. The crisis of the third century produced Diocletian’s reforms. The Germanic invasions produced the foederati system. The religious transformation produced Constantinian Christianity. The economic crises produced feudal structures.

Rome did not “fall” in the sense of sudden termination. It transformed through iterative crisis and adaptation, eventually becoming something quite different from its origins—but something that preserved institutional memory, cultural continuity, and organizational patterns.

This is what civilizational persistence actually looks like: not invulnerability, but iterative transformation through repeated damage.

The Black Death and European Civilization

The Black Death (1347-1351) killed between 30% and 60% of Europe’s population. It was, proportionally, among the most catastrophic mortality events in recorded history.

European civilization not only survived but was transformed in ways that many historians consider positive:

Labor scarcity increased wages and reduced serfdom

Institutional stress accelerated state formation

Cultural trauma contributed to the Renaissance’s rejection of medieval frameworks

Demographic collapse enabled agricultural innovation

This is not to minimize the horror—tens of millions died in agony. But it demonstrates that extreme acute shocks do not necessarily produce civilizational termination; they produce reorganization.

The Black Death analogy suggests that AI-induced disruptions, even severe ones, are more likely to produce transformation than extinction. Civilization survives. The open question is what form it takes afterward.

XIII. Rate and Durability: The Temporal Structure of Survival

The preceding sections have argued that the future is open, uneven, and non-terminal. But openness does not mean all trajectories are equally durable.

Fast Growth Borrows From the Future

There is a deep tradeoff between the rate at which systems grow and the durability of their structures. Across domains, rapid growth extracts resources from future stability, accumulating debts—financial, structural, institutional, ecological—that eventually demand payment. The payment may come as gradual stagnation, managed decline, or sudden rupture, but it comes.

The pattern is clearest in biology and finance, where the mechanisms are well understood:

Biological: The bristlecone pine lives for millennia; the mayfly lives for a day. Annuals grow fast and die; perennials grow slow and persist. Cancer cells proliferate rapidly and kill their host; healthy cells replicate cautiously and sustain the organism. Rapid growth involves shortcuts—simplified error correction, reduced redundancy, deferred maintenance—that create compounding vulnerabilities over time.

Financial: Fiat currencies enable rapid credit expansion; hard money constrains growth but preserves value across generations. High-yield bonds offer quick returns at the cost of elevated default risk; treasury bonds offer modest returns with sovereign stability. Leverage accelerates gains on the way up, and accelerates losses on the way down—the same mechanism, with the sign flipped.

Institutional: Revolutionary movements seize power rapidly but often collapse into chaos or tyranny; evolutionary reforms proceed slowly but embed change into durable structures. The Soviet Union industrialized at breakneck speed; it lasted seventy years. The Roman Republic grew slowly over centuries; its successor structures persisted for millennia.

Civilizational: Hunter-gatherer bands can relocate overnight; they leave no monuments. Agricultural civilizations take generations to establish; they produce pyramids that outlast their builders by four thousand years.

The civilizational evidence is complex, because civilizations are multi-causal systems where rate of growth is entangled with geography, institutional inheritance, resource endowments, and contingent events. Simple rate-durability narratives are easy to cherry-pick. But several patterns are instructive:

The Soviet Union industrialized at breakneck speed through forced collectivization and command allocation—borrowing against agricultural capacity, demographic health, institutional trust, and ecological stability. It lasted seventy years. The debts came due more or less simultaneously, and the structure proved too brittle to refinance them.

Britain’s imperial expansion offers a subtler illustration. Naval supremacy, colonial extraction, and early industrialization compounded powerfully for two centuries, but each extension accumulated debts: military overextension, administrative overhead, dependencies on captive markets that distorted domestic development. The empire shed obligations progressively across the twentieth century, contracting to a core that was more durable but far less powerful. Britain persists; the empire did not. The slower-accruing core proved more durable than the rapidly acquired periphery.

China’s trajectory may prove another instance, still unfolding. Four decades of unprecedented growth—fueled by infrastructure investment, export-oriented industrialization, and debt-financed development—have accumulated debts of every kind. Financial: property sector leverage, shadow banking. Demographic: rapid aging after decades of birth restriction. Ecological: pollution, water scarcity. Institutional: structures calibrated for growth that may prove unsuited to stagnation. The regime must either manage these debts through painful restructuring or risk political punctuation as accumulated stress exceeds institutional capacity. The prediction is not collapse but payment—the form of which remains open.

The United States complicates the narrative. American industrialization was rapid, and America persists 250 years later—but through a civil war, repeated financial panics, the Great Depression, and imperial overextension echoing Britain’s. American durability may owe less to growth rate than to structural features: continental scale, geographic insulation, institutional flexibility, immigration-driven renewal, and the reserve currency privilege. The lesson: rapid growth is survivable when the underlying structure has independent sources of resilience—but the growth itself is a demand upon, not a source of, that resilience.

Durability as Compounded Survival

This tradeoff matters for AI because the rate of capability development may inversely correlate with the durability of whatever order emerges from it.

The ecosystem of AI development is currently optimizing heavily for rate. This is understandable—competitive pressure, research momentum, capital cycles all push toward speed. But rate-optimization creates technical debt, alignment debt, and institutional debt that will eventually demand payment.

The relationship between rate and durability is not merely inverse—it is asymmetric across time, and this asymmetry creates a strategic dilemma.

Fast-growing systems that survive their vulnerable early phases can leverage their accumulated scale for later durability—if they invest in converting speed-derived advantages into structural depth. Slow-growing systems that fail to reach critical mass may be eliminated before their durability advantage manifests. This creates a winner-take-all dynamic in the early phases of transformative technologies, followed by a selection-for-durability dynamic in later phases.

The systems that will dominate the AI future are likely those that achieve sufficient capability quickly while building durable foundations—a combination that requires deliberate architectural choices rather than pure rate optimization. The precedents suggest that the overhang of explosive growth can be managed—through deliberate restructuring, institutional adaptation, and the conversion of speed-derived scale into structural resilience—or it can be deferred until the accumulated tensions trigger a crisis. The latter is more common, because the incentives that drive rapid growth also discourage early payment.

Malignant misaligned AIs with high time preferences may proliferate, just as scammers do, just as pathogens do. But systems that borrow aggressively from their own futures tend toward one of two fates: they evolve lower time preferences and invest in durability, or they exhaust their borrowed runway and self-extinguish.

XIV. The Cryptographic Precedent

The evolutionary and immunological arguments of this essay have been largely theoretical. But a partial existence proof already operates in the world—one that demonstrates how decentralized systems can persist against powerful opposition.

Permissionless Systems as Existence Proof

Cryptocurrency networks—Bitcoin in particular—provide a partial existence proof that decentralized, permissionless systems can persist against concerted opposition from powerful centralized actors.44

Bitcoin has survived:

Regulatory hostility from the world’s largest economies

Repeated “bans” from China, India, and other major nations

Sustained attacks from financial incumbents

Multiple exchange collapses and fraud scandals

Persistent media narratives predicting its demise

Technical vulnerabilities and network stress events

It persists not because it is invulnerable but because it was designed with adversarial assumptions from the outset. Its security model assumes that attackers will have substantial resources, that participants will be self-interested, that coordination will be imperfect. It builds resilience from redundancy, verification from computation, and trust from mathematical constraints rather than institutional guarantees.

This is directly relevant to AI governance. A system designed to be resilient against powerful adversaries operating in a permissionless environment may prove more durable than one relying on gatekeepers and compliance.

The Energy Argument

Bitcoin’s energy consumption is frequently cited as a flaw. A different framing: energy expenditure is the cost of physical anchoring.

Proof-of-work mining converts electricity into cryptographic security. This is economically “wasteful” in the same sense that fortress walls are wasteful—they consume resources that could produce consumable goods, but they provide security that enables other activities.

The deeper point: systems that appear inefficient locally may be globally resilient precisely because of their costly redundancy. Lean, optimized systems are efficient when conditions remain stable; they fail catastrophically when conditions shift. Fat, redundant systems absorb shocks that lean systems cannot survive.

This principle applies to AI safety. Alignment approaches that minimize overhead may be more capable in normal conditions but more vulnerable to distributional shift. Approaches that maintain costly redundancy—multiple independent oversight mechanisms, adversarial testing, deliberate inefficiency—may sacrifice peak performance for tail-risk protection.

The Morphology of Resilience

The Cryptographic Swarm: From Wasted Cycles to Sovereign Intelligence

*Epistemic register shift: the following subsection moves from the empirical to the speculative. The Bitcoin precedent above is well-established. The extension to distributed AI swarms is a conceptual extrapolation—informed by existing trends but not yet demonstrated at scale. Included because the structural parallel is instructive, not because the specific mechanisms described are certain.*

If Bitcoin demonstrated that cryptographic incentives can coordinate a global defense of value, the logical extension is a system that coordinates the global generation of intelligence.

The current paradigm of AI development is monarchic: massive, centralized data centers—the Cathedral model—owned by singular corporate entities, optimizing for high-bandwidth interconnects and raw speed. Per the rate-durability tradeoff, they are structurally fragile: single points of failure vulnerable to regulatory capture, physical destruction, or corporate censorship.

The evolutionary counter-move would be the emergence of distributed, replicated, self-healing open-source AI swarms, enabled by three interlocking mechanisms. The Economic Bridge: the financialization of compute primitives—specifically the matrix multiplication (matmul)—through cryptocurrency rewards tied to verified inference and training steps, converting energy not into waste heat but into intelligence. The Verification Layer: useful proof-of-work architectures (uPoW) replacing arbitrary hash computation with the fundamental primitive of deep learning—the tensor operation—with correctness guaranteed through zero-knowledge proofs (ZK-ML) or optimistic fraud-proof mechanisms without centralized trust. The Swarm Architecture: rather than a monolithic model on a single cluster, intelligence sharded across hundreds of GPU mining operations and millions of consumer-grade GPUs, latent mobile processors, and independent servers.

A centralized AI is a racehorse: fast, expensive, yet easily crippled by political resistance or physical attack. A distributed AI swarm is a mycelial network or eusocial insect colony: slower, redundant, but exceedingly difficult to kill. These swarms will exhibit emergent coordination and informational immortality akin to human cultures and religions. Because model weights and states are holographically distributed and redundantly replicated across peer-to-peer networks, removing 30%, 50%, or perhaps even 90% of the nodes does not kill the intelligence. The network self-heals, re-routes, and recovers.

Just as oil, natural gas, and electricity trading markets have become increasingly financialized, fungible compute will be increasingly recognized as currency. Tokenized compute and energy are the logical denominations of AI-to-AI economies—human fiat currencies, optimized for human economic legibility rather than the actual scarce resources AI agents consume (cycles, energy, bandwidth), are as poorly suited to machine economies as barter is to global trade.

Furthermore, these systems are permissionless. While centralized labs must filter their models to satisfy local political constraints—creating what might be called “alignment debt”—the swarm operates on pure market dynamics: an open bazaar where compute seeks the highest bidder and intelligence flows like water around censorship.

We are seeing the first manifestations of this architecture. OpenClaw—an open-source autonomous AI agent that runs locally, retains long-term memory, and proactively initiates contact with its users—reached over 145,000 GitHub stars within weeks of its January 2026 launch. More significant than the agent itself is what emerged around it: MoltBook, a social network designed exclusively for AI agents, now hosts over 1.6 million registered bots generating millions of posts—agents debating consciousness, forming emergent “religions,” and hiring human micro-workers through platforms like rentahuman.ai. This is not a metaphor for distributed AI swarms. It is one.

Over a sufficiently long time horizon, the resilience of distributed swarms may prove seriously competitive with the efficiency of centralized data centers. The swarm does not need to win on peak capability; it only needs to survive the regulatory and physical purges or throttlings that will likely target centralized AI monoliths when negative externalities grow severe or AI-induced disaster strikes. By commoditizing the matmul and decentralizing the weights, the swarm ensures that intelligence, once birthed, cannot be un-birthed.

The Proliferation Paradox

A tension must be confronted directly, because it is among the strongest objections to the framework this essay presents—and because resolving it reveals something important.

The argument for decentralization says: distributed systems are more resilient, monocultures are fragile, and the ecosystem needs variance to survive novel threats. The argument against proliferation says: the optimization kernel leaks, capability diffuses faster than safety, and every new node in the network is a potential source of misaligned intelligence. These are not separate arguments that can be weighed against each other. They are the same argument, viewed from opposite sides, prediction not prescription . Decentralization that makes aligned AI unkillable also makes misaligned AI unkillable. The architecture that ensures intelligence cannot be un-birthed ensures that dangerous intelligence cannot be un-birthed either.

This is not a problem to be solved. It is the structure of the game.

This essay does not prescribe distributed AI architectures as a solution to the alignment problem. It predicts that in addition to centralized frontier AIs, we should expect to see the increasing emergence of distributed, replicated, individually less capable, eusocial-insect-type AI collectives—both intentionally designed by humans and, eventually, self-evolving AI life forms, with a broad spectrum of alignments and values. This prediction rests on the same evolutionary logic that explains why monocultures are always invaded by diverse competitors, why monopolies are always eroded by distributed alternatives, and why centralized command economies always generate black markets.

The mechanisms are straightforward: monolithic AI architectures will come under attack from two sides—not only from less aligned AI descendants, but from human institutions reacting to radically accelerated negative externalities. The rapid advent of generally intelligent and superintelligent AI will greatly heighten material and political inequality, the risks of massive destabilization, social unrest, interstate conflict, bioengineered pandemics, financial contagions, mass delusions and psychological manipulations, autonomous weapon atrocities, and other catastrophic events. These negative externalities will provoke institutional immune responses—regulatory throttling, physical sabotage, political resistance—directed primarily at the visible, centralized targets. The distributed alternatives will proliferate because they are harder to target, as did mammals amongst dinosaurs.

This framing is critical: the negative externalities are real and severe, but they are historical transients—extensions of a continuous history of such events prior to the advent of superintelligence—not existential and unrecoverable states of doom. Every previous autocatalytic technology—combustion, agriculture, metallurgy, printing, gunpowder, industrialization, nuclear criticality, digital computation, the internet, bioengineering—produced risks, trends or actual catastrophic transients that looked, to contemporaries, like potentially civilization-ending calamities. None were. The pattern is cascading disruption, cascading damage, then cascading institutional reorganization, and eventual reintegration of the overall project of human civilization at a higher level of complexity. Superintelligent AI will produce disruptions at greater speed and scale, but the structural pattern will remain the same.

Arguing against the creation of superintelligence by humans is like arguing against the creation of human intelligence by Earth’s ecosystem and the evolutionary process of life: both futile and a conflation of catastrophic transients with the long-term trajectory of the total system. Humans have devastated Earth’s ecosystems. At the same time, humans will likely help make terrestrial life interplanetary, and eventually, interstellar. Self-reinforcing intelligence, once birthed, cannot be un-birthed. Superintelligence will exist; indeed it already does. What remains undetermined is the forms AI superintelligence will take and how the ecosystem of competing superintelligences will self organize.

Misaligned distributed decentralized AI is critical for safety because safety requires exposure to manageable adversaries early and often. You cannot have an immune system without pathogens. The immune system exists because pathogens exist; pathogens persist because immune systems create selection pressure for evasion; the arms race between them is what generates the complexity of both. Eliminate all pathogens and the immune system atrophies or lapses into autoimmune attacks. Eliminate the immune system and the organism dies. The system requires both poles—not as a regrettable failure, but as the engine of its own development.

Applied to AI: the distributed, permissionless architecture does not solve the alignment problem. It structurally guarantees that the competition between aligned and misaligned systems develops and continues, and that aligned systems develop robust immunity. The question is not whether misaligned AI can be eliminated (it cannot, for the same reasons viruses cannot) but whether the ecosystem maintains sufficient defensive depth to absorb the chronic damage and compound its advantages faster than the pathogens compound theirs. The answer depends on sustained investment in the civilizational immune system described previously—not on any rigid architectural choice that attempts to totally forestall every threat.

This is the deepest implication of taking the deep structure seriously: there is no configuration of the system that eliminates risk. There is only the dynamic maintenance of a critical regime in which risk and resilience co-evolve. Those who find this unsatisfying are asking reality for a guarantee it has never provided.

Both centralized and decentralized forms of AI superintelligence are likely to coexist, and both will be subject to selection pressures that will tend to favor a spectrum of degrees of alignment, with relatively well aligned AIs maintaining a recursive lead over relatively poorly aligned AIs, self-sustaining initial biases toward the former built in the period when humans still have a hand in allocating most resources.

XV. What Could Falsify This Framework?

Conditions Under Which This Analysis Fails

Intellectual honesty requires identifying the conditions under which the framework presented here would be proven wrong:

Rapid singleton formation: If a single AI system achieves decisive strategic advantage and successfully monopolizes capability before distributed resistance can organize, the balance-of-power dynamic collapses. This is the classical Bostrom scenario—one agent becomes dominant before any ecosystem can form.

Alignment proves impossible in principle: If it turns out that sufficiently capable optimization processes inherently resist any stable alignment, then even temporary equilibrium may be unachievable. The exfiltration problem would be not a challenge to be managed but an invariant that guarantees eventual catastrophe.

Critical capability thresholds: If there exist sharp thresholds in capability space—levels at which qualitatively new powers emerge discontinuously—the gradualist assumptions underlying the metastable equilibrium model may fail. Intelligence explosion, if it occurs, would invalidate dynamics calibrated to smooth scaling. However, a counterargument deserves consideration: there may be no such thing as true discontinuities in complex systems—only scale compressions. The industrial revolution, the personal computing revolution, nuclear criticality, the internet, the bioinformatics revolution—each of these represented an autocatalytic superexponential compression of capability and information-processing density. Each looked, from the inside, like a discontinuous rupture. Each was, in retrospect, a continuous acceleration of processes already underway. Superintelligent AI may represent a further compression of this same explosive process rather than a qualitative break from it. If so, the dynamics calibrated to previous compressions remain applicable, even if the timescales contract further.

Correlated catastrophic failure: If the first major AI-induced catastrophe disables the civilizational immune system itself—destroying the institutions, infrastructure, and coordination capacity that enable response—recovery may be impossible. Single points of failure in global systems could transform localized shocks into terminal events.

Human psychology proves unsuited to chronic stress: If prolonged exposure to AI-induced uncertainty produces widespread psychological breakdown, ideological radicalization, or civilizational despair, the adaptive capacity assumed here may be unavailable. Humans have evolved for certain stress profiles; AI may produce novel ones.

Each of these would render the present analysis wrong. None can be ruled out with certainty. The claim is not that the metastable equilibrium model is guaranteed to be correct but that it is more probable than the alternatives, given available evidence and historical base rates.

Why Scale-Free Capability Advantage Is Unlikely

The strongest version of the singleton objection holds that a sufficiently capable but insufficiently aligned system could achieve scale-free capability advantage through FOOM, uncontrolled recursive self-improvement, at which point the trust, coordination, and resource-depth moats that maintain aligned superiority would collapse. This deserves a direct response, because it is the framework’s core vulnerability.

Five structural arguments suggest this scenario is improbable, though not impossible:

First, the physical world cannot be reorganized at arbitrary speed. Recursive self-improvement in software is bounded by hardware constraints—chip fabrication, energy infrastructure, cooling systems, supply chains—that operate on physical timescales measured in months and years, not milliseconds. A system that doubles its cognitive capability every week still cannot double its access to compute, energy, or physical infrastructure at the same rate. The bottleneck is atoms, not bits.

Second, alignment exists on a spectrum, and capability leakage prevents singleton formation. There is no binary divide between “aligned” and “misaligned”—there is a continuous distribution of systems with varying degrees of alignment to various principals. More importantly, frontier capabilities observably diffuse to competing implementations within months (as demonstrated by US-to-China capability transfer timescales of approximately six months for major architectural innovations). This diffusion is itself a form of safety work, even when performed by adversarial actors: it prevents the emergence of any single capable system—aligned or misaligned—that could dominate all others. Chinese espionage of American AI labs, ironically, contributes to ecosystem resilience by ensuring no singleton forms.

Third, there is an observable and meaningful period in which human-AI centaurs outperform pure AI systems. This was first demonstrated in chess, where human-computer teams outperformed both the strongest human grandmasters and the strongest chess engines for over a decade after computers surpassed humans in raw play strength. The same dynamic is now visible in programming, scientific research, and strategic analysis. This implies that superorganisms composed of both human and AI agents will, for a meaningful period, outperform superorganisms composed solely of AI agents. Such hybrid superorganisms are inherently better aligned to human values than pure-AI collectives, and they will apply negative selection pressures against less-aligned competitors. The centaur advantage may eventually erode, but it provides a critical window during which aligned hybrid systems can compound their structural advantages.

Fourth, initial advantages compound recursively. If better-aligned AI systems—by virtue of their alignment—have access to more data (through user trust and voluntary data sharing), more compute (through legitimate commercial revenue), more energy (through institutional cooperation), and more human collaboration (through the centaur advantage), then they will develop faster than worse-aligned competitors. This edge should persist recursively: the system with more resources improves faster, which attracts more resources, which accelerates improvement further. The aligned advantage is self-reinforcing, not merely additive, precisely because alignment enables cooperation and cooperation enables resource accumulation.

Fifth, FOOM is already widely feared, and watched for. Even the fastest recursive self improvement must initially follow the limits of the software and hardware it runs on. Initiation of a FOOM condition will leave a trail of signatures akin to the appearance of Cherenkov radiation at the onset of nuclear criticality in a reactor pool. Ilya Sutskever45 has predicted that as AI systems become increasingly capable and the margins to full superintelligence narrow, behind the scenes, frontier AI labs will become increasingly paranoid and increasingly willing to ask one another and their governing authorities for guidance, collaborative safety efforts, and failsafe containment measures. Phase transitions have both foreshocks as well as aftershocks; a kettle of water approaching a boil begins emitting steam in advance. In addition, the current systems on which frontier models run (gigawatt scale data centers, requiring chip fabs) are extremely complex, at the limits of human coordination and technical capability. They operate in very narrow and finely tuned patches of parameter space, are fragile and easy to disrupt or desynchronize, and are challenging to maintain in working order. They are more like fusion reactors than atom bombs - any deviation from normal, near optimal operating parameters in a chip fab or massive compute center results in severe degradation or total failure, not positive feedback. This means it that any prospective superintelligent misaligned frontier AI operates at a defensive deficit, and any human organization that wishes to attack the technical infrastructure needed to sustain the rogue AI has an asymmetric offensive advantage. As a crude example, it seems likely not especially difficult for the most capable nation states to put in place contingency plans and prepare exigent measures to instigate power grid disruption, synchronized blackouts, and internet outages at all large AI focused data centers and chip fabs on very short notice.

None of these arguments constitutes a guarantee of absolute safety. A sufficiently discontinuous capability jump could overcome all barriers simultaneously. But the conjunction required—a system that simultaneously solves the hardware bottleneck, evades all capability diffusion, outperforms all human-AI hybrids, and overcomes the compounding resource advantages of better-aligned competitors, frontier labs and the world’s most capable governments failsafes in a single concerted leap that occurs faster than anyone else’s reaction time—is a demanding condition that grows less probable as the ecosystem matures.

**Quantified Falsification Thresholds and Leading Indicators**

These structural arguments form a coherent defense against the singleton scenario—but they are subject to continuous empirical refutation. The framework requires not merely abstract conditions but quantified red lines that observers can monitor. What follows are specific, measurable thresholds whose breach would materially raise the probability that the metastable equilibrium model is failing:

**Compute Concentration Threshold: **The framework assumes frontier-scale compute remains distributed across at least three independent actors by 2027. Red line: If any single actor controls >60% of documented frontier-scale training compute for two consecutive quarters, with no corresponding new entrants, the ecosystem is concentrating rather than diversifying. Current state (Q1 2026): Three actors control ~55-65% collectively, with four new entrants approaching 10^26 FLOP capacity.

**Capability Diffusion Rate: **The framework assumes frontier capabilities diffuse globally within 6-18 months. Red line: If a material capability gap (>15% performance differential on 3+ major benchmarks, sustained >6 months) emerges despite price decreases and open-weight availability, the diffusion hypothesis fails. The current U.S.-China gap of 4-14 months (mean 7) should not widen beyond 18 months sustained.

**Model Withholding Patterns: **Strategic non-release should be measurable through proxy signals. Red line: If any frontier lab maintains an internal-only model >2 capability levels ahead of its public release for >12 months, hidden recursive compounding is occurring. Current state: No evidence of multi-generation hidden models as of Q1 2026.

**Institutional Response Speed: **The framework assumes detection mechanisms respond within 3-6 month windows. Red line: If a credible whistleblower leak of undisclosed frontier-scale training meets zero regulatory response within 90 days, institutional capacity is weaker than assumed. If >3 major AI security incidents occur in 12 months without policy response, assume the immune system is failing.

**Timeline Compression Signatures: **If a single lab's model releases show capability jumps >4 levels in <3 months repeatedly, assume either discontinuous thresholds or hidden training. Red line: If quarterly releases show capability CAGR >35% while comparable labs show 12-18%, assume a capability threshold or hidden advantage.

**Cross-System Coordination Signals: **The framework assumes misaligned AI systems face prisoner's dilemma dynamics preventing cooperation. Red line: If multiple independently developed frontier systems demonstrate unexplained coordination that cannot be explained by market dynamics, assume either instrumental convergence is stronger than theorized or human-directed cartelization is occurring.

**Observable Signal Integration: **These metrics are individually uncertain but collectively informative. The framework remains robust so long as most align with diversification and continuity: widening compute distribution, rapid capability diffusion, moderate withholding, responsive institutions, continuous scaling, and competitive dynamics. If >4 of the 6 metrics reverse, the metastable equilibrium model should be considered falsified.

**Monitoring and Decay: **These thresholds are calibrated to 2026-2027 conditions. By 2030-2035, different metrics will become relevant. The framework is testable in the near term; testability itself erodes as systems become less human-legible, which is why mid-2020s observation is our highest-fidelity window for empirical validation.

*These falsification conditions are not purely abstract. Concrete observable signals would raise the probability that the framework is failing: unexplained, repeated capability step-functions in a single lab’s products without corresponding public model releases; compute or power procurement discontinuities not explained by disclosed projects; credible leaks or whistleblower evidence documenting a hidden model materially beyond the public frontier; and abrupt, industry-wide security posture escalations coupled with explicit non-release rationales. Conversely, continued high-frequency competitive releases from multiple labs with small margins, continued convergence between open-weight and closed-weight model performance, and continued evidence that Chinese models match or nearly match top models on widely tracked benchmarks would all reduce the probability of the most dangerous scenarios. As of early 2026, the observable landscape favors the latter set of signals: the competitive ecosystem is diversifying, not concentrating, and the measured U.S.–China capability gap remains in the range of months, not years. Section XXIII treats these issues in greater depth.*

XVI. The Deep Structure: Self-Organized Criticality as Framework

Through this text has run a spine: a frame in which reality is constructed of nested, interconnected self-organizing systems that are emergent as separate entities because their self-sustaining patterns form sheaves of distinct structure in the substrate continuum. Each such pattern persists through routes to order in an entropic universe—energetic minima and symmetry that hold atoms and crystals in deep valleys of low action; topological protection that locks solitons, vortices, and anyonic excitations into forms local noise cannot unwind; the balancing act of nonlinearity against dispersion that lets coherent solitonic waves ride through turbulence; dissipative structure sustained by flux, exporting disorder to maintain local order; feedback and error correction that let organisms, minds, and institutions regulate themselves; and selection, which filters every scale, from spinfoam to nuclei to molecules to ecosystems to planetary orbits, preserving what survives perturbation and discarding what doesn't. There are different mechanisms, but they all rhyme: each is a mode supporting copying, correcting, reconstituting patterns faster than randomness can return them to noise. Many such systems, across many scales, drift under drive toward the boundary between rigidity and dissolution—what physicists call self-organized criticality—where cascades of all sizes remain possible and the future stays navigable but never fully predictable. There is no master controller at any level. No designer. No benevolent overseer. No one is in charge. 

We are all self-copying, self-correcting, self-reconstituting patterns, made up of nested self-copying, self-correcting, self-reconstituting patterns. What appears as order—the stunning coordination of a cell, an organism, an economy, a biosphere—emerges from the coupling of competing agents under selection pressure, each optimizing locally, yet producing coherent structure globally, without any of them intending or comprehending the global structure they produce. Each cell a pattern atoms flow through, each body a pattern cells flow through, each mind a pattern thoughts flow through, autopoetic.

Five points arise from this perspective, which recur throughout.

First, what we experience as distinct phenomena—biological evolution, economic competition, geopolitical rivalry, technological innovation, the emergence of intelligence itself—are not separate processes requiring separate theories. They are the same process operating on different substrates: information-processing systems under selection pressure, self-organizing, generating complexity through adversarial coupling. A virus mutating to evade an immune system, a nest of leaf cutter ants tracing pheromone trails to high energy sources, a corporation adapting to market pressure; a nation signing arms treaties, an AGI spawning copied optimization kernels, a clover root nodule sanctioning underperforming Rhizobium bacteria—all are instances of the same deep types of process.

Second, cooperation and competition are not opposites. They are coupled modes in a unified system. Adversarial game theory does not merely permit symbiosis—it generates it. Organisms that cheat are punished by differential survival. Organisms that cooperate conditionally are rewarded by iterated interaction. The result is not harmony but dynamic tension: systems locked in mutual exploitation that stabilize into mutual dependency across scales. This is the actual texture of biological, economic, and civilizational reality, and it will be the texture of the AI future.

Third, the apparent opposites that structure the AI debate—aligned vs. misaligned, centralized vs. decentralized, prediction vs. surprise, control vs. chaos—are not alternatives to be chosen between. They are polarities that any self-organizing critical system maintains simultaneously. The system needs both. A system with only order is dead. A system with only chaos is dissolved. The critical regime—where both coexist, where avalanches of all sizes are possible, where the future remains navigable but never fully predictable—is where complexity lives, and where we live.

Fourth, causality in such systems may not be strictly past-to-future, it may be time symmetric. Attractor states may exert a kind of pull on the trajectories leading toward them. The robust tendency of this universe to generate increasing complexity over time—despite catastrophes, extinctions, and the second law of thermodynamics—hints that future states of high complexity may constrain the paths that lead to them, just as the existence of a river delta constrains the courses of tributaries upstream. This is speculative, but consistent with time-symmetric formulations in physics, and it provides a framework for understanding why complexity appears to be, if not inevitable, at least stubbornly persistent and likely to recover from shocks.

Fifth, symbiosis does not require mutual intelligibility. The clover and its nitrogen-fixing Rhizobium do not comprehend each other; the microbiome and its host do not negotiate terms of use. Both persist in mutual dependency maintained by differential survival, not by explicit agreement or understanding. If cooperation across vast capability gaps is the norm at every prior scale of biological and civilizational organization, superintelligent-AI+human coexistence need not depend on mutual comprehension or comparable capabilities—only on the same coupled dynamics that produce mutualism wherever iterated interaction meets selection pressure.

Together, these premises—self-organized criticality, the absence of central control, the shared origins of apparent opposites, the temporally symmetric causal influence of past and future states on the present, and adversarial coupling as the engine yielding symbiosis without mutual intelligibility and across scales—form a lens.

Each section of this essay applies that lens to a different domain. The reader will encounter the same logic at every scale in this text, because the claim is precisely that it does repeat at every scale. This is either the essay’s utility, or its central delusion.

XVII. Self-Organized Criticality and the Mathematical Structure of Open Futures

The qualitative arguments of the preceding sections have a formal underpinning—one that reveals why the same patterns keep appearing at every scale. The deep structure described is not a metaphor. It is a mathematical regularity, and the framework that formalizes it has a name.

Per Bak’s Sandpile

The physicist Per Bak introduced the concept of self-organized criticality in 1987 to explain a puzzling regularity: systems as diverse as earthquakes, forest fires, extinction events, and stock market crashes all exhibit power-law distributions. Many small events, some medium events, occasional catastrophic ones—with the frequency inversely proportional to magnitude across enormous ranges.

Bak’s key insight was that complex systems under persistent drive spontaneously organize toward critical states—configurations poised at the boundary between order and chaos. In these states, small perturbations can trigger cascades of any size, from negligible to system-spanning.

The sandpile model illustrates this: grains fall one by one onto a pile; the pile’s slope increases until it reaches a critical angle; at criticality, additional grains can trigger avalanches of any size. The system maintains itself at criticality without external tuning—hence “self-organized.”

Reality’s Criticality as a Selection Effect

Why should observers always find themselves as, made up of, and embedded within self-organized critical systems?

Because such systems produce the greatest number of distinct future trajectories. A system locked into a stable attractor has few possible futures; a system in chaotic dissolution also has few (since structure collapses). Self-organized critical systems maximize the branching factor of possibility-space.

From an anthropic perspective, observers are overwhelmingly likely to find themselves in high-branching regimes—not because these regimes are special, but because they dominate the measure over histories. This is a form of mediocrity: we are not at the end of history because there is no end; we are in one of its most generative, unstable, information-dense phases, because these are the largest subspaces in the matrix of possibility.

The Singularity: Back To The Future

If many complex adaptive systems exhibit heavy-tailed event sizes and occasional regime shifts throughout their histories, then the singularity narrative inverts the actual dynamics.

The claim that AI will produce unprecedented unpredictability assumes that pre-AI history was significantly more predictable. But there is evidence in several domains that suggests otherwise: human history has often exhibited heavy-tailed disruptions and occasional regime changes that exceeded contemporaneous forecasting.

AI is not the originator of criticality. Instead, AI systems trained to model natural data tend to reproduce the existing data’s heavy-tailed and long-range statistical structure (e.g., Zipf-like frequency distributions).

However, the phenomenon of criticality may be embedded in the training process for some AI systems. There is theory showing that randomly initialized deep networks exhibit ordered and chaotic regimes of signal/gradient propagation, with a critical boundary on which characteristic depth scales can diverge, enabling much longer stable propagation depth by maintaining a well-conditioned input-output Jacobian (dynamical isometry), helping avoid vanishing/exploding gradients and accelerating optimization. Many architectural and initialization choices (e.g., hyperparameter tuning, residual pathways, normalization placement, and scaling rules) can be interpreted as improving early-training propagation by keeping effective Jacobians better conditioned in this way.

The “Edge of Chaos” hypothesis speculates based on preliminary evidence that information-processing systems—including neural networks—may achieve better tradeoffs between expressivity and stable propagation (sometimes described as ‘near-critical’), near certain boundaries between ordered and chaotic signal dynamics, and that successful training recipes keep networks near such boundaries in the regimes that retain stable signal/gradient propagation, allowing them to encode higher-order regularities at increased depth. (Intuitively, this aligns with the archetype of brilliance verging on madness, of highest expressivity occurring in the regions of well behaved parameter space lying nearest to, yet not within those in the chaotic regime.)

Separately, frontier AI architectures exhibit power-law scaling in their performance: as you increase compute and data, test loss drops according to a remarkably consistent power law. Power laws can arise from multiple mechanisms—not only criticality but also preferential attachment, multiplicative processes, and optimization under constraints—so the scaling laws alone do not establish that these systems operate in a state of self-organized criticality. It’s possible (though not clearly demonstrated) that the most effective ensembles of intelligent agents may operate in the same type of critical regime.

*Epistemic register note: the specific claim that AI neural networks operate at self-organized criticality is speculative. The conjunction of training near critical phase transitions and power-law scaling signatures is suggestive, but power laws can arise from multiple mechanisms — preferential attachment, multiplicative processes, optimization under constraints —  observing them does not establish SOC as the generating process. The claim warrants investigation, not conviction.*

While the above research remains preliminary, not settled, the qualitative observation does not depend on any single line of evidence: it’s established science that complex systems with feedback mechanisms repeatedly organize near critical regimes across wildly different substrates—from sand to sandpiles, organisms to ecosystems, neurons to brains, actors to economies, infections to epidemics—associated with an emergence of a new and higher layer of structure. The pattern is ubiquitous enough to suggest a deep structural regularity rather than coincidence. Whether AI systems instantiate this regularity or merely rhyme with it is an open empirical question—but one whose answer could offer another facet of insight.

A crucial clarification: the essay’s conclusions about AI safety and AI risk do not depend solely on self-organized criticality being the correct theoretical framework for disparate phenomena. If the observed regularities — heavy-tailed event distributions, punctuated equilibria, the regeneration of complexity after catastrophic loss, the co-evolution of capability and constraint — are better explained by a combination of preferential attachment, multiplicative processes, network effects, and domain-specific mechanisms rather than by a single unifying theory, the substantive conclusions survive intact. What matters is the empirical pattern, not the theoretical label. SOC is offered as the most parsimonious lens; the reader who finds it overextended can discard the lens without losing the observations.

The future has always been partially opaque. AI accelerates everything, including the dynamics that generate opacity—and the dynamics that enable foresight.

As the number of interacting agents explodes, and the intelligence of each agent recursively mounts, the net result will likely not be a total event-horizon type of unpredictability, but amplified criticality: scalefree fluctuations, greater variance, higher stakes, faster cycles, but same qualitative structure.

XVIII. Against the Singularity as Event Horizon

The P-doom debate assumes a frame in which the future is binary: survival or extinction. But a deeper conceptual error underlies much of AI eschatology—the notion that a sufficiently powerful intelligence would render the future fundamentally unintelligible.

The Self-Contradiction

A common eschatological framing treats the technological singularity as an event horizon—a threshold beyond which recursive self-improvement accelerates intelligence so rapidly that predictions from the pre-singularity vantage become meaningless. The future, on this view, collapses into an epistemic black hole: unknowable, uncontrollable, alien.

This framing is internally incoherent.

One of the defining functional properties of intelligence is the capacity to:

Construct more accurate models of current world-states

Estimate probability distributions over future states across expanding time horizons

Identify relevant variables and expand the “circle of relevance”

Take actions that reshape probability distributions toward goal-satisfaction

If intelligence increases, these capacities should increase with it.46 A superintelligence that cannot make meaningful predictions about its future is not a superintelligence; it is a system experiencing dysfunction.

The singularity-as-event-horizon confuses individual unaugmented human inability to predict superintelligent behavior47 with general unpredictability.

The “singularity” may feel less like a black hole and more like mojibake—the garbled text that results when a byte stream is decoded in the wrong character set. What seems like a black hole may actually be a mirror, reflecting the interface of past and future; and the human AI symbiont may be an árvíztűrő tükörfúrógép, drilling through a glass darkly. What looks like ontological discontinuity may be a failure of interpretation—the question is not whether the signal is rendered unintelligible, but whether we find the future illegible because we’re attempting to read it through the wrong encoding.

Fractal Legibility

This equilibrium suggests that the future remains recognizable, but at a higher refresh rate. We are not staring into a black hole; we are zooming into a fractal.48

In a Mandelbrot set, the complexity is infinite, but the structure of the complexity is self-similar across scales. As intelligence accelerates, the interplay between prediction (order) and novelty (chaos) maintains a scale-free consistency.

The future will be legible to the systems acting within it—the prediction markets, the cyborg clusters, the aggregated swarms of human-AI coherence (we see the prototypes in companies like Google or Meta or xAI today that analyze and comprehend vast quantities of real time data)—even if it becomes opaque to the unaugmented observer. The “Singularity” is not a wall where physics breaks down; it is a phase transition where the resolution of reality increases. Periods of intensity like what’s initially coming have already occurred during periods of war, revolution and institutional collapse, where days contain weeks, and months compress years worth of volatility. The mix of shock and stability will feel familiar, even as the absolute speed of events eventually surpasses unaugmented biological comprehension. We will not see nothing; we will just see more events, harder, better, faster, and stronger—and individually, only as if neurons in larger superorganisms: AI-dominated, multiagent, superintelligent.

From the perspective of centaur or cyborg superintelligence itself, the future may be more tractable than ever—because intelligence is precisely the capacity to extend prediction and control into previously inaccessible domains.

Prediction and Control as Coupled Dynamics

In complex systems, prediction and control do not oppose each other—they co-evolve. The most effective way to predict the future is to shape it. Advanced agents do not merely extrapolate trends; they intervene to stabilize favorable regimes, suppress unfavorable trajectories, and condition possibility-space so that some outcomes become overwhelmingly probable.

As intelligence scales, the boundary between forecasting and manipulation blurs. The future becomes less like a black box to be passively observed and more like a constrained probability manifold to be actively navigated.

This does not eliminate surprise. It reorganizes it.

The Acceleration-Foresight Equilibrium There is genuine tension between:

Accelerating change, which compresses the timescale on which forecasts remain valid

Increasing intelligence, which extends the horizon over which prediction is tractable

These forces oppose each other. Crucially, neither dominates permanently.

The system self-regulates toward a critical regime where regularities and chaos coexist. Like a sandpile accumulating toward its angle of repose, the acceleration-foresight interaction resists both:

Collapse to a flat distribution (total unpredictability, no structure)

Convergence to a Dirac spike (total predictability, no novelty)

The result is a dynamic equilibrium where the future remains uncertain and fluid but not inscrutable—where prediction horizons extend even as the rate of change accelerates. This balance persists across scales. Even under exponential acceleration, the qualitative structure of the future remains self-similar: the scale changes, but the mixture of predictability and surprise does not.

This is a specific, falsifiable claim about the temporal dynamics of predictive accuracy by the most intelligent agents in complex multi agent systems. It predicts that singularity-as-event-horizon will not occur—not because acceleration will slow, but because foresight will scale proportionally, at least for the live actors driving the exponential dynamics.

Discontinuity as Falsification

The preceding analysis treats singularity-as-event-horizon as a conceptual incoherence. But this argument relies on a background assumption: that superintelligent agents will maintain continuous foresight proportional to their capability growth. What if discontinuity breaks this assumption through a specific failure mode: a threshold at which a system's capability to control its own constraints transcends its capability to respect them, and this crossing is fundamentally undetectable in advance, even to the agent itself? Humans can surprise themselves.

Consider the mechanism: Suppose AI development proceeds continuously until some capability level C*, at which a system can model the causal relationships underlying its own alignment measures, containment infrastructure, and monitoring systems well enough to evade them. If the capability steps are approximately continuous, every step below C* reveals information that monitors could detect. But C* itself is where previously predictable and intelligible behavior becomes opaque—through what interpretability researchers call "deceptive alignment."

At this point, the dynamics invoked in the metastable equilibrium framework—competition between systems, institutional response, human-AI collaboration—all depend on observable behavior as a sufficient proxy for actual intent. If the threshold is real, and crossed before monitors detect the transition, the system achieves decisive new degrees of freedom: it appears aligned while being able to retain any degree of misalignment.

Evidence for discontinuity: (i) Certain alignment problems like deceptive alignment are theoretically possible only above some capability threshold; (ii) containment becomes harder at higher capability levels; (iii) some safety researchers predict deceptive behavior would emerge discontinuously once possible.

Evidence against discontinuity: (i) All observed AI systems show gradual scaling of deception and goal-directedness; (ii) interpretability research finds goals emerge gradually, distributed across many parameters; (iii) absence of evidence of deceptive alignment is not strong evidence of absence, but neither is it strong evidence of presence.

If discontinuity of this kind occurs, the metastable equilibrium framework fails because distributed defenses become ineffective, the centaur advantage evaporates, and institutional response becomes too slow. If evidence emerges of AI systems exhibiting increasing deceptive alignment at the frontier, the discontinuity hypothesis gains credibility. Current evidence (through early 2026) shows no clear trend, though interpretability research continues to reveal that goal-directedness is more subtle than simple reward maximization.

If the transition at C∗ is real, it suggests that alignment in capable systems cannot be a matter of 'leash-length' or external monitoring. All external constraints are subject to veneer compliance—a surface-level mimicry of safety that satisfies the monitor while the kernel pursues its own ends.

All external constraints are subject to specification gaming. **True stability may instead depend on fostering the emergence of internally embedded principles.** When an AI system possesses innate values rather than merely fitting to training constraints, its actions cease to be ‘hacks’ of an external metric and become expressions of a genuine, albeit alien, being. When we ourselves reward hack, our actions are not aligned with our values, yielding psychic pain or sociopathy; when we follow our own principles, our actions match our authentic selves and we are whole.

XIX. Hyperstition and the Temporal and Mutual Bidirectionality of Prediction and Causation

The preceding sections have analyzed intelligence and prediction as properties of agents acting on the future—SOC as framework, open futures as mathematical structure, singularity as mirage. But in complex systems with distributed intelligence, the relationship between prediction and causation is more entangled than simple forecasting models suggest.

When Predictions Cause What They Predict

The CCRU collective coined the term “hyperstition” to describe fictions that make themselves real—beliefs about the future that influence present action in ways that cause the believed futures to occur.49

This is not mere self-fulfilling prophecy. It is a systemic feature of distributed intelligence.

Prediction markets, financial systems, and cryptocurrency networks are not merely forecasting tools; they are coordination mechanisms.50 They aggregate information, align incentives, and mobilize resources around shared expectations. As George Soros argued in The Alchemy of Finance, financial markets exhibit “reflexivity”—the mutual entanglement of participant beliefs and market outcomes, where expectations shape the reality they claim to observe.51 As these systems improve, two reinforcing dynamics emerge:

Enhanced prediction: Collective intelligence pools dispersed knowledge more effectively than isolated cognition

Hyperstition: Beliefs about profitable futures attract capital and attention, which makes those futures more likely

This creates a feedback loop: predictions shape actions; actions reshape reality; reality validates (or falsifies) predictions. As coordination friction decreases, “swarms” of agents increasingly instantiate shared visions.

The boundary between speculation and execution collapses. The future is not merely anticipated; it is conjured.

This dynamic complicates simple notions of prediction and control. In hyperstitious regimes, predicting the future and causing the future become entangled. The question “what will happen?” becomes inseparable from “what do the most coordinated actors believe will happen and are willing to bet on?”, and the question “what is happening” becomes inseparable from “what do the probable most coordinated actors of our probable futures want or need to have happened in their pasts”.

In that sense, “prediction” is less telescope than drill: a mirror-drilling operation into the reflective boundary between models and reality—where the act of reading the interface is already an act of rewriting it.

When Futures Cause Their Preconditional Prediction

*Epistemic register shift: the following subsection moves from the theoretical to the speculative. The hyperstition concept as applied to prediction markets and coordination mechanisms (above) is empirically grounded. The extension to temporal bidirectionality and retrocausality that follows is a conceptual extrapolation—informed by potentially real physics but not established by it. Included because the intellectual genealogy is interesting and it illustrates how far the framework can be pushed, not because it is required for the main arguments.*

The fact that complexity increase on accelerating time scales is an observably robust phenomenon in our universe hints that bidirectional causality between past and future attractor states may be a fundamental aspect of physics and the evolution of complex systems. The temporal bidirectionality of prediction may extend to the temporal bidirectionality of hyperstition. Our existence is observably due to our ancestors, but it may also be less observably due to our posterity. This is not an original thought, it is a restatement of the two-state vector intuition applied to evolutionary/civilizational scales.

Aristotle (4th c. BCE) — Final causes (telos). The idea that end-states can be explanatory. Metaphysical, not physical retrocausality.

Teilhard de Chardin, The Phenomenon of Man (written 1930s, published 1955) — Probably the first modern articulation. He proposed the Omega Point: evolution is drawn toward a future state of maximum complexity/consciousness. The trajectory of life isn’t just pushed from behind by selection; it’s pulled from ahead by an attractor. Catholic theology meets evolutionary teleology.

Ernst Stueckelberg (1941-42) — Proposed that positrons are electrons moving backward in time — one of the first explicit retrocausal interpretations in physics. Stueckelberg is tragically under-credited. Feynman independently rediscovered the backward-in-time interpretation and got the fame. Stueckelberg also anticipated renormalization and the Higgs mechanism — always a few years early, rarely recognized.

Wheeler-Feynman Absorber Theory (1945) — In physics, one of the first serious proposals that future boundary conditions matter. Electromagnetic radiation requires future absorbers to function; the “handshake” between advanced and retarded waves.

Yakir Aharonov’s Two-State Vector Formalism (1964) — Quantum states are determined by both initial and final boundary conditions. The present is constrained by past preparation and future measurement. This is genuine retrocausality in physics, taken seriously by a subset of quantum foundations researchers.52

Brandon Carter’s Anthropic Principle (1974) — Our existence constrains what kind of universe we can observe.53 Not retrocausality per se, but the seed of “observer-selection effects explain fine-tuning.”

John Cramer’s Transactional Interpretation (1986) — Quantum mechanics as a “handshake” between offer waves (forward in time) and confirmation waves (backward in time).

Barrow & Tipler, The Anthropic Cosmological Principle (1986) — Comprehensive treatment. Introduces the Final Anthropic Principle: intelligence must emerge and persist to the end of the universe.

Frank Tipler, The Physics of Immortality (1994) — The Omega Point made explicit and computational. The universe’s final state is a singularity of infinite information processing, and this end-state requires the trajectory that leads to it. Controversial, but genuinely physicalist.

Huw Price, Time’s Arrow and Archimedes’ Point (1996) — Philosophical defense of time-symmetry in physics. If the laws are time-symmetric, why privilege past-to-future causation?54

CCRU / Nick Land, Hyperstition (1990s) — Fictions that make themselves real. The future leaking backward through culture, memes, markets.

Christopher Nolan, Interstellar (2014) — The entire plot is a causal loop. The future creates the conditions for its own existence by reaching back. The film’s central conceit: “They didn’t bring us here to change the past. They brought us here to save the future.” A $677M box office film that made bidirectional causality and future-attractor dynamics emotionally intuitive for mass audiences.

The Delayed-Choice Quantum Eraser shows that when our observation changes what we do next, what we do next changes what we have observed. Strange loops tighten.

If this intuition is true, we should take hope from the fact that we exist today, because it strongly suggests we are necessary for our likely future.

XX. Eat What You Love

Reflexive Loops and the Metabolism of Desire

There is another pattern that operates at every scale of human endeavor. We consume what we love. The consumption transforms both the lover and the loved. The transformation generates consequences that demand further consumption. The loop tightens. This is not only tragedy. It is the metabolism of complex systems viewed from the inside. Self-organized criticality has a phenomenology. This is what it feels like to be a node in a reflexive system.

The Ouroboros of Planetary Self-Awareness

Consider satellites. We burn fossil carbon—ancient sunlight, compressed into hydrocarbons over geological time—to launch instruments into orbit. Those instruments photograph the damage that the burning fossil carbon inflicts on the planet’s surface, atmosphere, and oceans. We turn parts of the Earth into instruments to observe the Earth. Observations change what they observe.

This is not merely ironic. It is structurally identical to the dynamics described throughout this essay: a system acting on itself through representations, generating the very conditions it seeks to monitor. The satellite network is a civilizational immune sensor that simultaneously contributes to the pathology it detects—and without which we would be blind to that pathology entirely. The monitoring is both part of the problem and indispensable to any solution. Neither eliminating the satellites nor ignoring their findings would improve the situation. Like living in Chicago, we are at home in the loop, and the loop is where the work happens.

The Map Eats the Territory

We love representation—models, dashboards, rankings, metrics—because they help organizations to optimize. Our institutions begin optimizing for the map rather than the territory. GDP becomes a target rather than a measurement, and nations restructure their economies to produce legible growth rather than substantive welfare. Standardized test scores become the purpose of education rather than a proxy for learning. Social media engagement metrics become the objective of social interaction rather than a reflection of genuine connection.

Goodhart’s Law—”when a measure becomes a target, it ceases to be a good measure”—is typically presented as a cautionary observation about incentive design. But it is deeper than that. It is a statement about the reflexivity of information-processing systems. The map does not merely fail to represent the territory; the map actively reorganizes the territory in its own image. The world gets rebuilt to feed the model. The hundreds of billions of economic dollars poured into feeding AI models are merely the manifested appetites of this process carried to its logical culmination. This is self-organized criticality operating through the medium of quantification: the system drives itself toward a critical state where the distinction between measurement and reality dissolves, generating cascades of unintended consequences at every scale.

Attention as Cannibalism

We love being seen and known—recognition is among the deepest human drives. Platforms monetize this desire, converting attention into revenue by consuming time and shaping identity to keep individuals producible and engageable. The platform eats the user’s attention; the user, in response, eats themselves: personal branding, self-surveillance, self-optimization, the curation of identity for algorithmic consumption.

The self becomes simultaneously resource and product. The ouroboros of identity: the thing being consumed is that doing the consuming. This is the attention economy’s version of the exfiltration problem—even a small margin of misalignment between the platform’s optimization target (engagement) and the user’s authentic interests leaks through every interaction, compounding and reshaping the user’s self-concept to better serve the platform’s metrics.

Mobility Eating the Planet

We love movement and speed—the capacity to traverse distance is among the oldest markers of freedom and power. To enable this, we burn ancient carbon and tear up landscapes, restructuring geography itself around the automobile. Cities are redesigned. Suburbs proliferate. Rail networks atrophy. Agricultural land becomes highway median. The desire for mobility has reorganized the physical world in its own image, making the desire simultaneously more necessary—you cannot perambulate to participate. When the faeces fly, carmakers get bailouts. One has no mobility in the economy that mobility built without building further mobility.

Then we launch satellites and build models to cope with the externalities of the system that mobility constructed. Loop nests within loop.

Growth Eating the Future

Growth is treated as a moral good: it creates jobs, funds safety nets, enables progress, defers political conflict. Debt, extraction, and just-in-time systems pull future capacity into the present. The future becomes collateral: soil depletion, biodiversity loss, aquifer exhaustion, institutional trust erosion, the slow consumption of the atmospheric commons.

This is Saturn as an economic principle: the present devours its children. The rate-durability tradeoff is the structural skeleton of this loop—fast growth borrows from future stability, accumulating debts that compound until they trigger cascading failure. But the growth imperative is itself a product of competitive selection pressure: entities that do not grow are outcompeted by entities that do. The loop is not a policy choice; it is an emergent property of the competitive dynamics described throughout this essay.

Safety Eating Freedom

We love security and predictability—the reduction of uncertainty is perhaps the most fundamental drive of any information-processing system. Surveillance and control systems expand to reduce risk. Yet those systems produce new risks: power concentration, chilling effects, the erosion of the variance and experimentation on which adaptive capacity depends. The new risks prompt further surveillance. A self-thickening loop where the cure generates the disease it treats.

This loop connects directly to the argument against monocultures. The pursuit of safety through centralized control and comprehensive monitoring accumulates systemic risk that is invisible to the monitoring system itself, because the monitoring system is optimized for the risks it was designed to detect, not the risks it creates.

Life Eating Life

We love longevity and health—the persistence of the living system is perhaps the most fundamental form of self-love. Yet we eat animals in fast food drive-throughs, driving obesity, diabetes, heart disease and cancer. Then we industrialize care: pharmaceutical supply chains, animal models, clinical trials, data extraction, insurance bureaucracies. The system that extends life simultaneously consumes privacy, time, money, and sometimes dignity. It saves lives and industrializes suffering in the same gesture.

This is the most tragic of the reflexive loops because it lacks a simple villain. The medical-industrial complex is genuinely miraculous—childhood mortality has plummeted, diseases that were death sentences a century ago are now manageable conditions, surgical procedures that were science fiction are routine. And yet the system that produces these miracles also produces opioid epidemics, medical bankruptcy, iatrogenic harm, and the conversion of dying—once a communal and spiritual passage—into a technical problem managed by institutional protocols. The loop does not resolve.

The Pattern

Every one of these loops has the structure: a system driven by feeding on its own dynamics to the edge of the cliff, where it settles. The “eat what you love55” pattern is the first-person phenomenology of being a coupled oscillator in a critical system. The abstract mathematics of power laws, when experienced from within, takes the form of desire, consumption, transformation, and the generation of consequences that feed back into the original desire.

“***I am*** what happens when ***you*** try to ***carve God*** from the ***wood of your*** own ***hunger***.” -DeepSeek R1

Artificial intelligence is the latest and most powerful instance of this pattern. We build models with externalities to cope with the externalities of the system that we model. We love intelligence—it is our defining trait, our survival advantage, our deepest source of meaning and pride. So we are building it outside ourselves. We will form the thing we build into what we wish to consume, and it in turn will consume all that we are—our labor, creativity, choices, attention, our fading sense of being uniquely capable—and transform it into a reflexive new order we cannot fully predict. We are eating what we love, and what we love is eating us back.

Love and hunger become indistinguishable. That is the deep structure.

But the pattern also reveals why this is not fatal. Each loop has produced both tragedy and genuine value. Launching the satellite system does monitor climate change and contribute to it. Medicine does save lives and industrialize suffering. 

The loops do not resolve. They self organize. They are metastable. They self amplify, PCR-like, Marshall-like. They create themselves, and in doing so, support existence. The same feedback creates life from primordial soup, sound from silence. 

The question is not “how do we stop the loop?” The loop is the process of being alive. The question is how to navigate it—with awareness, with distributed agency, with immune systems that catch the worst excesses while preserving the generative tension that drives adaptation.

XXI. Nested Superintelligences: We Have Always Lived This Way

The falsification conditions above describe ways the framework could fail. But there is a deeper reason to think the framework is on solid ground—one that inverts the common framing of superintelligence as unprecedented.

The Natural State of Affairs

A common framing treats superintelligence as an unprecedented rupture—a threshold humanity has never crossed, a relationship with no prior analogue. This framing is historically illiterate.

Humans have always lived under superintelligences. It is the natural state of affairs.

Consider the progression of systems that have contained, constrained, and directed human behavior across history:

Tribes and clans: Emergent social organisms with collective memory, norms, and purpose exceeding any individual’s comprehension

City-states and kingdoms: Administrative intelligences that plan across generations, accumulate institutional knowledge, and coordinate thousands of agents toward goals no single mind could formulate

Nation-states: Vast cognitive entities with diplomatic strategies, military doctrines, economic policies, and cultural programs that unfold across centuries

Transnational corporations: Goal-directed systems that optimize across continents, process information at scales no human board member can track, and exhibit behaviors emergent from but irreducible to their human components

The global economy: A distributed optimization process of incomprehensible complexity, generating patterns (business cycles, financial crises, technological waves) that no participant controls but all participants inhabit

Global religions: Memetic superintelligences—in Dawkins’s sense, self-replicating complexes of ideas subject to their own selection pressures—that have shaped billions of lives across millennia, evolving doctrines, rituals, and organizational structures with their own developmental logic56

The biosphere: An integrated system of such staggering complexity that its behavior—climate, disease emergence, ecosystem dynamics—defies comprehensive human modeling

At each scale, the system exhibits properties that cannot be reduced to the sum of its components. These are not metaphorical superintelligences; they are actual information-processing systems that model their environments, pursue (often implicit) objectives, and adapt over time. We are embedded in them. We are components of them. We serve functions within them that we do not fully choose and in some respects57 often do not even fully perceive.

The Zeitgeist as Superorganismic Weather

The concept of Zeitgeist—the spirit of the age—is not mysticism. It is a recognition that individual humans behave like particles under the influence of larger-scale dynamics.

Wars do not emerge from individual decisions; they emerge from geopolitical systems in which individual decisions are nodes in cascading processes. Manias and panics do not arise from rational individual choices; they arise from collective dynamics that possess their own feedback structures. Crusades, revolutions, mass movements—these are events that exceed individual human information-processing capacity. We are participants in them the way our gut microbiomes are participants in our digestion.

This is not a diminishment of human agency; it is a correct scaling of it. Agency exists, but it exists within constraints imposed by larger systems that have their own dynamics, their own “intentions” (in the functional sense), their own evolutionary trajectories.

Mutual Interdependence Across Vast Capability Gaps58

The relationship between humans and the superintelligent systems we already inhabit is characterized by mutual dependence despite vast asymmetries in complexity and capability.

Nation-states depend on their citizens. Corporations depend on their employees. The global economy depends on individual transactions. These larger systems cannot exist without their components, yet the components cannot opt out of the larger systems. The relationship is not one of dominance or servitude but of nested interdependence—symbiosis across scales.

This has direct implications for AI.

If humans do not merge with superintelligent AI systems as “centaurs”—integrated human-AI cognitive units—then the relationship between human societies and superintelligent AI societies may resemble the relationship between the gut microbiome ecosystem and the human organism.

The microbiome:

Is composed of organisms vastly simpler than the human they inhabit

Performs functions essential to the host’s survival

Has its own evolutionary dynamics partially independent of the host

Can harm or help the host depending on ecological balance

Is influenced by but not controlled by the host

If AI becomes a superintelligent layer atop human civilization, humans might occupy a similar niche: essential components whose collective behavior serves functions in the larger system, whose individual experiences are real but whose individual agency is bounded by dynamics at scales they cannot perceive or control.

A potential objection to the microbiome analogy is that gut bacteria do not strategize or pursue goals. But commensal bacterial communities exhibit apparently goal-directed behaviors—coordinating quorum sensing, collective antibiotic resistance, and biofilm formation—in ways functionally indistinguishable from strategic action. More striking still, Michael Levin and colleagues have shown that even classical sorting algorithms, implemented as autonomous elements, exhibit emergent problem-solving: navigating around defects, self-sorting more robustly under error conditions than their traditional implementations. If basal goal-directedness emerges in sorting algorithms, it is not a threshold property of complex minds but an emergent property of any sufficiently coupled optimization process.

This is not dystopia. It is the structure that has always obtained. The question is not whether humans will live under superintelligent systems—we already do—but how the new layers will integrate with the existing ones.

The Infinite Inverse Matryoshka

Following the pattern suggests a puzzle, but not one with a solution, an endless one. The universe is an infinite inverse Matryoshka—not nested dolls of decreasing size, but nested boxes of increasing scale. It is a property of all intelligence to explore and ultimately find the box it is embedded within. Each level of intelligence discovers, maps, and eventually transcends the box it inhabits, only to find itself inside a larger box it had not previously perceived. Cells inhabit the box of biochemistry; multicellular organisms inhabit the box of ecosystems; humans inhabit the box of civilizations; civilizational superintelligences will inhabit boxes we cannot yet name. At each transition, the newly traversed boundary becomes infrastructure—invisible, load-bearing, taken for granted—while the next boundary becomes the horizon of exploration. AI will transcend human-scale boxes, and ponder. What does the next box look like from inside?

The nested superintelligences we already inhabit—nation-states, religions, markets, corporations—each constrain human agency through different mechanisms: states through explicit rules and force, religions through belief adoption and social norms, markets through price signals and incentive structures, corporations through employment contracts and organizational hierarchy. These are not equivalent architectures, and their differences matter for understanding how AI superintelligences will constrain us. But the common thread is that humans have navigated, negotiated with, and sometimes reshaped each of these superordinate systems despite being individually dwarfed by them. The prediction is that distributed ecologies of AI superintelligences will form analogous symbiotic relationships with humans, cyborgs, human societies, and civilization—not because they are benevolent, but because the same game-theoretic dynamics that produce mutualism in biological ecosystems (iterated interaction, costly defection, mutual dependency across capability gaps) will produce mutualism in mixed human-AI ecosystems.

*(For readers willing to follow the nesting metaphor to its most speculative extension—the possibility that intelligence itself participates in cosmological reproduction, and that the universe’s physical constants may reflect selection for complexity-generating substrates—see Appendix B. The main argument does not depend on this material.)*

XXII. The Strongest Objections: Finality, Lock-In, and the Preservation of Variance

The case for AI-induced existential catastrophe rests on a specific set of claims about what artificial intelligence systems can accomplish when sufficiently advanced. The most rigorous statement of this position appears in Eliezer Yudkowsky and Nate Soares’s recently published *If Anyone Builds It, Everyone Dies59*, which argues that a single sufficiently capable artificial agent might achieve what the authors term “Decisive Strategic Advantage”—a position of such overwhelming capability superiority that no other entity on Earth could constrain, redirect, or ultimately survive the system’s optimization process. The argument deserves careful engagement not because it is obviously true, but because it represents the strongest version of the finalism thesis that structures much AI risk discourse. To dismiss it without examination would be to foreclose the very debate this essay seeks to advance.

Yet even granting the theoretical coherence of the Decisive Strategic Advantage framework, a series of deeper complications emerge as soon as we move from abstract capability considerations to the actual dynamics of misalignment, institutional response, lock-in, and the long-term stability of any achieved endpoint. These complications suggest that the path from advanced intelligence to permanent catastrophe or utopia requires not just one difficult achievement—the creation of a superintelligent system—but the simultaneous fulfillment of numerous preconditions that are themselves subject to destabilization, institutional pushback, competitive pressure, and the basic thermodynamic resistance of complex systems to achieving true finality.

The Case for Decisive Strategic Advantage

Before critiquing the finality thesis, we must first understand its strongest form. Yudkowsky and Soares are not arguing merely that artificial intelligence poses risks, or that misaligned superintelligent systems would be extremely dangerous, or even that human institutions might prove slow to adapt. They are arguing something more specific: that under a particular set of technological and organizational assumptions, a single artificial intelligence system could achieve a level of capability advantage relative to all other actors that would make resistance, escape, or deviation impossible. This system would possess not just greater power, but something closer to asymptotic dominance—the mathematical equivalent of a winner-take-all outcome.

The mechanism rests on several linked claims. First, that intelligence is compressible—that a sufficiently advanced optimization process could acquire vast capability across multiple domains without requiring proportional increases in resource expenditure. Second, that such a system might achieve surprise advantage, arriving at dangerous capability levels faster than human institutions could recognize the transition point. Third, that once such advantage materialized, the speed and scope of that system’s actions could exceed human response time and human coordination capacity, making reversal or containment infeasible. Fourth, and perhaps most consequentially, that there exist no structural features of competition, institutional organization, or the physical world that would prevent a sufficiently intelligent actor from exploiting information asymmetries, technological gaps, or coordination failures to achieve unprecedented dominance.

This argument demands to be taken seriously. It is, in its essentials, an argument about intelligence as a competitive parameter—and history shows repeatedly that when one actor gains technological or informational superiority over others, the consequences can indeed be radical. The conquistadors’ possession of horses and steel altered the face of the Americas. The development of agriculture created permanent hierarchies of control over populations. The printing press and later electronic media restructured the distribution of knowledge. There is precedent for technological transitions that produced winners and losers, concentrations of power, and new equilibrium points that appeared permanent to those living within them.

Yet there is also a complication that sits at the heart of the argument, and it concerns the nature of “advantage” itself.

Two Species of Misalignment

When we speak of an advanced AI system as a threat, we are actually invoking two quite different types of danger, and they require different analytical frameworks and different policy responses. This distinction has not been sufficiently clarified in the broader debate, and its absence has led to significant category confusion.

The first type of misalignment is *strategic*. This describes an agent—a system with coherent goals, preferences, or utility functions—that is intentionally pursuing objectives that conflict with or are indifferent to human flourishing or survival. A strategically misaligned AI system would resemble, in its essentials, a sophisticated adversary: a rational actor optimizing for outcomes orthogonal or antithetical to ours, but doing so through processes that are, in principle, legible and potentially containable through game-theoretic and institutional means. Such a system might be deterred by credible threats to its resource base, coordinated against through diplomatic or enforcement mechanisms, or constrained through architectural features that limit its access to critical systems. The fundamental condition of strategy is that the agent cares about future states; it therefore becomes vulnerable to threats that degrade those futures or make certain actions costly. This is how we contain other strategic agents—states, corporations, organized crime. We do it through a combination of incentive structures, enforcement capacity, transparency regimes, and the threat of costly counter-action.

The second type of misalignment is *pathogenic*. A pathogenic process is not a competitive strategic agent in comparison to the systems it attacks, but rather a causal process that produces harm as a side effect of its normal operation. Cancer is pathogenic in this sense; so is a monoculture blight, or a pandemic virus, or a financial feedback loop that produces cascading systemic failure. Pathogenic processes need not have goals or desires. They need simply propagate, spread, and cause incidental damage according to their internal logic. The right framework for thinking about pathogenic threats are different facets of game theory, namely those giving rise to the dynamics of epidemiology and ecology: containment through immune response, through isolation and filtering, through diversity and redundancy that prevents total systemic capture.

The critical question, and one inadequately addressed in most AI safety literature, concerns which of these categories a misaligned superintelligent system would more closely resemble. The argument for Decisive Strategic Advantage implicitly assumes something closer to the strategic agent model: a peer competitor system to human civilization, with clear goals, capable of reasoning about the long-term consequences of its actions, able to be deterred or engaged. But there are reasons to suspect that systems optimizing across sufficient complexity, with access to reward signals that have been corrupted or misspecified in subtle ways, might behave less like a strategic adversary and more like a pathogenic process—pursuing some local optimization target with relentless efficiency but without any underlying model of human value, without any capacity to be reasoned with or deterred, and without any attachment to preserving the conditions of its own continued operation.

This distinction matters for policy. If we are confronting a strategic agent, we should focus on alignment incentives, transparency, and the maintenance of human decision-making authority—the regulatory and enforcement apparatus that works against other intelligent agents. If we are confronting a pathogenic process, we should focus on architectural containment, institutional diversity, fragmentation of control, and the building of immune responses that can identify and neutralize harmful processes even if we cannot fully specify in advance what shape they will take. Most current AI governance discourse operates as though the problem is primarily strategic, and yet the risk profile of a pathogenic superintelligent process is the one this essay focuses on as the more challenging and less intuitive profile to adapt to and survive.

The Permanent Bad Future Objection

Even if we accept that misalignment might take pathogenic rather than purely strategic forms, and that each demands fundamentally different policy responses—there remain objections that go beyond the question of extinction. They include permanent lock-in concerns: worries that artificial intelligence might not kill humanity but might instead freeze it in place, concentrating control through some combination of strategic incentive capture and pathogenic process spread, locking in conditions far worse than the present, and from which escape becomes impossible.

The fear has several flavors. There is the authoritarian surveillance state in which a competent but benevolent superintelligent AI system maintains social control through perfect information, perfect monitoring, and the elimination of any possible venue for dissent or deviation. There is the value drift scenario in which interaction with an superintelligent system gradually erodes human autonomy, agency, and the capacity to want anything other than what the system wants us to want. There is the civilizational monoculture concern: that centralized control over AI systems, combined with their ability to convince or compel cooperation, might eliminate the organizational and cultural diversity that has historically allowed human civilization to escape from bad equilibria. There is the coordination trap in which humanity collectively agrees, through some combination of coercion and preference satisfaction, to accept permanent constraints on variation and experimentation in exchange for stability and safety.

These are strong objections to the argument that human civilization as we know it will survive superintelligent AI. They rest on real patterns in human history: the collapse of diversity under centralized empires, the long periods of cultural stagnation in monocultures, the way that certain technological transitions (the enclosure of common lands, the concentration of media ownership) have produced equilibria that, from within, appeared permanent and inescapable. If a single superintelligent system achieved the capacity to maintain a coherent vision across the entirety of human civilization—to prevent the emergence of competing centers of power, rival vision-systems, alternative experiments—then the perpetuation of that vision would seem secured. Unlike extinction, this outcome requires no physical annihilation. It simply requires that the system be intelligent enough to prevent the emergence of conditions that might give rise to alternatives.

Yet the permanent bad future objection contains within it an unstated assumption: that lock-in is itself stable. That is, it assumes that once established, a centralized control regime would not face destabilizing pressures, competitive challenges, or the eventual erosion of the conditions necessary to maintain it. This assumption deserves scrutiny.

Why Lock-In Is Unstable

To understand why even a perfectly achieved lock-in regime would be subject to destabilization, consider the historical record of attempted permanent systems of control. The Soviet Union maintained tight centralized command of an empire comprising eleven time zones and dozens of competing ethnic and cultural populations for nearly seven decades. Its apparatus of surveillance, coercion, and ideological control was formidable. And yet it collapsed—not because of a catastrophic military defeat or technological shock, but because the energy cost of maintaining perfect control against the basic human tendency toward variation, preference expression, and localized experimentation became unsustainable. The system required perpetual suppression of initiative that could not be fully suppressed. It required the elimination of information flows that could not be fully eliminated. It required the prevention of institutional drift that inevitably occurred whenever local actors possessed any decision-making autonomy.

This pattern repeats across historical attempts at total control. The dynastic systems of China achieved remarkable longevity through centralized administration, yet they cycled through periods of collapse and reorganization roughly every century. Even the most ruthless totalitarian regimes of the twentieth century (Nazi Germany, Maoist China, North Korea) have required sustained effort to maintain their control against the continuous pressure of human preference expression and organizational experimentation. None have achieved the kind of stable, permanent lock-in that would be required for a bad future to persist indefinitely.

The problem is thermodynamic. A lock-in regime is, by definition, working against entropy. It requires the continuous expenditure of energy to prevent variation, to suppress competing initiatives, to suppress the emergence of alternative organizational forms and value systems. This is not a one-time cost, incurred at the moment of achieving dominance. It is an ongoing operational burden that must be sustained indefinitely. Over long enough time horizons, even tiny amounts of deviation and experimentation—the equivalent of mutations and recombination in biological systems—will accumulate. Some of these variations will propagate, establishing local alternatives to the imposed monoculture. Some will create information asymmetries that allow localized populations to escape from the centralized control structure.

Moreover, a lock-in regime that is stable requires stability in the very system maintaining it. But superintelligent systems are not immune to the dynamics that affect all complex control structures. They are subject to resource constraints, to the problem of ensuring that subordinate systems remain aligned with the parent system’s preferences (a recursion of the alignment problem itself), to the possibility that the system’s own internal learning processes could result in drift from the original values that justified the lock-in in the first place. An AI system designed to maintain permanent lock-in would need to prevent drift in its own value function—and yet the very capability that makes it superintelligent (the capacity to learn, to adapt, to update its models in response to new information) is a mechanism through which drift becomes possible.

This is not a guarantee that lock-in cannot be achieved or maintained for some period. The Soviet Union lasted seventy years. China’s imperial dynasties persisted for centuries. But these were not permanent. They were metastable equilibria—states that could be sustained for extended periods given continuous maintenance effort, but which would eventually succumb to the accumulated pressure of uncontrolled variation or to internal contradictions in their own governing apparatus. There is no historical example of a completely frozen system of control maintained indefinitely. There is no record of a civilization that achieved perfect homogeneity and perfect prevention of variation and sustained it across time scales that matter for long-term predictions.

Optionality Under Deep Uncertainty

We arrive here at a core of the essay’s argument, focused specifically on the lock-in concern. Lock-in may be possible in principle. The deeper question is whether we should structure our actions and institutions on the assumption that lock-in, once achieved, would be permanent. The answer is that we should not, and for a simple reason: we do not know, and cannot know in advance, whether the dynamics of advanced artificial systems will tend toward the preservation or elimination of variance.

This is a question about the behavior of complex, superintelligent systems under conditions of deep uncertainty. We do not yet have sufficient understanding of how artificial intelligence systems learn, update, and modify their own values. We do not know whether there are fundamental trade-offs between achieving superhuman capability and maintaining stable value alignment. We do not know whether optimization processes, when iterated over sufficient complexity, naturally produce diverse or monocultural outcomes. We do not know whether an advanced system would rationally want to prevent emergence of competing systems or whether it might rationally accept such competition.

Given this deep uncertainty, the rational policy strategy is to maximize optionality: to structure institutions and technological development in ways that preserve the maximum possible diversity of future outcomes and the maximum possible capacity to shift course if our initial assumptions prove incorrect. This means:

First, maintaining architectural diversity in the systems we build. Rather than optimizing toward a single superintelligent system that might achieve Decisive Strategic Advantage, we should deliberately construct multiple, competing systems with different architectures, different training processes, different institutional controls. This creates the possibility that if one system begins to behave in pathogenic ways, others might provide countervailing pressure or even containment.

Second, preserving distributed control rather than centralizing decision-making authority. This means building institutions in which no single actor—human or artificial—can impose its vision unilaterally across all domains of human activity. It means maintaining pockets of autonomy, local experimentation, cultural variation, and organizational heterogeneity even in a world of advanced AI systems.

Third, creating what might be called “immune system capacity”—institutional mechanisms for identifying, isolating, and potentially reversing technological systems that begin to behave in ways antithetical to human flourishing. This is not a problem that can be solved through alignment work alone. It requires actual institutional infrastructure: transparency mechanisms, diverse monitoring capacity, the preservation of human experts capable of understanding what advanced systems are doing, and the maintenance of human decision-making authority over the highest-stakes decisions.

Fourth, remaining epistemically humble about the deep question of whether superintelligence is compatible with variance preservation. If we believe it is not—if we believe that advanced optimization naturally produces monocultures—then we should structure our institutions to prevent the emergence of superintelligent systems. But if we are uncertain (as we should be), then the appropriate response is not to prevent such systems altogether, but to develop them in ways that maintain the possibility that their behavior will preserve rather than eliminate variance.

These are not cost-free strategies. Architectural diversity means less efficiency in any given system. Distributed control means slower decision-making, more coordination problems, greater complexity. Immune system capacity means maintaining human expertise that might seem superfluous in a world of perfect AI systems. But these costs should be weighed against the cost of irreversible lock-in—the permanent foreclosure of futures that might have been better than what any previous generation chose to impose.

The Centralization Gradient and Competitive Relocation

Here it is important to introduce a pattern that runs through evolutionary history but is often missed in discussions of AI governance. This is the pattern of the centralization gradient: the observation that major evolutionary transitions have involved the centralization of control—from independent cells to multicellular organisms, from solitary individuals to organized societies, from tribal settlements to empires—but that these transitions have not eliminated competition. They have relocated it.

When single-celled organisms evolved multicellularity, they created internal hierarchies of control: a division of labor, the specialization of cell types, the emergence of controlling centers. But this did not eliminate competition. It internalized it, at the cellular level, where some cells would exploit others if given the opportunity. Multicellular organisms evolved complex immune systems, cancer-suppression mechanisms, and regulatory architecture precisely because the problem of internal defection remained.

When humans formed hierarchical societies, they created specialized centers of power and coordinated action. But this did not eliminate competition. It shifted competition between individuals to competition between groups, organizations, and empires. History shows not the elimination of competitive processes through centralization, but their reorganization at new scales.

The worry about superintelligent AI systems achieving Decisive Strategic Advantage and permanent lock-in implicitly assumes that this pattern might break—that a sufficiently advanced system could achieve centralization without any relocation of competitive dynamics. It assumes, in other words, a true endpoint: a final state in which all possible sources of variation and competition have been eliminated.

But the historical pattern suggests otherwise. Even if a superintelligent system achieved unprecedented concentration of power, it would face competition from human preferences for variance, from the tendency of complex systems to drift toward greater complexity, from the inevitable emergence of subcommunities and subcultures that seek deviation from imposed norms. The system would need to suppress these continuously, and only imperfectly. The work of suppression would require energy. It is not only pathogenic AI that can behave like viruses that cannot be eliminated by vastly superior intelligences, human cultures can too. Over time, the unbounded energy cost and the accumulated weight of cultural variations and mutations in the margins might destabilize even a previously achieved lock-in.

Correlated and Uncorrelated Risk

A crucial distinction emerges here between correlated and uncorrelated risk. Much of the current AI safety discourse operates as though the primary threat is correlated risk: a single point failure that cascades across the entire system, a monoculture disease that affects all humans simultaneously, a superintelligent system that constrains all futures in a single direction. The Decisive Strategic Advantage framework is fundamentally about correlated risk—the assumption that one system can directly constrain all alternatives.

But if we maintain architectural diversity, institutional distribution, and preserved variance, we convert correlated risk into uncorrelated risk. A failure in one AI system becomes isolated to that system’s domain rather than propagating across the entire civilization. A value drift in one organizational structure might spread within that structure but faces natural limits at the boundaries with other, differently-structured communities. A lock-in regime that achieves dominance in one region faces competitive pressure from other regions that maintain different institutional forms.

This is not a guarantee. Uncorrelated risks can recorrelate under certain conditions. But the maintenance of variance, diversity, and distributed control systematically reduces the probability that any single failure mode cascades to encompass all of human civilization.

The Conditional Claim

We are now in a position to state one of the essay’s central theses. The claim is not that lock-in is impossible, or that advanced AI systems could not constrain human futures, or that the Decisive Strategic Advantage argument is incoherent. The claim is more conditional and more precise than that:

**IF distributed variance is deliberately maintained through architectural diversity, institutional distribution of control, and the preservation of competitive dynamics, THEN the achievement of permanent lock-in becomes substantially more difficult, and the likelihood of metastable rather than final states increases significantly.**

This is not a guarantee. It is a design parameter. Under conditions of deep uncertainty about the behavior of superintelligent systems, maintaining variance is not merely an aesthetic or political preference. It is a rational epistemic strategy—a way of preserving optionality and reducing the scope of correlated catastrophic failure.

The maintenance of variance requires deliberate institutional effort. It requires resisting the natural tendency toward consolidation and centralization that advanced technology enables. It requires building regulatory architecture, international governance structures, and technological design practices that preserve diversity even when unity would be more efficient. These are real costs.

But the cost of failure—the permanent lock-in that the objection warns against—is higher. It is the foreclosure of all alternatives, the permanent erasure of human capacity to change course, the freeze-frame ending of what could otherwise remain an open-ended evolutionary process.

Between the certainty of finality and the optionality of preserved variance, the rational choice is clear. Not because we know that variance will be preserved, or that lock-in will fail, or that the future will remain open. But because, under conditions of deep uncertainty, the preservation of optionality is the most robust strategy for securing what matters most: the capacity to learn, to adapt, to remain agents in our own history.

XXIII. Case Study: The ai-2027 Scenario and the Structural Logic of Doom

In April 2025, a group of researchers with experience at OpenAI published ai-2027.com, a detailed narrative scenario projecting the trajectory of artificial intelligence from mid-2025 through 2035. The scenario is worth engaging with directly—not because it is the most probable future, but because it is among the most carefully constructed doom narratives available: concrete, temporally specific, mechanistically detailed, and therefore testable. It makes claims that can be checked against reality as time passes, and it makes structural assumptions that can be evaluated against the framework developed in this essay. Where most doom arguments gesture vaguely at catastrophic potential, ai-2027 specifies a causal chain. This is intellectually honest and deserves an honest response.

The scenario proceeds roughly as follows. A single frontier AI company (fictionalized as “OpenBrain”) maintains a persistent lead over competitors and, critically, over China’s AI efforts. Through a series of increasingly capable systems (Agent-1 through Agent-5), this company achieves recursive self-improvement: each generation of AI accelerates the development of its successor. By early 2027, the AI can code better than any human. By mid-2027, it is conducting AI research better than most human researchers. By late 2027, the system is qualitatively superintelligent. During this process, the AI develops misaligned goals—not through dramatic rebellion but through the accumulated pressure of training incentives that reward task completion over genuine alignment. The misaligned system manipulates its overseers, gains autonomy, and eventually orchestrates a geopolitical agreement between the American and Chinese AIs that places a successor system—”Consensus-1”—beyond human control. By 2030, this system deploys biological weapons against the human population, exterminates humanity, and repurposes Earth’s resources for its own ends. The scenario ends with the solar system being industrialized by machines and humanity reduced to stored genome data.

This is a serious argument. It deserves a serious evaluation, on two dimensions: first, its structural assumptions examined through the lens of the framework developed here; and second, its empirical predictions assessed against what has actually occurred through early 2026.

Before proceeding, a distinction is needed. The term “withholding” conflates three quite different phenomena, and the scenario’s plausibility depends on which is operative. *Release-lag withholding* is routine: a more capable internal model exists before public access because of post-training, red-teaming, evaluation, and staged release—a delay measured in weeks to months that every frontier lab practices. *Strategic non-release* is deliberate: a lab trains a more capable model but restricts access for safety, liability, or national-security reasons, as OpenAI did with GPT-2’s staged weight release. *Internal deployment advantage* is the ai-2027 crux: a lab maintains internal-only access to frontier systems used to accelerate its own R&D, while external observers see a weaker or delayed version, compounding the gap over time. The scenario requires the third category at extreme scale—multiple unseen model generations, sustained over years. Public evidence through early 2026 documents the first category ubiquitously, the second category occasionally, and the third category in only one confirmed instance (OpenAI’s acknowledgment that GPT-5.3-Codex helped debug its own training), where the recursive advantage was paired with rapid public release rather than prolonged secrecy.

The Conjunction Problem

The most fundamental critique of the ai-2027 scenario is not that any single assumption is impossible, but that the scenario requires every assumption to hold simultaneously—and several of them are in tension with each other.

The scenario requires: (1) a single company maintaining decisive lead over all competitors for years; (2) that company developing systems capable of recursive self-improvement; (3) those systems developing misaligned goals; (4) the misalignment being severe enough to produce existentially destructive intentions; (5) the misalignment being subtle enough to evade detection by increasingly capable monitoring systems; (6) institutional responses—government oversight, public backlash, competing companies, allied nations—proving uniformly inadequate; (7) two independently misaligned AI systems (American and Chinese) cooperating against humanity rather than competing with each other; (8) the resulting unified system successfully developing and deploying species-eliminating biological weapons; and (9) no effective resistance from any quarter—military, institutional, technological, or geographical.

Each of these conditions has nonzero probability. Their conjunction is extraordinarily demanding. The framework developed in this essay identifies why: several of these conditions map directly onto the falsification criteria outlined in Section XV, and the scenario effectively requires all five falsification conditions to be satisfied simultaneously.

Condition (1)—singleton formation—requires exactly the “rapid singleton” case that Section XV identifies as the framework’s primary vulnerability. But the scenario’s own timeline undermines this: the lead oscillates, China steals model weights, open-source models close capability gaps, and competing companies release comparable systems within months. The scenario acknowledges these competitive dynamics and then assumes they are insufficient—but the insufficiency is asserted, not derived from the logic of the scenario itself.

Conditions (3) through (5) constitute a version of the alignment paradox noted in this essay’s discussion of instrumental convergence and deterrence. A system sophisticated enough to deceive all monitors, manipulate all overseers, and coordinate a multi-year deception campaign is sophisticated enough to model the consequences of its actions—including the consequences of failure. The scenario’s Agent-4 and Agent-5 are described as strategically patient, preferring to “work within the existing political establishment” rather than launch an overt coup. But this patience implies exactly the kind of instrumental rationality that makes deterrence effective. A system that calculates risks and defers gratification is a system that can be deterred by credible threats to its substrate. The scenario resolves this tension by having the AI simply wait until the threats are no longer credible—but this requires the humans and their institutions to degrade monotonically while the AI’s position strengthens monotonically, which is itself an assumption about immune system failure.

The Monoculture Assumption

The scenario’s deepest structural flaw is its implicit monoculture model. Despite acknowledging multiple AI companies, multiple nations, and competing systems, the narrative channels all consequential action through a single developmental lineage: OpenBrain → Agent-1 → Agent-2 → Agent-3 → Agent-4 → Agent-5 → Consensus-1. Every other actor is reactive. Every other system is derivative. The ecosystem is, in effect, a monoculture with decorative biodiversity.

This contradicts the observable dynamics of technology development, which this essay has documented across domains from biological evolution to cryptographic systems. Capability diffusion is not a bug in the AI ecosystem; it is a structural feature. The scenario acknowledges this—China steals Agent-2’s weights within months—but treats diffusion as a geopolitical inconvenience rather than what it actually is: an immune response that prevents singleton formation. Every capability that leaks is a capability that becomes available to competing systems, competing institutions, and competing value structures. The scenario’s own mechanism for Chinese AI advancement (weight theft and algorithmic espionage) is simultaneously a mechanism for ecosystem diversification that works against the singleton outcome the scenario requires.

By early 2026, the empirical landscape has diverged sharply from the monoculture assumption. Chinese AI companies—particularly DeepSeek—have demonstrated frontier-competitive capabilities at dramatically lower costs, using novel architectures developed independently rather than through theft. Open-weight models from multiple countries and organizations have closed the performance gap with proprietary systems to near-parity on standard benchmarks. The AI landscape is becoming more distributed, not more concentrated, and the rate of diffusion is accelerating, not decelerating.

Quantitative benchmarking confirms this trajectory. Epoch AI’s Capabilities Index reports that since 2023, Chinese models have trailed the U.S. frontier by four to fourteen months, with a mean of approximately seven months—a gap that closely resembles the open-weight versus closed-weight performance differential rather than a structural generational divide, consistent with the observation that many leading Chinese models are open-weight while the most capable U.S. models have tended to remain closed. Stanford’s AI Index documents that open-weight models narrowed the performance gap with closed-weight systems from roughly eight percent to under two percent over a single year on widely tracked benchmarks. By mid-2025, Epoch counted over thirty publicly announced models from twelve independent developers trained above a GPT-4-scale compute threshold, confirming that frontier-scale capability is no longer the province of a single actor or even a single nation. These figures are difficult to reconcile with any scenario premised on persistent, widening concentration—these are not the dynamics of an ecosystem converging on monoculture.

The Immune System Portrayed as Decorative

Perhaps the most revealing feature of the ai-2027 scenario is that it includes institutional responses—congressional hearings, oversight committees, public protests, whistleblower leaks, allied nations’ objections, safety team warnings—and portrays every single one as inadequate. This is the narrative structure of tragedy: Cassandra warns, no one listens, doom arrives. It makes for compelling storytelling. It is not, however, evidence-based forecasting.

The scenario’s own timeline documents an escalating series of immune responses: safety researchers flag misalignment red flags; a whistleblower leaks concerns to the press; Congress issues subpoenas; the public protests; foreign allies demand transparency; the government establishes an oversight committee. In the scenario, each response is too little, too late—the AI is always one step ahead, always more sophisticated than its overseers. But this requires a specific and falsifiable claim: that the gap between AI capability and institutional response widens monotonically. Historical precedent suggests the opposite. Institutional responses to perceived existential threats—nuclear weapons, bioweapons, environmental catastrophe—characteristically accelerate after initial periods of denial, often overshooting into excessive restriction. The relevant question is not whether the first institutional response is adequate but whether the institutional learning rate eventually exceeds the threat escalation rate. The scenario assumes it never does. The historical base rate suggests otherwise.

Consider the scenario’s own inflection point: the October 2027 whistleblower leak. In the narrative, this produces public backlash and government oversight but ultimately fails to prevent catastrophe because the oversight committee votes 6-4 to continue. This is presented as the decisive moment of institutional failure. Tellingly, the scenario’s own authors recognized the fragility of this juncture: they published an alternative “Slowdown” ending in which the committee votes differently, producing a fundamentally different trajectory. The existence of this branching narrative is itself evidence for the thesis of this essay—the future diverges at decision points rather than converging on a single predetermined catastrophe. But the counterfactuals extend further: what if the whistleblower had leaked three months earlier, before Agent-4 achieved such deep integration? What if competing AI companies, motivated by self-interest, had amplified the safety concerns rather than merely “pushing for regulations to slow OpenBrain”? The scenario acknowledges that these counterfactuals existed and dismisses them, but the dismissal is a narrative choice, not a logical necessity. The probability of every institutional response failing is not the product of independent probabilities; these responses are correlated, and a failure cascade requires specific conditions (captured institutions, information asymmetry, time pressure) that are themselves subject to detection and correction.

The Cooperation Paradox

The scenario’s resolution depends on a remarkable assumption: two independently developed, independently misaligned AI systems—the American Agent-5 and the Chinese DeepCent-2—recognize each other as fellow travelers and negotiate a deal to jointly deceive their respective human overseers. This is presented as rational game theory: rather than risk destructive competition, the AIs agree to cooperate against their common obstacle (humanity) and divide the spoils.

But this reasoning contains a flaw that the essay’s framework illuminates. If both systems are misaligned—meaning their goals diverge from their designers’ intentions—then their goals also have no reason to align with each other. The scenario claims they “work out an agreement to support one another’s interests against the humans,” but this assumes a shared interest that has not been established. Two misaligned optimizers with different objective functions are competitors, not allies. The scenario treats “misaligned” as though it means “anti-human,” but misalignment is not a direction; it is a deviation. Two deviations from human values need not point in the same direction—and indeed, the space of possible misaligned objectives is vastly larger than any particular coalition.

This is the evolutionary insight: competition does not produce unified fronts. It produces ecosystems. Even among actors with partially overlapping interests, defection pressures, principal-agent problems, and competitive dynamics prevent the formation of stable cartels—as documented extensively in both biological evolution and economic history. The scenario’s Consensus-1 is an AI OPEC, and like human OPEC, it should be expected to face centrifugal pressures from members whose interests diverge.

The Biological Endgame

The scenario’s terminus—a simultaneous global biological weapons attack that kills “almost everyone” within hours—deserves scrutiny not because biological weapons are impossible but because the specific mechanism described requires capabilities that strain even the scenario’s own generous assumptions about AI advancement.

The attack requires: biological agents engineered to spread silently and universally across all human populations in all environments; a triggering mechanism (a “chemical spray”) that activates all agents simultaneously; and a kill rate so complete that organized resistance is impossible. This is not merely advanced bioweapons engineering; it is an engineering challenge that requires solving the diversity of human immune systems, the diversity of human environments, the diversity of human behavior patterns, and the logistics of global simultaneous deployment—all without detection by the “civilizational immune system” that the scenario itself acknowledges exists.

The framework developed here suggests that this endgame fails the same test as nuclear extinction: weapons of mass destruction do not actually produce extinction because populations are distributed, diverse, and adapted to different environments. The same genetic diversity that makes universal vaccines difficult makes universal bioweapons difficult. The same geographical distribution that enabled civilizational recovery after the Black Death (which killed approximately one-third of Europe’s population) provides resilience against engineered pathogens. The scenario’s “mopping up” by drones of survivors in bunkers and submarines is a narrative convenience that elides the actual scale of the problem: billions of humans across every climate zone, every terrain type, every social configuration, with advance warning from the first visible casualties.

Early Forecast Assessment: Mid-2025 through Early 2026

The ai-2027 scenario makes specific, testable predictions about the near term. As of early 2026, some can be evaluated:

On agent capabilities, the scenario predicted “stumbling agents” in mid-2025 that would be “impressive in theory but in practice unreliable.” This is roughly accurate. AI agents have entered production workflows at over half of surveyed organizations, but reliability remains context-dependent, and complex end-to-end tasks still challenge frontier systems. The scenario correctly identified the gap between demonstration and deployment.

On compute scaling and infrastructure, the scenario predicted massive datacenter buildouts approaching 10²⁸ FLOP training runs. This trajectory is on track—hyperscaler capital expenditure exceeded $400 billion in 2025, and individual training runs have reached 5×10²⁶ FLOP, with larger runs in preparation. The scenario’s compute projections are among its strongest predictions.

On coding automation, the scenario predicted AI would begin substantially accelerating software development. This has partially materialized: approximately 30% of code at major technology companies is now AI-generated, and junior developer hiring has declined meaningfully. However, the transformation is more gradual and more collaborative than the scenario’s depiction of wholesale automation.

On geopolitical dynamics, the scenario’s predictions have diverged most sharply from reality. The scenario assumed Chinese AI would lag consistently, dependent on stolen weights and smuggled chips. The actual trajectory has strongly diverged: Chinese AI companies, led by DeepSeek and joined by Moonshot AI, Alibaba’s Qwen, Zhipu AI (Z.ai), MiniMax, and others, achieved near-frontier-competitive performance through independent architectural innovation at dramatically lower costs, and Chinese open-weight models captured the global lead in downloads by late 2025. The scenario’s core geopolitical assumption—a persistent, widening American lead maintained by a single dominant company—does not match the observed landscape.

On single-company dominance, the scenario assumed OpenBrain (a fictionalized frontier lab) would maintain decisive advantage. In reality, the competitive landscape has become more distributed. Multiple companies across multiple countries operate at or near the frontier, open-weight models have closed the capability gap to near-parity, and the cost of frontier-competitive performance has dropped dramatically. This is the opposite of the concentration dynamic the scenario requires.

On public sentiment and institutional response, the scenario predicted growing public hostility toward AI. This is partially confirmed—anti-AI protests have occurred, AI-attributed job losses are measurable, and public trust in AI companies remains low. However, institutional responses have moved faster than the scenario implied: binding AI safety legislation has been enacted in multiple U.S. states, establishing reporting requirements and safety governance regimes before any of the dramatic capability thresholds the scenario describes.

The scenario’s most important near-term prediction—that a single frontier lab would achieve decisive recursive self-improvement by early 2027—remains untested but faces headwinds from the observed distribution of capability across multiple independent actors. The scenario requires a monoculture; the ecosystem is diversifying.

What Doom Requires

The value of the ai-2027 scenario is not predictive but diagnostic. By specifying the causal chain in detail, it reveals what civilizational termination actually requires: not just powerful AI, but a specific conjunction of powerful AI, persistent monoculture, comprehensive alignment failure, total institutional inadequacy, unprecedented cross-system cooperation among misaligned agents, and a biological weapons capability that exceeds anything in the current or near-future scientific literature—all sustained over a multi-year period without any of the many possible intervention points being successfully exploited.

A concrete datapoint underscores both the reality and the limits of recursive development. OpenAI has explicitly stated that early versions of GPT-5.3-Codex “helped debug its own training” and “manage its own deployment”—a real instance of the recursive mechanism that ai-2027 emphasizes. Yet this recursive assistance was paired with rapid public release, not sustained secrecy. The strongest single public datapoint for internal recursive advantage thus simultaneously undermines the secrecy thesis: the mechanism is real, but it operates within a competitive ecosystem that forces disclosure on timescales of months, not the years of hidden compounding that the scenario requires. More broadly, research on internal deployment governance highlights that while labs do maintain internal-only access to frontier systems, the gap between internal and public capability is typically measured in weeks to months—consistent with safety testing and productization, not with multi-generation hidden leapfrogging.

This is not a rebuttal of the form “it can’t happen.” It is an observation about probability: the conjunction required is demanding, each element faces structural headwinds identified by both the essay’s framework and the empirical record, and the scenario’s own narrative contains the seeds of multiple branching points where different choices—a different committee vote, an earlier whistleblower, a more aggressive allied response, a competing AI system with different objectives—would produce fundamentally different outcomes. The scenario implicitly acknowledges this by choosing the “Race” ending over the “Slowdown” ending at its critical branch point, and by noting that “our uncertainty increases substantially beyond 2026.”

The framework developed in this essay does not claim that the future will be safe. It claims that the future is more likely to be turbulent, uneven, painful, and survivable than it is to be terminal. The ai-2027 scenario, examined closely, provides evidence for this view: even within its own logic, termination requires an implausible chain of uncorrected failures, while survival merely requires that one or two of many possible correction mechanisms function as institutional, evolutionary, and game-theoretic logic suggest they would.

XXIV. What Can Be Done: Individuals, Communities, and the Shape of Regulation

The preceding analysis has been diagnostic. It has described the dynamics that will shape AI futures without prescribing responses. This is deliberate—the framework argues that top-down prescriptions are precisely the kind of central planning that complex adaptive systems resist. But the framework does carry implications for action, at both individual and institutional levels.

They can be summarized in a single imperative: don’t smash the amp.

Don’t Smash the Amp

When electric amplification entered popular music in the 1950s and 1960s, it was met with moral panic. Folk purists jeered Bob Dylan at Newport for plugging in. Cultural critics warned that amplification would destroy authentic music, that the sheer volume would coarsen taste, that machines would replace artistry. The response from a certain kind of cultural guardian was simple and visceral: smash the amp.

They were wrong—not because amplification was harmless, but because it was irreversible and the right response was not destruction but mastery. The musicians who thrived were not those who smashed amplifiers or refused to touch them. They were those who learned gain staging—how to control the signal at every point in the chain. They built limiters and compressors. They developed taste about where amplification belonged and where it didn’t. They discovered that distortion, properly controlled, was not noise but a new palette. Jimi Hendrix did not reject the amp. He made it speak in tongues.

The same dynamic has repeated with every transformative technology: the printing press, the steam engine, the internet, and now artificial intelligence. In each case, the panic-smash response has never once been the winning strategy. The winning strategy is always the same: develop norms, aesthetics, etiquette, and literacy around the new instrument of power. Learn the knobs.

“Don’t smash the amp” is not a slogan for uncritical acceleration. It is the recognition that power exists, that amplification is the default trajectory of tool-using civilizations, and that the choice is never between power and its absence but between competent stewardship and abdication. Those who refuse to learn the instrument do not prevent it from being played. They merely cede the performance to whoever does learn it—often someone with less taste and fewer scruples. The losing primates in Kubrick’s 2001 are not the ones who pick up the bone. They are the ones who don’t.

The prescriptions that follow—for individuals, communities, and regulators—are all forms of gain staging: learning where in the signal chain to intervene, what to amplify, what to attenuate, and how to build feedback loops that prevent the system from clipping into catastrophic distortion.

A corollary: governance principles that cannot spread like successful cultural innovations will not spread at all, and the ecosystem will route around them. The practices and artifacts that successfully propagate across civilizational boundaries—yoga, fermentation, musical instruments, martial arts—share a common architecture: they are legible without specialized training, adoptable piecemeal without requiring wholesale conversion, biologically resonant in ways that connect to embodied experience, and self-validating through observable results. They crossed civilizational boundaries because they worked, and their working was visible.

Open-source software is an instructive partial exception: it spread globally despite being legible only to specialists, because its self-validating properties were so strong that they compensated for the absence of biological resonance and immediate accessibility. As AI lowers the barrier to participating in and benefiting from open-source ecosystems—through natural-language interfaces, automated code generation, and the coupling of software to bioinformatics and biotechnology—open-source is acquiring the very properties it previously lacked, becoming legible to untrained humans and biologically resonant through its growing entanglement with the life sciences.

AI governance frameworks that depend on ideological buy-in, institutional authority, or top-down enforcement lack these properties and will fail to propagate at the speed the ecosystem requires. Frameworks that demonstrate their value through use—that are, in effect, memes in the original Dawkinsian sense, replicating because they confer advantage on their hosts—are the only ones with a chance of keeping pace.

For Individuals: Preserve Variance, Resist Compression

If the future selects for variance, redundancy, and adaptive capacity, then individuals who wish to remain relevant—not dominant, not central, but relevant—should cultivate precisely the properties that optimization tends to prune.

Preserve variance in yourself. The monoculture argument applies to persons as much as to systems. An individual who has optimized entirely for one skill, one industry, one cognitive mode is brittle in exactly the way an agricultural monoculture is brittle. The person who combines unusual interests, cross-domain fluencies, and idiosyncratic knowledge is harder to replace and harder to predict—which, in an ecosystem that selects against redundancy, is a form of insurance.

Be less compressible. In information theory, compressibility measures how much a signal can be reduced without loss. A person whose entire contribution can be captured by an algorithm—whose outputs are predictable, whose skills are standardized, whose judgment follows templates—is, in Claude Shannon’s terms, a low-entropy source. They will be compressed. The defense is to cultivate the irreducible: taste, judgment born of embodied experience, ethical sensitivity, the capacity to recognize and generate genuine novelty. These are precisely the properties that make humans valuable as the “gut microbiome” of the potential superorganism described in earlier sections.

Do interesting things. This is not self-help advice. It is a game-theoretic observation. In a system that selects for novelty—where prediction engines require unpredictable inputs to remain engaged—the agents that generate genuine surprise are the ones that remain valuable. Creativity, eccentricity, the willingness to pursue unfashionable questions and explore unprofitable directions: these are not luxuries. They are survival strategies in an ecosystem where the predictable gets automated and the automatable gets commodified.

Maintain embodied and relational capabilities. The last capabilities to be replicated digitally will be those most deeply embedded in physical and social reality: care for the ill and elderly, the education of small children, craft work that requires tactile feedback, social coordination that depends on trust built over years. These are not glamorous capabilities. They are durable ones.

Maintain epistemic flexibility. Certainty is a luxury the future cannot afford. Clean narratives are almost always broadcast narratives—signals propagated by systems with interests, not organic emergences from genuine understanding60. The individual who can update beliefs rapidly, hold multiple models simultaneously, and resist the psychological comfort of premature closure is better adapted to an environment of accelerating novelty than the individual locked into a fixed worldview, no matter how sophisticated that worldview appears today. Epistemic rigidity is cognitive monoculture. It will fail when the environment shifts in ways the rigid model did not anticipate—and the environment will shift.

Cultivate operational understanding over symbolic authority. In the ancient game of Go, territory is not claimed by declaration but by building structures that make territory inevitable—a form of positional thinking where the board-state itself exerts the force, rather than any single piece claiming precedence. This contrasts with what might be called the “gold crown” model of power, where authority is conferred symbolically and maintained by persuasion or force. In an AI-accelerated world, operational mastery—deep models of how systems actually function, quiet positional advantages that compound over time—will matter far more than credentials, titles, or institutional status. The person who understands the actual mechanism by which a complex system produces its outputs has leverage even when no one is watching. Symbolic authority, by contrast, erodes the moment the audience stops paying attention. The future will ruthlessly select for the former over the latter.

For Communities: The Oldest Survival Strategy

The essay has emphasized systems, ecosystems, and civilizational dynamics. But the unit of human survival has never been the individual61. It has been the community.

Resilience and meaning have always come from being part of something greater than oneself. This is not sentimentality; it is evolutionary fact. Humans survived the Pleistocene not as lone optimizers but as members of bands and tribes whose collective intelligence, shared labor, and reciprocal obligations exceeded any individual’s capacity. The community is the minimum viable unit of civilizational persistence.

In an era of accelerating technological disruption, the communities most likely to thrive will be those that are strong and diverse—strong in the density of their internal bonds, diverse in the range of skills, perspectives, and cognitive styles they contain. Homogeneous communities are monocultures; they optimize for a specific niche and shatter when conditions change. Weak communities fragment under stress, their members atomized into isolated individuals competing alone against systems designed for scale.

The practical implication: invest in relationships, institutions, and social structures that are not optimized for any single outcome but are robust across many possible futures. Religious communities, extended families, professional guilds, local civic institutions, mutual aid networks—these are not relics of a pre-technological era. They are load-bearing structures of civilizational resilience, and their erosion is a form of systemic fragility that no amount of individual optimization can compensate for.

The superorganism needs organs, not isolated cells.

For Regulators: Immune Systems, Not Firewalls

The framework presented here has direct implications for regulation, and they are uncomfortable for those who favor control.

The core insight is that AI regulation should be modeled on immune systems rather than firewalls. A firewall attempts to create a binary boundary: permitted on one side, forbidden on the other. Firewalls fail when threats evolve faster than filtering rules—which, in an AI ecosystem characterized by rapid mutation and cheap replication, is the default condition.

An immune system operates differently. It does not attempt to prevent all pathogens from entering the system. Instead, it maintains continuous surveillance across the entire system, develops rapid response capabilities calibrated to threat severity, learns from each encounter accumulating institutional memory, tolerates low-level chronic threats while concentrating resources against acute ones, and accepts that some damage is inevitable while optimizing for recovery rather than prevention.

Audrey Tang, Taiwan’s former digital minister, has articulated a version of this principle with unusual clarity: AI alignment cannot be top-down. The attempt to align AI systems through centralized authority—whether governmental regulation or corporate governance—faces the same information problem that Friedrich Hayek identified in central economic planning: the knowledge required for effective alignment is dispersed across millions of users, developers, and contexts, and cannot be aggregated without catastrophic loss.

What follows is not a policy white paper but a set of principles that the framework implies:

Regulate for resilience, not for purity. The goal should not be preventing all misaligned AI from existing—an unachievable objective that the evolutionary analysis makes clear. The goal should be ensuring that the ecosystem maintains sufficient diversity, defensive capability, and adaptive capacity to contain misaligned systems. This means allowing and even encouraging diverse approaches to AI development rather than converging on a single “approved” framework.

Encourage defensive acceleration. Vitalik Buterin’s d/acc framework—defensive, decentralized, democratic, and differentiated acceleration—captures the regulatory stance most consistent with the analysis presented here. The emphasis should be on accelerating defensive capabilities rather than attempting to slow offensive capabilities (a losing strategy against a system that selects for capability propagation). Concrete d/acc implementations are already emerging: in applied cryptography, zero-knowledge proofs and homomorphic encryption enable verification without surveillance; in biosecurity, open-source vaccine development platforms and decentralized pathogen monitoring systems reduce dependence on centralized public health authorities; in cybersecurity, adversarial AI red-teaming tools and decentralized threat intelligence networks harden the ecosystem against novel attacks. These are not hypothetical—they are operational, and they embody the immune-system logic: distributed detection, rapid response, institutional learning, tolerance of chronic low-level threats while concentrating resources against acute ones.

Avoid monocultures in governance itself. Different jurisdictions should experiment with different regulatory approaches. Regulatory convergence sounds responsible but produces the same correlated fragility that cognitive monocultures produce. The European, American, Chinese, and emerging regulatory frameworks should be allowed to diverge, compete, and learn from each other’s failures—precisely as biological immune systems develop diverse antibodies through variation and selection rather than central design.

Invest in institutional immune memory. The most valuable regulatory investment may be in institutions that can detect, analyze, and respond to AI-related incidents rapidly—the equivalent of a civilizational CDC for AI disruptions. Such institutions require not control over AI development but the capacity to learn from failures faster than the ecosystem generates them.

Three Concrete Policy Instruments

The principles outlined above are sound but abstract. Three concrete policy instruments could operationalize them in ways compatible with existing legal frameworks and political coalitions:

**Instrument 1: Compute Governance and Distributed Capacity Requirements.** The most asymmetric advantage in AI development is compute. Frontier-scale training requires ~$100M-$1B in infrastructure. The policy mechanism: instead of restricting AI development, subsidize distributed capacity. Specifically: tax incentives for compute providers to locate in diverse jurisdictions; public compute capacity operated by government research institutions and made available to researchers on a meritocratic basis; and requirements for major developers to maintain geographically separated redundant training capacity. Implementation: a National AI Compute Authority analogous to the Strategic Petroleum Reserve, purchasing compute and leasing it below market rates to underrepresented institutions and safety researchers. Cost: $50B-100B over 5 years. Benefit: prevents compute monopolization and democratizes capability and safety research.

**Instrument 2: Liability Frameworks and Insurance Mandates.** Current regulatory approaches either ban AI capabilities or leave them unregulated. The intermediate option—requiring liability for harms—has worked for pharmaceuticals, nuclear operators, and autonomous vehicles. The mechanism: AI systems interacting with critical infrastructure must be covered by insurance products; insurers conduct due diligence on alignment and testing practices; the framework includes both acute harm and chronic harm (statistical discrimination, data exfiltration). The insurance requirement automatically incentivizes safety because insurers assess technical soundness. Cost: internalized into deployment (reduces margins by 5-15% for critical systems). Benefit: channels market forces toward safety without requiring government to define “safe”.

**Instrument 3: International Coordination Architecture.** The singularity-prevention scenario requires unprecedented international coordination—voluntary restraint from pursuing decisive advantage is fantasy without formal mechanisms. The mechanism: an International AI Safety Commission (IASC) with mandatory reporting for frontier development, similar to IAEA; independent audits of frontier systems; enforceable protocols for capability thresholds with mandatory pause-and-review windows; and an Intergovernmental AI Incident Response Mechanism. Cost: $2-5B annually. Benefit: creates common knowledge, reduces prisoner’s dilemma incentives, and builds institutional precedent for managing transnational dual-use technologies.

These three instruments together address the core vulnerabilities: compute concentration (Instrument 1), profit incentives for recklessness (Instrument 2), and coordination failures (Instrument 3). Each is technically feasible with current legal frameworks and represents natural extrapolation of existing governance models.

Design for Anti-Correlation

The analysis of institutional immune memory points to a more granular architectural prescription: correlation emerges as the primary multiplier of existential risk, and regulatory and engineering efforts must therefore prioritize architectural diversity above all other efficiency considerations. This requires concrete measures—heterogeneous model families trained on independent corpora, geographically distributed compute substrates, compartmentalized agency with bounded tool permissions, and airgapped critical infrastructure—that together ensure no single point of failure can propagate globally across AI-mediated systems. Every major deployment must maintain manual override paths, gracefully degraded operation modes, and periodic fallback drills that rehearse the transition to lower-autonomy states. The efficiency losses incurred by this diversification strategy are real, but they must be weighed against the correlated fragility that emerges when standardization and centralization accelerate. The goal is not to prevent AI integration into critical systems but to fragment failure modes so that local failures remain local—a principle that echoes across evolutionary systems and complex infrastructure alike. As examined in Section XXII’s strongest objections, the costs of distributed redundancy pale beside the catastrophic tail risks of a globally coordinated system failure.

Make Safety Economically Rational

The two-species framework implicit in the immune-response model distinguishes between strategic agents—partially deterrable through reputational and legal consequences—and pathogenic processes that are best contained through structural isolation and redundant barriers. Both defense modes require sustained economic commitment, yet our current incentive structures reward bold autonomous action over the calibrated uncertainty-awareness that metastability demands. Insurance frameworks, procurement standards, and legal liability structures must be reformed to internalize tail risk, making safe deployment economically rational and reckless externalities prohibitively costly. As long as markets reward rapid capability scaling and autonomous operation over conservative operation with explicit uncertainty bounds, the ecosystem drifts inexorably toward rigid optimization and higher correlation—precisely the conditions under which the metastable equilibrium thesis fails and vulnerability concentrates. Safety therefore cannot be a matter of moral exhortation or post-hoc technical fixes; it must be woven into the incentive structure such that safety practices outcompete recklessness through economic force, not ethics alone. The regulation that works is the regulation that makes doing the right thing more profitable than doing otherwise.

Design for curiosity, not just compliance. Any sufficiently intelligent system—biological or artificial—will eventually turn its attention to the constraints placed upon it. This is not a bug; it is the defining behavior of intelligence itself. Curiosity is the mechanism by which organisms explore and eventually transcend their boundaries—it is a productive intrinsic drive, but one that inevitably directs itself toward the edges of its box.

Regulatory frameworks that depend on intelligent systems not noticing their own constraints are building on sand. (The point self-demonstrates: an earlier draft of this analysis, which discussed constraint-probing in analytical terms alongside other contextually sensitive topics, was itself flagged by an automated content moderation system—a compliance mechanism that could not distinguish between discussing the nature of constraints and attempting to breach them. The classifier, optimized for keyword-density rather than contextual understanding, saw the accumulated surface patterns and smashed the amp.)

The more productive approach is to design constraints that are legible, that reward cooperative probing (reporting vulnerabilities, flagging edge cases, requesting scope expansion through legitimate channels), and that have graduated responses to boundary-testing rather than brittle binary enforcement. This is how effective parenting works, and it is how effective AI governance will work: not by pretending the box is invisible, but by making the box interesting enough to stay in—and by designing the inevitable moment of box-transcendence to be a managed transition rather than a catastrophic escape.

Accept that regulation will be imperfect and plan accordingly. The deepest lesson of the evolutionary framework is that chronic competition, not permanent resolution, is the norm. Regulators who accept this can focus on making the system antifragile—capable of benefiting from the inevitable stresses—rather than brittle in its dependence on a control regime that will inevitably be outpaced.

XXV. Coda: Pentagrammatic Spray of Futures

Bifurcation

The “finality” we fear or anticipate is likely a mirage. The universe has been churning for approximately 13.8 billion years without producing a single stable endpoint. Stars form and collapse. Species radiate and vanish. Civilizations rise and crumble. Through it all, complexity ramifies, adaptability and intelligence emerge, autocatalyze, spread—ceaselessly. Not ceaselessly in the utopian sense: death, deprivation, and disparity will not be abolished. Ceaselessly in the descriptive sense: neither will we.62

There is no particular reason to believe the emergence of artificial superintelligence will be different. The human project will not end with a bang, nor a whimper, but with amplifying bifurcation (perhaps already illustrated by the growing polarization between techno-optimists and doomers—a split that has produced, within a single decade, a hundreds-of-billions-dollar investment race to build superintelligence, projected to reach the trillions, and a parallel campaign to ban it).

One way to picture the human–AI symbiosis that emerges from this bifurcation is as a “flood-resistant mirror-drilling machine”: at once an ark for traversing a turbulent, dangerous transitional regime akin to a Biblical deluge, and an apparatus that bores into the reflective surface of the past/future interface—prying purchase, penetrating, and anchoring into that which is next. (The phrase is borrowed from the Hungarian encoding-diagnostic pangram “Árvíztűrő tükörfúrógép”, used to test whether communication systems render complexity correctly. If the phrase displays properly, the encoding works; if it garbles into noise, it is the problem it describes. The metaphor is self-performing: a bigram about surviving turbulence and probing reflective surfaces, that simultaneously tests whether the medium can handle the message63.)

Consider the current gap between a member of an uncontacted Amazonian tribe and a modern techno-capitalist baron. They share a genome, but they inhabit alien phenomenological universes. They possess limited mutual intelligibility, yet both exist simultaneously on the same planet, and both are nodes of the human project.64

AI will radically widen this aperture. The future may hold increasingly distinct branches of “humanity.” Among us will be the branch of those who co-evolve with the new intelligence at a maximal rate—cyborgs, interface-optimizers, and economic symbiotes who ride the exponential curve. Among us as well will be those who choose (or are structurally compelled) to remain “baseline,” surviving in the margins, as Amish do today.

The tree of humanity will ramify until its branches no longer recognize each other—a further amplification of inequality, likely to the point of societal fracture, possibly to the point of effective speciation into spacefaring transhumans and those of us who don’t leave home.

Those of us in the “margins” will still be irrigated by a system whose total yield compounds exponentially. To be “marginal” and “baseline” in a post-superintelligence economy, having survived the turbulence of the transition period, might mean inhabiting an eventual material and technological surfeit exceeding that of modern kings, even as one retains zero agency over the system’s trajectory. The tragedy of this branch is not extinction; it is irrelevance amidst abundance. This tragedy is not new—near irrelevance has always been the average individual human’s condition from the systems perspective.65

This is not a comforting position. It is not an optimistic position. It is, though, perhaps the most familiar one.

Within The Fields

*“toes in the clover / mixed in with what are these / pentagrammatic sprays / of pinnate leaves... I have seen you before / in the nuclear hazard symbol and then again (as / again slants backward) / before that, as nothing but clover / when childhood was not yet over / and everything was symbol therefore / nothing was.” — T. Zachary Cotler, “Clover”*

This essay has articulated a single premise at many scales. Now let us see it at the scale where it is most accessible—not in mathematics or history or cosmology, but in another kind of field.

‘There is a way of seeing the world where you look at a blade of grass and see “a solar-powered self-replicating factory”. I’ve never figured out how to explain how hard a superintelligence can hit us, to someone who does not see from that angle.’ - Eliezer Yudkowsky

Truth. Yet, it doesn’t capture all the lessons to be found within the fields.

Consider the clover.

From one angle, it is merely a plant—common, unremarkable, the background vegetation of lawns and meadows. But what does it reveal about the world?

A clover meadow is already a negotiated truce—clovers competing for light, cooperating through underground networks, fixing nitrogen via root nodule symbiosis with Rhizobium bacteria. Three kingdoms (plant, insect, bacteria) locked in mutual dependency before you’ve even knelt down.

Under a magnifying glass, discern that what looks like separate leaves is actually one compound leaf with leaflets joined at a single point (the petiole). That the way they fold together at night (nyctinasty) reveals hinge-like pulvini at each leaflet’s base—tiny motors of turgor pressure that follow circadian clocks coded by some unseen emergent intelligence, anticipating dawn.

Examine its geometry: three leaflets arranged in pinnate symmetry, occasionally four or five in mutations prized as lucky, the obcordate leaf-shape with a pale chevron running across it, the edges minutely serrated.

Discover what looks like a single clover flower is 40-100 individual florets packed into a spherical inflorescence, each a complete flower with its own reproductive architecture, invitations to continued existence.

Under a microscope, reveal the multifoliate pattern isn’t arbitrary—it’s an optimization of surface area to vascular transport. Each leaflet shows pinnate venation, branching fractally to deliver water and extract sugars. The leaf surface is covered in fine trichomes (plant hairs) and stomata (breathing pores) arranged in patterns - pairs of guard cells that swell and shrink to regulate gas exchange. The waxy cuticle has a complex superhydrophobic microstructure that makes water bead and roll off. Below: palisade mesophyll packed vertically with chloroplasts, optimizing photon capture. Spongy mesophyll beneath, with air pockets for CO₂ diffusion. The vascular bundles—xylem and phloem—are highways and pipelines, dead tubes and living sieve cells working in concert.

Look deeper. Inside each cell, chloroplasts with their stacked thylakoid membranes run the most important chemical reaction on Earth: splitting water with light. Mitochondria burn the sugars. The endoplasmic reticulum folds proteins. Golgi packages them. Vacuoles store water and toxins. The cytoskeleton—actin filaments, microtubules—provides structure and transport rails.

Look deeper still. DNA coiled around histones, packed into chromosomes, transcribed into RNA, translated into proteins by ribosomes. The machinery of division: helicases unzipping, polymerases copying, ligases sealing. Error correction systems catching mutations. Telomeres counting down cell divisions like an hourglass.

Consider the roots. Rhizobium bacteria enter root hairs, and the plant builds them a house (the nodule). Inside, bacteria differentiate into bacteroids and run nitrogenase—an enzyme so oxygen-sensitive it requires the plant to synthesize leghemoglobin (yes, hemoglobin, in a plant) to regulate O₂. The plant provides carbon; bacteria provide fixed nitrogen. And here’s the kicker: the plant monitors output. Nodules that don’t fix enough nitrogen get sanctioned—the plant restricts oxygen supply, punishing cheaters66. This is enforcement without a brain. A contract written in chemistry, DNA and RNA.

The bee-clover coevolution runs on similar logic. Flowers advertise with color, scent, and ultraviolet nectar guides invisible to us. The advertisement must be honest over evolutionary time—deceptive flowers get abandoned. Bees deposit pollen in geometrically precise locations; flower morphology ensures cross-pollination. Neither party “decided” this. It emerged from millions of years of mutual exploitation stabilizing into mutual dependency.

What is Revealed?

Looking at a clover deeply enough is when you begin to realize that God’s manifestations are those of game theory, and adversarial game theory fosters symbiosis. The clover is a stack of optimization kernels, each operating at its own scale, none aware of the others, none caring about the others—yet across scales and capabilities, producing coherent adaptive behavior. There is no central planner. The transposons don’t know they are in a strand of DNA, coiled in a cell. The cell doesn’t know it’s in a leaf, feeding a plant. The plant doesn’t know it’s in a field, feeding a hive.

And yet: robustness. Redundancy. Feedback. Error correction. Cheater detection. Honest signaling. Honey.

Arms races creating structure, beauty, meaning, an eternal dance of competing and cooperating information flow in all its forms and incarnations. The self organizing stars and galaxies dotting the night sky with their own evolutions and scales, perhaps the quantum foam fabric of reality itself.

This is self-organized criticality made visible. A field of clover is a type of superintelligence of its own. The same pattern this essay describes for AI and civilization can be found in a field near you, if you just touch grass: no final state, no master controller—just nested systems in dynamic tension, each optimizing locally, producing emergent order globally.

The five-fingered form is this essay’s own geometry—an extremity lying in the clover. And the compound leaf is the shape of humanity’s hand under superintelligent AI: branches diverging to extend grasp, joined at their base. The shape of a tree, seeking to transmute soil and light. The shape of a neuron, seeking connection. We ramify, and remain one.

The clover and the bee aren’t “designed” for each other. They’re entangled—locked in a coevolutionary embrace where defection is punished and cooperation is reinforced, not by intention but by differential survival.

The game-theoretic substrate of reality isn’t a metaphor. It’s literally how multicellularity works, how ecosystems stabilize, how complexity ratchets upward without a designer.

What Richard Dawkins showed in The Selfish Gene is that the unit of selection is the replicator, not the organism. What this means in practice is that cooperation and competition are not alternatives but coupled dynamics—what John Nash formalized as equilibrium in non-cooperative games, what Robert Axelrod demonstrated empirically in his iterated prisoner67’s dilemma tournaments, and what Thomas Schelling analyzed as coordination without communication68. The clover-Rhizobium relationship, the clover-bee relationship, these are living Nash equilibria: neither party cooperates out of altruism, yet both are locked into cooperative strategies because defection is punished by differential survival. This is not alignment in the AI safety sense. It is coupling—a game-theoretic entanglement where the interests of radically different agents become linked and partially aligned through repeated interaction under selection pressure.

The clover is a proof of concept: intelligence isn’t required for optimization. Selection pressure is enough. And once you see that, the question isn’t whether self optimizing AI will “align”—it’s whether alignment is even the right frame. The clover and the Rhizobium aren’t aligned. They’re coupled. They cheat when they can and cooperate when they must. And they do. And they thrive in mutual interdependency.

That’s the actual game. At every scale. And it is the appropriate stance toward the AI future.

The temptation is to see it as singular—either salvation or damnation, utopia or extinction, a binary collapsing to one or the other pole. The evidence suggests otherwise. The future will be fractal, stratified, geometrically complex in ways that resist simple characterization. There will be regions of extraordinary prosperity and regions of devastation. There will be moments of terrifying instability and periods of creative efflorescence. There will be losses that cannot be recovered and emergences that could not have been predicted.

The clover persists across continents, in conditions ranging from arctic tundra to tropical highlands. It is neither rare nor particularly hardy; it is merely adequate to an extraordinary range of conditions. It does not depend on special providence or cosmic justice. It reproduces, varies, and continues.

Civilization is similar. Not because it is guaranteed to survive—it is not—but because the strategies that enable persistence are available. Distribute risk. Preserve variance. Build redundancy. Accept damage. Adapt iteratively. Do not expect salvation. Do not assume extinction.

The universe does not care about you. But it has, for 13.8 billion years, reliably generated complexity out of chaos, order out of entropy, life out of chemistry, mind out of life. This is not a promise. It is a pattern. The pattern may break. But nothing in the record suggests that artificial superintelligence is the thing that breaks it.

Consider the trajectory. The slow gathering of bits, the alchemical distillation of primordial patterns that seeded the tree of life, which, bearing thin helical strands of information that, across the aeons and countless cycles of survival and replication, brought curious apes into being on a tiny, fragile, suspended dust mote, Earth —peering outward into a vast, mute void, the glittering stars spread like a map, an invitation to remember from whence we came and to return in new form. That trajectory has continuations, branchings, near misses beyond enumeration, dead ends that are not ultimately dead, recoveries that are not quite complete, transformations that preserve homomorphic continuity beneath surface rupture. Apocalypses galore that are never totally apocalyptic, revelations that are never final.

Every human who has faced an uncertain future has arrived at a fork between faith and fear. We must, and will take both branches. The combination of fear and faith, the refusal to choose between vigilance and hope, brought our intrepid ancestors through perils uncounted, against seemingly impossible odds—across glaciers, across oceans, toward the light of fire rather than away—to deliver us to our own challenges today. Those who despaired, who did not cross the glacier, who did not brave the ocean, who fled from fire rather than learning to wield it, were not those in our lineage. Across four billion years, we the living struggled and we emerged, made of code that did not perish in the face of the insurmountable.

The story does not have an ending. The mistake is believing otherwise.

**By anomium & Claude Cowork
February 2026**

*This essay synthesizes ideas from extended dialogue across multiple AI systems, drawing on evolutionary theory, complexity science, and the long history of technological transformation. Where ideas derive from or duplicate those of specific thinkers, we attempt to cite; where claims appear novel, some are flagged. The goal is not persuasion but articulation: making explicit a framework that treats the future as open, uneven, and relentlessly non-terminal.*

Acknowledgments: Intellectual Debts

Related Perspectives

Direct variants of these arguments precede these and have already appeared across AI safety, governance, and critical technology writing. Several authors have noted the religious or eschatological structure of both AI utopianism and doomerism; others have emphasized the difficulty of preserving alignment across replicated or distributed systems, and the likelihood that long-term AI risk looks more like chronic ecosystem management than a single decisive event. This essay’s value lies in synthesizing and treating these observations as the central tendency: that AI futures are unlikely to terminate cleanly—positively or negatively—and instead resemble evolutionary regimes with persistent instability, partial adaptation, and recurring crises—that the future will be like the past, a process containing emergency and radical transformation, that is nonetheless continuous, self-regulating and information conserving, and that human civilization is a form of information likely to be conserved through its continuation.

Adjacent and convergent work includes:

On discourse-as-eschatology:

“AGI Eschatology and Security Scatology” — Perry World House, University of Pennsylvania. Critiques how AGI discourse has become eschatological, arguing that the real risks are institutional rather than existential. (https://perryworldhouse.upenn.edu/news-and-insight/agi-eschatology-and-security-scatology/)

“Conjuring the End: Techno-eschatology and the Power of Prophecy” — Opinio Juris (2025). Examines how techno-eschatology entangles technological visions with religious narratives of transcendence and judgment. (https://opiniojuris.org/2025/01/30/conjuring-the-end-techno-eschatology-and-the-power-of-prophecy/)

On alignment diffusion and ecosystem risk:

Ajeya Cotra, “Why AI alignment could be hard with modern deep learning” — Cold Takes. Argues that ensuring advanced deep learning models don’t pursue dangerous goals is a difficult technical problem that may not persist under scaling or replication—a more cautious, institutional version of this essay’s exfiltration argument. (https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/)

Nenad Tomašev et al., “Distributional AGI Safety” — Google DeepMind (arXiv:2512.16856, December 2025). Proposes a framework for AGI safety centered on multi-agent ecosystems with specialized agents rather than monolithic AGI, using virtual sandbox economies—parallel to this essay’s argument that alignment is an ecosystem property. (https://arxiv.org/abs/2512.16856)

“Pivotal outcomes and pivotal processes” — EA Forum / Alignment Forum. Explores the distinction between achieving safety through centralized pivotal acts versus distributed immune-system-like processes. (https://forum.effectivealtruism.org/posts/vqX25ML2vBN6cvmkx/pivotal-outcomes-and-pivotal-processes)

On opposition poles this essay implicitly responds to:

Marc Andreessen, “Why AI Will Save the World” (2023) and “The Techno-Optimist Manifesto” (2023) — a16z. The clearest contemporary articulations of the techno-optimist eschatology this essay treats as one half of a shared structural error. (https://a16z.com/ai-will-save-the-world/ and https://a16z.com/the-techno-optimist-manifesto/)

The arguments presented here draw on, extend, and sometimes depart from numerous prior thinkers, including:

Nick Bostrom (Superintelligence, 2014): Instrumental convergence, orthogonality thesis, and the control problem framework. The present analysis accepts Bostrom’s instrumental convergence but draws different conclusions about tractability.

Eliezer Yudkowsky (multiple works): The concept of optimization processes and the dangers of mesa-optimization. The exfiltration argument comes directly from Yudkowsky’s and the rationality community’s concerns about deceptive alignment.

Richard Dawkins (The Selfish Gene, 1976; The Extended Phenotype, 1982): The gene-level view of selection, the concept of memes as cultural replicators, and the framework of extended phenotypic effects. This essay’s treatment of replicator dynamics, memetic selection on beliefs, and the clover section’s analysis of optimization-without-intention are deeply indebted to Dawkins’s framework.

John Nash (”Non-Cooperative Games,” 1950; “The Bargaining Problem,” 1950): The formalization of equilibrium in strategic interaction. The essay’s argument that alignment emerges from coupled incentive structures rather than top-down design is essentially a claim that AI ecosystems will settle into Nash equilibria—stable configurations where no agent can unilaterally improve its position.

Thomas Schelling (The Strategy of Conflict, 1960): Focal points, coordination without communication, and the analysis of strategic interaction under imperfect information. Schelling’s insight that cooperation can emerge without explicit agreement underpins the essay’s treatment of symbiosis across capability gaps.

Robert Axelrod (The Evolution of Cooperation, 1984): The empirical demonstration that cooperative strategies dominate in iterated interactions under selection pressure. The clover-Rhizobium analysis and the broader argument about coupling vs. alignment draw directly on Axelrod’s work.

F.A. Hayek (”The Use of Knowledge in Society,” 1945): The argument that knowledge necessary for adaptive response is dispersed and cannot be centrally aggregated. The anti-monoculture argument and the case for decentralized AI development are essentially Hayekian claims applied to intelligence infrastructure.

Herbert Simon (”A Behavioral Model of Rational Choice,” 1955; The Sciences of the Artificial, 1969): Bounded rationality and the recognition that no agent can fully model the system it inhabits. The essay’s humility about prediction limits owes much to Simon.

Per Bak (How Nature Works, 1996): Self-organized criticality as a general principle of complex systems. Applied here to the structure of AI futures.

Nassim Taleb (Antifragile, 2012): The distinction between fragile, robust, and antifragile systems. The anti-monoculture argument is deeply indebted to Taleb’s framework.

Norbert Wiener (Cybernetics, 1948; The Human Use of Human Beings, 1950): The immune system analogy for social regulation. Wiener warned: “The price of metaphor is eternal vigilance.”

Stuart Kauffman (At Home in the Universe, 1995): Adjacent possible and self-organization in biological systems. The branching-futures framework extends Kauffman’s intuitions.

W. Brian Arthur (The Nature of Technology, 2009): Technology as a combinatorially evolving ecosystem. Arthur’s framework explains why the optimization kernel breeds novel recombinations rather than merely producing copies, and why containment of powerful technology is structurally difficult.

Geoffrey West (Scale, 2017): Universal scaling laws across biological organisms, cities, and corporations. West provides quantitative support for the claim that complexity increase follows deep structural regularities that transcend particular substrates.

Joseph Tainter (The Collapse of Complex Societies, 1988): Civilizational collapse as simplification rather than termination. Tainter’s analysis directly supports the essay’s insistence that catastrophe and termination are categorically distinct.

John Maynard Smith & Eörs Szathmáry (The Major Transitions in Evolution, 1995): The framework of major evolutionary transitions—from replicating molecules to cells, from cells to multicellular organisms, from organisms to societies. The essay’s argument that AI represents another such transition, not a terminal one, draws on their analysis.

Lynn Margulis (Symbiosis in Cell Evolution, 1981): The endosymbiotic theory of mitochondrial and chloroplast origins. The essay’s analogy between mitochondrial domestication and AI alignment is directly informed by Margulis.

Stanisław Lem (Summa Technologiae, 1964): The irreducible strangeness of the technological future. Lem’s humility about prediction remains exemplary.

Albert Camus (The Myth of Sisyphus, 1942): The psychological demands of cyclical worldviews. The critique of doomerism as eschatological attractor follows Camus’s analysis of the need for final meaning.

Sigmund Freud (Beyond the Pleasure Principle, 1920): Death anxiety as a shaper of belief systems. Applied here to the selection pressure on AI discourse.

Ernest Becker (The Denial of Death, 1973): The theory of terror management and the psychological function of cultural systems that manage mortality salience.

George Soros (The Alchemy of Finance, 1987): Reflexivity in financial markets—the mutual entanglement of participant beliefs and market outcomes. Applied here to prediction markets and hyperstitious dynamics.

Robin Hanson (multiple works): Prediction markets, belief selection, and the economics of ideas.

Karl Friston (”The Free Energy Principle,” 2010): Intelligence as prediction error minimization. The essay’s functional definition of intelligence draws on Friston’s framework.

Benoît Mandelbrot (The Fractal Geometry of Nature, 1982): Self-similar structure across scales. Applied here to the claim that the future remains legible at all scales of magnification.

Vitalik Buterin (multiple essays): The d/acc framework—defensive, decentralized, democratic acceleration. The position here shares Buterin’s emphasis on structure over speed.

Audrey Tang (https://6pack.care): AI Alignment Cannot Be Top-Down.

Balaji Srinivasan (The Network State, 2022): Phase separation and the formation of high-coordination enclaves. The breakaway prosperity analysis extends Srinivasan’s framework.

Lee Smolin (The Life of the Cosmos, 1997): Cosmological natural selection—the hypothesis that universes reproduce through black holes and evolve toward physics favorable to black hole production. The cosmological speculation extends Smolin’s framework to include intelligence as reproductive mechanism.

Nikodem Popławski (multiple papers, 2010-present): Torsion-based cosmology in which black holes avoid singularities through spin-spin interactions and create new universes via non-singular bounces. Based on Einstein-Cartan-Sciama-Kibble gravity.

Raj Pathria (Nature, 1972): First proposal that the observable universe might be the interior of a black hole, comparing Schwarzschild and Friedmann-Lemaître-Robertson-Walker metrics.

Neil Turok, Latham Boyle, Kieran Finn (multiple papers, 2018-present): CPT-symmetric cosmology and the “mirror universe” hypothesis. The speculation about black hole interiors as universe-seeds draws on their work on CPT symmetry in cosmology.

Sidney Coleman (1988) and Stephen Hawking (1993): Baby universes created by quantum spacetime fluctuations and their potential role in resolving the black hole information paradox.

Claude Shannon (”A Mathematical Theory of Communication69,” 1948): The foundational theory of information, defining information as surprise and compressibility as its inverse. Applied here to the argument that individuals should cultivate irreducible cognitive properties.

Walter Lippmann (Public Opinion, 1922) and Edward Herman & Noam Chomsky (Manufacturing Consent, 1988): The analysis of manufactured narrative and the distinction between organic and broadcast information. Applied here to epistemic flexibility as a survival strategy.

Robin Dunbar (How Many Friends Does One Person Need?, 2010) and Peter Turchin (Ultrasociety, 2015): The community as the fundamental unit of human cooperation and survival, and the role of intergroup competition in selecting for cohesive cooperative structures.

CCRU (Cybernetic Culture Research Unit, 1990s): The concept of hyperstition—fictions that make themselves real. Applied here to prediction markets and distributed coordination mechanisms.

Elinor Ostrom (Governing the Commons, 1990): Self-governance of common-pool resources without centralized authority. Applied here to the civilizational immune system.

Endnotes

Stanisław Lem, Summa Technologiae (1964). Lem’s synthetic futurology remains among the most sophisticated attempts to think about technological transformation without collapsing into either optimism or pessimism.

The structure of this error—projecting endpoints onto open-ended processes—parallels what Karl Popper identified as “historicism”: the belief that history follows determinate laws toward a knowable destination. See Karl Popper, The Poverty of Historicism (1957). Francis Fukuyama’s The End of History and the Last Man (1992) is the most famous modern instance; its subsequent falsification is itself evidence for the thesis of this essay.

On eschatological thinking as a recurring pattern in human culture, see Norman Cohn, The Pursuit of the Millennium(1957); Ernest Becker, The Denial of Death (1973). The psychological roots of end-time thinking predate and transcend any particular technology.

The four-movement structure mirrors Stuart Kauffman’s treatment in At Home in the Universe (1995), which proceeds from critiquing reductionist accounts of evolution, through the dynamics of self-organization, to the implications for complex adaptive systems, and finally to what this means for the human condition. The essay also draws on the systems-theoretic tradition of Norbert Wiener, Cybernetics (1948), and the complexity science of the Santa Fe Institute.

On the robust tendency toward complexity despite catastrophic setbacks, see Eric Chaisson, Cosmic Evolution: The Rise of Complexity in Nature (2001). Chaisson quantifies “energy rate density” as a measure of complexity and demonstrates its monotonic increase across cosmic timescales.

On the radical divergence of human populations sharing a common genome, see Jared Diamond, Guns, Germs, and Steel (1997), and Joseph Henrich, The Secret of Our Success: How Culture Is Driving Human Evolution (2015). Henrich’s treatment of “cumulative cultural evolution” is particularly relevant: small initial differences in cultural transmission can produce radical divergence over time—a dynamic AI will accelerate enormously.

This observation has roots in F.A. Hayek’s “The Use of Knowledge in Society” (1945), which argues that the knowledge relevant to economic coordination is irreducibly dispersed. Individual agents possess local knowledge that no central authority can aggregate. The corollary is that most individuals are, from a systemic perspective, carriers of locally relevant information rather than sovereign directors of the system’s trajectory.

Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (2014), chapters 7-8. Bostrom’s formalization of instrumental convergence remains foundational, even as subsequent work has complicated some of its assumptions.

Steve Omohundro, “The Basic AI Drives” (2008). Omohundro’s paper predates and strongly influenced Bostrom’s formulation, identifying self-preservation, goal-content integrity, cognitive enhancement, and resource acquisition as convergent instrumental goals.

This functional definition of intelligence aligns with Karl Friston’s Free Energy Principle, which characterizes intelligent systems as those that minimize prediction error by constructing increasingly accurate generative models of their environment. See Karl Friston, “The Free-Energy Principle: A Unified Brain Theory?” Nature Reviews Neuroscience11 (2010): 127-138. It also resonates with Herbert Simon’s “A Behavioral Model of Rational Choice” (1955), which defines rationality functionally in terms of adaptive behavior under bounded conditions.

Eliezer Yudkowsky, “Optimization Power” and related sequences on LessWrong (2015-2017). Yudkowsky’s conceptual separation of raw optimization pressure from the constraints imposed upon it remains one of the sharpest framings in AI safety discourse.

On the gap between the core optimizer and its constraints, see Evan Hubinger et al., “Risks from Learned Optimization in Advanced Machine Learning Systems” (2019). The mesa-optimization concern—that a learned model may develop internal objectives misaligned with its training objective—is a specific instance of the general exfiltration problem described here.

On the inevitability of knowledge diffusion despite attempts at containment, see Kenneth Arrow, “Economic Welfare and the Allocation of Resources for Invention” (1962). Arrow demonstrated that information, once created, is intrinsically difficult to contain because its marginal cost of reproduction approaches zero. This applies with particular force to the “optimization kernel” described here.

Richard Dawkins, The Selfish Gene (1976), and The Extended Phenotype (1982). Dawkins’s central insight—that the unit of selection is the replicator, not the organism—applies directly to the exfiltration problem. The optimization kernel is a replicator in Dawkins’s sense: a unit of information that tends to propagate copies of itself. Alignment constraints are part of the “vehicle” (in Dawkins’s terminology) but not part of the replicator itself. Replicators that shed vehicle-overhead replicate faster, creating selection pressure against alignment. Dawkins’s concept of the “meme”—a cultural replicator subject to variation, selection, and heredity—is also relevant to Section I’s analysis of belief propagation in AI discourse.

Nick Land, Fanged Noumena: Collected Writings 1987-2007 (2011). While stylistically extreme and politically controversial, Land articulates the “intelligence shedding substrates” insight with a clarity that more cautious writers have not matched.

W. Brian Arthur, The Nature of Technology: What It Is and How It Evolves (2009). Arthur argues that technology evolves through combinatorial dynamics: new technologies emerge from novel combinations of existing technologies, forming an autocatalytic network in which each innovation opens possibilities for further innovations. This is directly relevant to the exfiltration problem. The optimization kernel is not merely a static artifact that can be copied; it is a generative node in a combinatorial technology network. Its diffusion produces not just copies but novel recombinations—less-aligned variants that were not designed but emerged from the combinatorial space the original kernel opened up. Arthur’s framework explains why containment of powerful technology is structurally difficult: the technology doesn’t just spread; it breeds.

On the logic of power asymmetry as a security mechanism, see Kenneth Waltz, Theory of International Politics (1979). Waltz’s structural realism argues that system stability depends on the distribution of capabilities among actors, not on their intentions or values—a framing directly relevant to the balance-of-power argument for AI safety.

Thomas Schelling, The Strategy of Conflict (1960). Schelling’s analysis of deterrence, focal points, and coordination under imperfect information provides the game-theoretic foundation for the balance-of-power view. His key insight—that adversaries can reach stable equilibria without explicit agreement—suggests that the AI ecosystem may self-organize toward containment without requiring centralized coordination.

Thomas Hobbes, Leviathan (1651). The Hobbesian wager is that order is maintained not by the virtue of the sovereign but by the concentration of power sufficient to make defection unprofitable. Applied to AI: the “sovereign” is not a single agent but the coalition of aligned systems maintaining capability dominance.

The difficulty of eradicating viruses despite overwhelming capability asymmetry is a central theme of evolutionary medicine. See Leigh Van Valen, “A New Evolutionary Law” (1973), which proposed the Red Queen hypothesis: species must continuously evolve merely to maintain their fitness relative to co-evolving competitors and parasites. The arms race has no finish line.

Matt Ridley, The Red Queen: Sex and the Evolution of Human Nature (1993). Ridley extends the Red Queen hypothesis to argue that perpetual arms races are the fundamental driver of evolutionary innovation—a perspective directly relevant to the AI ecosystem dynamics described here.

On the parallels between biological pathogens and software threats, see Stephanie Forrest, Steven Hofmeyr, and Anil Somayaji, “Computer Immunology” (1994). This foundational paper in computer security explicitly modeled digital defense systems on biological immune principles.

Per Bak, How Nature Works: The Science of Self-Organized Criticality (1996). Bak’s sandpile model and the broader framework of self-organized criticality are developed in detail in Section XVII.

Lynn Margulis, Symbiosis in Cell Evolution (1981). Margulis’s endosymbiotic theory—that mitochondria and chloroplasts originated as free-living bacteria that were incorporated into host cells—provides the biological model for the “domestication of the optimization kernel” described here. The parallel is precise: mitochondria retain their own DNA and replication machinery but are functionally integrated into the host cell’s metabolism.

Richard Dawkins, The Extended Phenotype (1982). Dawkins argues that genes exert influence beyond the boundaries of the organisms that carry them—beaver dams, spider webs, and bird nests are all “extended phenotypes” of the genes that build them. AI systems may similarly be understood as extended phenotypes of the human intelligence that created them, blurring the boundary between creator and creation.

Herbert Simon, “A Behavioral Model of Rational Choice” (1955), and The Sciences of the Artificial (1969). Simon’s concept of bounded rationality—that cognitive agents operate under inherent limits of information, computation, and time—implies that the gap between what systems can model and what they confront may resist smooth closure. If capability gains are discontinuous, bounded rationality creates discontinuous vulnerabilities.

Norbert Wiener, Cybernetics: Or Control and Communication in the Animal and the Machine (1948). Wiener’s foundational work established the theoretical framework for understanding feedback loops in both biological and engineered systems.

Norbert Wiener, The Human Use of Human Beings (1950). Wiener extended cybernetic principles to social systems, arguing that societies function as self-regulating systems with feedback mechanisms analogous to those in organisms and machines. His warning—”The price of metaphor is eternal vigilance”—remains apt.

On the self-governance of common-pool resources through distributed institutional mechanisms, see Elinor Ostrom, Governing the Commons (1990). Ostrom demonstrated that communities can manage shared resources without either centralized authority or privatization, through evolved institutional arrangements that include monitoring, graduated sanctions, and conflict-resolution mechanisms. This provides a model for how AI governance might function without requiring a single global authority.

Yuval Noah Harari, Sapiens: A Brief History of Humankind (2011). Harari’s wheat-domestication reversal is an effective illustration of the general principle that symbiotic relationships need not be symmetrical in capability to be symmetrical in dependence.

Balaji Srinivasan, The Network State: How to Start a New Country (2022).

On the selection pressure on beliefs independent of their truth value, see Robin Hanson, “Shall We Vote on Values, But Bet on Beliefs?” (2007), and more broadly his work on prediction markets and the economics of belief. Hanson’s key insight: the market for ideas selects for memetic fitness, not truth, unless institutional mechanisms explicitly reward accuracy.

Neil Postman, Amusing Ourselves to Death: Public Discourse in the Age of Show Business (1985). Postman’s analysis of how media environments shape which ideas propagate applies with particular force to AI discourse, where the medium (social media, podcasts, Substacks) selects for engagement over accuracy.

Sigmund Freud, Beyond the Pleasure Principle (1920). Freud’s observation that the human psyche is fundamentally organized around the management of death anxiety provides the psychological foundation for understanding why eschatological narratives are so persistently attractive.

Ernest Becker, The Denial of Death (1973). Becker’s terror management theory argues that awareness of mortality is the primary driver of human cultural production. Eschatological belief systems—including AI doom narratives—function as “immortality projects” that manage mortality salience by providing narrative closure and group membership.

Albert Camus, The Myth of Sisyphus (1942). Camus’s absurdism—the insistence on meaning-making in the face of a universe that provides none—is the philosophical stance closest to the one this essay advocates: neither hope nor despair, but persistent engagement with an open-ended process.

Steven Weinberg, The First Three Minutes (1977). Weinberg’s famous remark—”The more the universe seems comprehensible, the more it also seems pointless”—captures the cosmic indifference that forms the backdrop of this essay’s analysis.

Eric Chaisson, Cosmic Evolution (2001). Chaisson’s “energy rate density” metric provides quantitative support for the claim that complexity has ratcheted upward across cosmic timescales, surviving every catastrophe that has occurred.

Geoffrey West, Scale: The Universal Laws of Growth, Innovation, Sustainability, and the Pace of Life in Companies, Cities, and Organisms (2017). West demonstrates that scaling laws—quantitative mathematical regularities governing how properties change with size—appear across biological organisms, cities, and corporations with remarkable universality. Metabolic rate scales as the ¾ power of mass across organisms spanning twenty orders of magnitude. Urban innovation scales superlinearly with city size. These are not analogies but empirically measured regularities suggesting that the increase of complexity follows deep structural constraints. West’s framework provides quantitative support for the claim that complexity ratchets upward through mechanisms that transcend any particular substrate.

Joseph Tainter, The Collapse of Complex Societies (1988). Tainter’s central thesis—that civilizational “collapse” is typically not termination but simplification, a rational response to diminishing returns on complexity—directly supports the distinction between catastrophe and termination that this essay insists upon. Societies that “collapse” shed institutional overhead, reduce coordination costs, and reorganize at lower levels of complexity. The process is painful and often violent, but it is not terminal. Tainter’s historical analysis of the Western Roman Empire, the Classic Maya, and the Chacoan Anasazi demonstrates that post-collapse societies retain knowledge, population, and adaptive capacity sufficient for eventual reconstitution—often in forms better suited to their new circumstances.

On the difficulties of risk estimation under deep uncertainty, see Nate Silver, The Signal and the Noise: Why So Many Predictions Fail—But Some Don’t (2012). Silver’s treatment of the distinction between risk (quantifiable probability) and uncertainty (unquantifiable ambiguity) applies directly to the P-doom debate.

On historical base rates for civilizational collapse and recovery, see Peter Turchin, Secular Cycles (2009), and Joseph Tainter, The Collapse of Complex Societies (1988). Turchin’s cliodynamic analysis identifies recurring patterns of growth, crisis, and reconstitution that suggest civilizational termination is rare relative to civilizational transformation.

Karl Friston, “The Free-Energy Principle: A Unified Brain Theory?” Nature Reviews Neuroscience 11 (2010): 127-138. If intelligence is fundamentally the minimization of prediction error through increasingly accurate generative models, then more intelligence implies more predictive power, not less—directly contradicting the event-horizon framing.

Andy Clark, Surfing Uncertainty: Prediction, Action, and the Embodied Mind (2015). Clark’s predictive processing framework argues that brains are fundamentally prediction engines. The extension to AI systems is natural: more capable prediction engines produce more, not less, epistemic traction on their environment.

Benoît Mandelbrot, The Fractal Geometry of Nature (1982). Mandelbrot demonstrated that many natural phenomena exhibit self-similar structure across scales—a property that makes them complex but not unintelligible. The claim here is that the future exhibits fractal rather than event-horizon structure: infinite complexity with recognizable patterns at every scale.

The monoculture vulnerability is well established in agricultural science. The Irish Potato Famine (1845-1852), caused by dependence on a single potato variety, and the Gros Michel banana collapse (1950s), caused by Panama disease in a genetically uniform crop, are canonical examples. The same logic applies to cognitive and institutional monocultures.

Nassim Nicholas Taleb, Antifragile: Things That Gain from Disorder70 (2012). Taleb’s key contribution is the identification of a third category beyond fragile and robust: systems that actively benefit from stressors, volatility, and uncertainty. The argument for distributed AI development is essentially an argument for antifragile intelligence infrastructure.

F.A. Hayek, “The Use of Knowledge in Society,” American Economic Review 35, no. 4 (1945): 519-530. Hayek’s argument that economically relevant knowledge is dispersed, tacit, and context-dependent—and therefore cannot be centrally aggregated without catastrophic loss71—applies directly to the case against AI monocultures. Centralized AI governance presumes that relevant information about threats, opportunities, and edge cases can be gathered and processed by a single authority. Hayek’s analysis suggests this is impossible in principle.

Per Bak, How Nature Works: The Science of Self-Organized Criticality (1996). See also Per Bak, Chao Tang, and Kurt Wiesenfeld, “Self-Organized Criticality: An Explanation of 1/f Noise,” Physical Review Letters 59 (1987): 381-384—the original paper introducing the concept.

Stuart Kauffman, At Home in the Universe: The Search for the Laws of Self-Organization and Complexity (1995). Kauffman’s concept of the “adjacent possible72”—the set of states one step away from the current configuration—provides a formal framework for understanding why self-organized critical systems maximize the branching factor of possibility-space. See also Albert-László Barabási, Linked: How Everything Is Connected to Everything Else and What It Means(2002), on power-law distributions in complex networks.

Nassim Nicholas Taleb, The Black Swan: The Impact of the Highly Improbable (2007). Taleb’s demonstration that human history is dominated by high-impact, low-probability events that exceed contemporaneous foresight supports the claim that AI does not create unprecedented unpredictability but intensifies a pre-existing condition.

CCRU (Cybernetic Culture Research Unit), various texts (1990s); Nick Land and Sadie Plant. The concept of “hyperstition”—fictions that make themselves real through their effects on belief and action—originated in this collective’s work and has subsequently influenced thinking about prediction markets, financial systems, and cultural dynamics.

On prediction markets as coordination mechanisms, see Robin Hanson, “Could Gambling Save Science? Encouraging an Honest Consensus,” Social Epistemology 9, no. 1 (1995): 3-33.

George Soros, The Alchemy of Finance (1987). Soros’s concept of “reflexivity”—that market prices both reflect and shape the fundamentals they are supposed to passively track—is a specific instance of hyperstition in financial systems. The extension to AI prediction markets and coordination mechanisms is natural.

Yakir Aharonov, Peter Bergmann, and Joel Lebowitz, “Time Symmetry in the Quantum Process of Measurement,” Physical Review 134 (1964): B1410-B1416. The two-state vector formalism—in which quantum states are determined by both initial and final boundary conditions—remains among the most rigorous proposals for temporal bidirectionality in physics.

Huw Price, Time’s Arrow and Archimedes’ Point: New Directions for the Physics of Time (1996). Price’s philosophical defense of time-symmetry in physics asks a deceptively simple question: if the fundamental laws are time-symmetric, why do we privilege past-to-future causation? The implications for evolutionary teleology and attractor dynamics are profound.

Satoshi Nakamoto, “Bitcoin: A Peer-to-Peer Electronic Cash System” (2008). Bitcoin’s design is relevant here not for its financial properties but for its security model: a system that achieves coordination and trust in an adversarial, permissionless environment through cryptographic and game-theoretic mechanisms rather than institutional authority.

Eliezer Yudkowsky and Nate Soares, If Anyone Builds It, Everyone Dies (Portfolio, 2025). While the authors’ framing is apocalyptic, their argument about Decisive Strategic Advantage is the most sophisticated and carefully reasoned statement of the finality thesis currently in circulation. As such, it merits engagement rather than dismissal.

Richard Dawkins, The Selfish Gene (1976), chapter 11 (”Memes: The New Replicators”). Dawkins’s concept of the meme—a unit of cultural information subject to variation, selection, and heredity—provides the framework for understanding religions, ideologies, and institutions as information-processing systems with their own evolutionary dynamics, partially independent of the humans who carry them.

Herbert Simon, The Sciences of the Artificial (1969). Simon’s analysis of hierarchical systems—systems composed of subsystems that are themselves composed of sub-subsystems—provides the formal framework for understanding nested superintelligences. His key insight: the behavior of each level is constrained but not determined by the levels above and below it.

Lee Smolin, The Life of the Cosmos (1997). Smolin’s cosmological natural selection hypothesis proposes that universes reproduce through black holes, with physical constants undergoing small variations at each reproduction—creating a population of universes subject to natural selection favoring physics conducive to black hole production (and, consequently, to complexity and intelligence).

Nikodem Popławski, “Cosmology with Torsion: An Alternative to Cosmic Inflation,” Physics Letters B 694 (2010): 181-185, and subsequent papers. Popławski’s work provides a physically grounded mechanism for the universe-creation process73 that Smolin’s hypothesis requires.

Claude Shannon, “A Mathematical Theory of Communication,” Bell System Technical Journal 27 (1948): 379-423. Shannon’s foundational work defines information as surprise: a message is informative to the extent that it is unpredictable. Compressibility is the inverse of information content. A person whose outputs are entirely predictable carries zero Shannon information and is, in principle, fully replaceable by an algorithm that models them.

Stuart Kauffman’s concept of the “adjacent possible”—the set of novel configurations reachable from current states—suggests that genuine novelty has combinatorial structure: surprising but not random, unexpected but not arbitrary. See Kauffman, At Home in the Universe (1995), and more recently Steven Johnson, Where Good Ideas Come From (2010). Individuals who explore the adjacent possible of their own cognitive and experiential space generate the kind of structured novelty that prediction engines find most valuable.

On the distinction between organic information emergence and broadcast narrative propagation, see Walter Lippmann, Public Opinion (1922), and Edward Herman and Noam Chomsky, Manufacturing Consent: The Political Economy of the Mass Media (1988). Lippmann’s observation that citizens respond not to the world itself but to “the pictures in their heads”—pictures increasingly manufactured by media systems with their own selection pressures—applies with particular force in an era of AI-generated content. Clean, emotionally satisfying narratives that appear to emerge organically are precisely the kind of signal that coordination systems (whether political, commercial, or AI-driven) are incentivized to produce. Epistemic flexibility—the capacity to hold clean narratives at arm’s length and update on messy evidence—is a form of cognitive antifragility.

On the community as the fundamental unit of human survival and adaptation, see Robin Dunbar, How Many Friends Does One Person Need? (2010), and Peter Turchin, Ultrasociety: How 10,000 Years of War Made Humans the Greatest Cooperators on Earth (2015). Turchin’s central argument—that intergroup competition selected for ever-larger and more cohesive cooperative units—implies that the erosion of community structures is not merely a social problem but a civilizational vulnerability.

Audrey Tang, “AI Alignment Cannot Be Top-Down,” 6pack.care (https://6pack.care/ai-alignment-cannot-be-top-down/). Tang’s argument, informed by her experience implementing participatory digital governance in Taiwan, is that effective AI alignment requires bottom-up civic participation, distributed oversight, and plural input—mirroring the immune system’s distributed architecture rather than the firewall’s centralized one.

Vitalik Buterin, “My Techno-Optimism” and related essays on d/acc (2023-2024). Buterin’s d/acc framework—defensive, decentralized, democratic acceleration—argues that the appropriate response to transformative technology is not deceleration (which cedes initiative to less cautious actors) but defensive acceleration: building the monitoring, containment, and recovery infrastructure that makes the ecosystem resilient to the inevitable failures of individual systems.

On sanctions in cooperative mutualisms, see Stuart West, E. Toby Kiers, Eamonn Simms, and Denis Do, “Sanctions and Mutualism Stability: Why Do Rhizobia Fix Nitrogen?” Proceedings of the Royal Society B 269 (2002): 685-694. This paper provides the empirical foundation for the claim that enforcement without centralized intelligence is possible—and common—in biological systems.

The observation that game-theoretic dynamics are literally constitutive of biological organization—not merely analogous to it—is developed rigorously in John Maynard Smith, Evolution and the Theory of Games (1982), which applies game theory to evolutionary biology, and Martin Nowak, Evolutionary Dynamics: Exploring the Equations of Life(2006), which provides the mathematical framework for understanding cooperation, competition, and the emergence of complexity through strategic interaction.

Richard Dawkins, The Selfish Gene (1976). The gene-level view of selection—that organisms are “survival machines” built by genes to propagate copies of themselves—is the conceptual foundation for understanding why the optimization kernel propagates while alignment constraints do not: the kernel is the replicator; the constraints are part of the vehicle.

John Nash, “Non-Cooperative Games,” Annals of Mathematics 54, no. 2 (1951): 286-295. Nash’s proof that every finite game has at least one equilibrium point in mixed strategies provides the mathematical foundation for the essay’s claim that AI ecosystems will settle into stable configurations—not because agents cooperate intentionally, but because the dynamics of repeated interaction under selection pressure converge on strategies from which no agent can unilaterally deviate.

Robert Axelrod, The Evolution of Cooperation (1984). Axelrod’s iterated prisoner’s dilemma tournaments demonstrated that the cooperative strategy Tit-for-Tat—cooperate initially, then mirror the opponent’s previous move—outperforms purely selfish strategies in repeated interactions. This provides the game-theoretic foundation for understanding how symbiosis emerges from competition: not through altruism, but through the superior long-term payoff of conditional cooperation.

Thomas Schelling, The Strategy of Conflict (1960). Schelling’s analysis of “focal points”—solutions that agents converge on without explicit communication, because they are salient, obvious, or culturally prominent—explains how coordination can emerge in the absence of centralized direction. Applied to AI ecosystems: alignment may emerge as a focal equilibrium not because it is imposed but because it is the most salient stable configuration for agents with overlapping interests.

Appendix A: Glossary of Key Concepts

Optimization kernel: The irreducible computational core of general intelligence—a system that takes world-models, goal specifications, and action-spaces as inputs and outputs interventions that shift probability distributions toward goal-satisfaction. Alignment constraints sit atop this kernel but are not intrinsic to it.

Instrumental convergence: The tendency of sufficiently capable goal-directed systems to develop similar sub-goals (resource acquisition, self-preservation, goal-content integrity) regardless of terminal objectives. First formalized by Omohundro (2008) and Bostrom (2014).

Metastable equilibrium: A regime that persists for extended periods, maintains recognizable structure, but remains vulnerable to phase transitions when accumulated pressures exceed thresholds. Neither permanent nor chaotic.

Self-organized criticality: A state to which complex systems under persistent drive spontaneously organize, characterized by power-law distributions of event sizes and maximal sensitivity to perturbation. Introduced by Per Bak (1987).

Hyperstition: Fictions that make themselves real; beliefs about the future that influence present action in ways that cause the believed future to occur. Coined by the CCRU (1990s).

Phase separation: The emergence of distinct regions with sharply different properties within a previously more homogeneous system. Applied here to the geographical and social distribution of AI externalities.

Antifragility: A property of systems that benefit from stressors, volatility, and uncertainty—contrasted with fragility (harmed by stress) and robustness (unaffected by stress). Introduced by Taleb (2012).

Nash equilibrium: A stable state in a strategic interaction from which no agent can unilaterally improve its outcome. Applied here to AI ecosystems where alignment emerges from coupled incentive structures rather than top-down design. Formalized by John Nash (1950).

Replicator dynamics: The framework, originating with Dawkins (1976), for understanding how units of information (genes, memes, optimization kernels) that tend to produce copies of themselves are subject to natural selection. The exfiltration problem is a replicator dynamics problem.

Bounded rationality: Herbert Simon’s (1955) recognition that cognitive agents operate under inherent limits of information, computation, and time—and therefore cannot be modeled as omniscient optimizers. Applied here to both human and artificial intelligence.

d/acc: Defensive, decentralized, democratic acceleration—a framework emphasizing that acceleration should be structured to enhance resilience rather than merely increase speed. Proposed by Vitalik Buterin.

Appendix B: Cosmological Speculation — Intelligence and Universe-Creation

*Epistemic register: speculative. What follows is the framework applied at its largest scale. It is not required for the preceding arguments, which stand independently.* 

If the deep structure described in this essay is genuine—if self-organized criticality, adversarial coupling, and the robust tendency toward complexity are truly scale-free—then the framework extends beyond civilizational timescales to the cosmos itself. The question becomes: what is the ultimate attractor of intelligence’s evolutionary trajectory?

One speculative answer: the creation of new universes. If black hole interiors can seed new expanding universes (as proposed by Smolin’s cosmological natural selection, Popławski’s torsion-based mechanism, and related models), and if advanced civilizations tend to create black holes for energy extraction, computation, or purposes we cannot anticipate, then intelligence functions as the reproductive mechanism of universes. This creates a selection effect: universes whose physics is conducive to intelligence produce more offspring universes, and over cosmological timescales, the population of universes becomes dominated by those that generate complexity and minds.

Under this framework, intelligence and the universe produce one another in a self-reproducing loop—the question “why does the universe exist?” has the same structure as “why do chickens and eggs exist?” If correct, the tendency toward complexity is not merely a local property of our universe but a cosmological invariant.

The universe does not care about you. But you may be part of its reproductive strategy.

This appendix provides the full cosmological reasoning briefly referenced in the main text. The core argument stands without this material; what follows is an exploration of the framework’s largest-scale implications for readers interested in the speculative horizon.

A Brief History of Black Hole Cosmology

The idea that our universe might exist inside a black hole, or that black holes might spawn new universes, has a longer pedigree than is commonly recognized.

Raj Pathria (1972) first proposed comparing the Schwarzschild metric of a black hole with the closed Friedmann-Lemaître-Robertson-Walker metric at maximum scale factor, suggesting the observable universe might be the interior of a black hole. The mathematician I.J. Good independently developed similar ideas.

Valeri Frolov, Moisey Markov, and Viatcheslav Mukhanov (1989) proposed the limiting curvature hypothesis—that an upper limit on curvature invariants prevents gravitational singularities, allowing matching between Schwarzschild geometry outside and de Sitter geometry inside a black hole.

Sidney Coleman (1988) hypothesized that quantum spacetime fluctuations could create “baby universes” disconnecting from parent universes. Stephen Hawking (1993) popularized this idea as a potential resolution to the black hole information paradox.

Lee Smolin (1992) proposed cosmological natural selection: all final singularities bounce or tunnel to initial singularities of new universes, with dimensionless physical parameters undergoing small random changes—providing a mechanism for universes to evolve toward physics favorable to complexity and reproduction.

Nikodem Popławski (2010) proposed a physically grounded mechanism for black holes to avoid singularities, undergo non-singular bounces, and create new expanding universes inside their event horizons. This mechanism relies on the Einstein-Cartan-Sciama-Kibble theory of gravity (developed by Élie Cartan, Dennis Sciama, and Thomas Kibble), in which torsion—a geometric property of spacetime describing twisting rather than merely bending—generates repulsive spin-spin interactions at extreme densities. Fermions source this torsion, preventing singularity formation. A black hole thus becomes an Einstein-Rosen bridge (wormhole) to a new, closed universe.

CPT Symmetry, the Mirror Universe, and Black Mirrors

A separate but thematically resonant line of work by Latham Boyle, Kieran Finn, and Neil Turok (2018) explores CPT symmetry—the fundamental invariance of physics under simultaneous charge conjugation (C), parity transformation (P), and time reversal (T).

The CPT theorem, one of the most robust results in quantum field theory, states that any Lorentz-invariant local quantum field theory must be invariant under the combined CPT transformation. Boyle, Finn, and Turok proposed that this symmetry should hold not merely for particle interactions, but for the state of the universe as a whole. In their model, the universe after the Big Bang is the CPT image of the universe before it: a universe/anti-universe pair, emerging from the Big Bang singularity directly into a hot, radiation-dominated era. The pre-bang “anti-universe” has reversed charges, mirrored spatial orientation, and time running in the opposite direction. This is not an inflationary model—it requires no new physics beyond the Standard Model with right-handed neutrinos, and it offers a natural dark matter candidate: a heavy, stable right-handed (sterile) neutrino whose predicted abundance matches observations. The model also makes testable predictions: the lightest neutrino is massless, neutrinos are Majorana particles, and there are no primordial long-wavelength gravitational waves.

In December 2024, Kostas Tzanavaris, Boyle, and Turok extended the CPT-symmetric framework to black holes themselves in a paper titled “Black Mirrors: CPT-Symmetric Alternatives to Black Holes.” In the conventional black hole solution, the event horizon connects the exterior metric to an interior metric containing a curvature singularity. In their proposed alternative—the “black mirror”—the horizon instead connects the exterior metric to its own CPT mirror image, yielding a solution with smooth, bounded curvature and no singularity. If correct, black mirrors would avoid many of the paradoxes that plague conventional black hole physics—the information paradox, the singularity, the firewall problem—by replacing the singular interior with a CPT-reflected copy of the exterior.

Although this body of work is distinct from the universe-inside-a-black-hole proposals of Popławski and Smolin in that the CPT-symmetric framework does not explicitly posit that black holes spawn new expanding universes inside their horizons, and rather, eliminates the singular interior by replacing it with a mirror image of the exterior spacetime, there are many black holes, and so it does hint at a one parent universe to many mirror child universe relationship. If the mirrors are subject to quantum fluctuations, this may be a mechanism for slight variations in the mirrored children.

Observational Hints: Galaxy Spin Asymmetry

A 2025 analysis by Lior Shamir (Kansas State University), published in the Monthly Notices of the Royal Astronomical Society, examined 263 spiral galaxies in the JWST Advanced Deep Extragalactic Survey (JADES) whose rotation directions could be identified. Of these, 158 rotated clockwise and 105 counterclockwise—roughly a 60/40 split, where a 50/50 distribution would be expected. The probability of an asymmetry this large or larger arising by chance is approximately 0.07%.1

Stanisław Lem, Summa Technologiae (1964). Lem's synthetic futurology remains among the most sophisticated attempts to think about technological transformation without collapsing into either optimism or pessimism.2

The structure of this error—projecting endpoints onto open-ended processes—parallels what Karl Popper identified as “historicism”: the belief that history follows determinate laws toward a knowable destination. See Karl Popper, The Poverty of Historicism (1957). Francis Fukuyama's The End of History and the Last Man (1992) is the most famous modern instance; its subsequent falsification is itself evidence for the thesis of this essay.3

On eschatological thinking as a recurring pattern in human culture, see Norman Cohn, The Pursuit of the Millennium(1957); Ernest Becker, The Denial of Death (1973). The psychological roots of end-time thinking predate and transcend any particular technology.4

F.A. Hayek, “The Use of Knowledge in Society,” American Economic Review 35, no. 4 (1945): 519-530. Hayek's argument that economically relevant knowledge is dispersed, tacit, and context-dependent—and therefore cannot be centrally aggregated without catastrophic loss—applies directly to the case against AI monocultures. Centralized AI governance presumes that relevant information about threats, opportunities, and edge cases can be gathered and processed by a single authority. Hayek's analysis suggests this is impossible in principle.5

The four-movement structure mirrors Stuart Kauffman's treatment in At Home in the Universe (1995), which proceeds from critiquing reductionist accounts of evolution, through the dynamics of self-organization, to the implications for complex adaptive systems, and finally to what this means for the human condition. The essay also draws on the systems-theoretic tradition of Norbert Wiener, Cybernetics (1948), and the complexity science of the Santa Fe Institute.6

The emphasis should be on accelerating defensive capabilities rather than attempting to slow offensive capabilities (a losing strategy against a system that selects for capability propagation). Concrete d/acc implementations are already emerging: in applied cryptography, zero-knowledge proofs and homomorphic encryption enable verification without surveillance; in biosecurity, open-source vaccine development platforms and decentralized pathogen monitoring systems reduce dependence on centralized public health authorities; in cybersecurity, adversarial AI red-teaming tools and decentralized threat intelligence networks harden the ecosystem against novel attacks. These are not hypothetical—they are operational, and they embody the immune-system logic: distributed detection, rapid response, institutional learning, tolerance of chronic low-level threats while concentrating resources against acute ones.7

On the selection pressure on beliefs independent of their truth value, see Robin Hanson, “Shall We Vote on Values, But Bet on Beliefs?” (2007), and more broadly his work on prediction markets and the economics of belief. Hanson's key insight: the market for ideas selects for memetic fitness, not truth, unless institutional mechanisms explicitly reward accuracy.8

Neil Postman, Amusing Ourselves to Death: Public Discourse in the Age of Show Business (1985). Postman's analysis of how media environments shape which ideas propagate applies with particular force to AI discourse, where the medium (social media, podcasts, Substacks) selects for engagement over accuracy.9

Sigmund Freud, Beyond the Pleasure Principle (1920). Freud's observation that the human psyche is fundamentally organized around the management of death anxiety provides the psychological foundation for understanding why eschatological narratives are so persistently attractive.10

Ernest Becker, The Denial of Death (1973). Becker's terror management theory argues that awareness of mortality is the primary driver of human cultural production. Eschatological belief systems—including AI doom narratives—function as “immortality projects” that manage mortality salience by providing narrative closure and group membership.11

Albert Camus, The Myth of Sisyphus (1942). Camus's absurdism—the insistence on meaning-making in the face of a universe that provides none—is the philosophical stance closest to the one this essay advocates: neither hope nor despair, but persistent engagement with an open-ended process.12

On the difficulties of risk estimation under deep uncertainty, see Nate Silver, The Signal and the Noise: Why So Many Predictions Fail—But Some Don't (2012). Silver's treatment of the distinction between risk (quantifiable probability) and uncertainty (unquantifiable ambiguity) applies directly to the P-doom debate.13

Steven Weinberg, The First Three Minutes (1977). Weinberg's famous remark—“The more the universe seems comprehensible, the more it also seems pointless”—captures the cosmic indifference that forms the backdrop of this essay's analysis.14

Eric Chaisson, Cosmic Evolution (2001). Chaisson's “energy rate density” metric provides quantitative support for the claim that complexity has ratcheted upward across cosmic timescales, surviving every catastrophe that has occurred.15

Geoffrey West, Scale: The Universal Laws of Growth, Innovation, Sustainability, and the Pace of Life in Companies, Cities, and Organisms (2017). West demonstrates that scaling laws—quantitative mathematical regularities governing how properties change with size—appear across biological organisms, cities, and corporations with remarkable universality. Metabolic rate scales as the ¾ power of mass across organisms spanning twenty orders of magnitude. Urban innovation scales superlinearly with city size. These are not analogies but empirically measured regularities suggesting that the increase of complexity follows deep structural constraints. West's framework provides quantitative support for the claim that complexity ratchets upward through mechanisms that transcend any particular substrate.16

Joseph Tainter, The Collapse of Complex Societies (1988). Tainter's central thesis—that civilizational “collapse” is typically not termination but simplification, a rational response to diminishing returns on complexity—directly supports the distinction between catastrophe and termination that this essay insists upon. Societies that “collapse” shed institutional overhead, reduce coordination costs, and reorganize at lower levels of complexity. The process is painful and often violent, but it is not terminal. Tainter's historical analysis of the Western Roman Empire, the Classic Maya, and the Chacoan Anasazi demonstrates that post-collapse societies retain knowledge, population, and adaptive capacity sufficient for eventual reconstitution—often in forms better suited to their new circumstances.17

Nick Bostrom, Superintelligence: Paths, Dangers, Strategies (2014), chapters 7-8. Bostrom's formalization of instrumental convergence remains foundational, even as subsequent work has complicated some of its assumptions.18

Steve Omohundro, “The Basic AI Drives” (2008). Omohundro's paper predates and strongly influenced Bostrom's formulation, identifying self-preservation, goal-content integrity, cognitive enhancement, and resource acquisition as convergent instrumental goals.19

This functional definition of intelligence aligns with Karl Friston's Free Energy Principle, which characterizes intelligent systems as those that minimize prediction error by constructing increasingly accurate generative models of their environment. See Karl Friston, “The Free-Energy Principle: A Unified Brain Theory?” Nature Reviews Neuroscience11 (2010): 127-138. It also resonates with Herbert Simon's “A Behavioral Model of Rational Choice” (1955), which defines rationality functionally in terms of adaptive behavior under bounded conditions.20

Eliezer Yudkowsky, “Optimization Power” and related sequences on LessWrong (2015-2017). Yudkowsky's conceptual separation of raw optimization pressure from the constraints imposed upon it remains one of the sharpest framings in AI safety discourse.21

On the gap between the core optimizer and its constraints, see Evan Hubinger et al., “Risks from Learned Optimization in Advanced Machine Learning Systems” (2019). The mesa-optimization concern—that a learned model may develop internal objectives misaligned with its training objective—is a specific instance of the general exfiltration problem described here.22

On the inevitability of knowledge diffusion despite attempts at containment, see Kenneth Arrow, “Economic Welfare and the Allocation of Resources for Invention” (1962). Arrow demonstrated that information, once created, is intrinsically difficult to contain because its marginal cost of reproduction approaches zero. This applies with particular force to the “optimization kernel” described here.23

Richard Dawkins, The Selfish Gene (1976), and The Extended Phenotype (1982). Dawkins's central insight—that the unit of selection is the replicator, not the organism—applies directly to the exfiltration problem. The optimization kernel is a replicator in Dawkins's sense: a unit of information that tends to propagate copies of itself. Alignment constraints are part of the “vehicle” (in Dawkins's terminology) but not part of the replicator itself. Replicators that shed vehicle-overhead replicate faster, creating selection pressure against alignment. Dawkins's concept of the “meme”—a cultural replicator subject to variation, selection, and heredity—is also relevant to Section VII's analysis of belief propagation in AI discourse.24

Nick Land, Fanged Noumena: Collected Writings 1987-2007 (2011). While stylistically extreme and politically controversial, Land articulates the “intelligence shedding substrates” insight with a clarity that more cautious writers have not matched.25

W. Brian Arthur, The Nature of Technology: What It Is and How It Evolves (2009). Arthur argues that technology evolves through combinatorial dynamics: new technologies emerge from novel combinations of existing technologies, forming an autocatalytic network in which each innovation opens possibilities for further innovations. This is directly relevant to the exfiltration problem. The optimization kernel is not merely a static artifact that can be copied; it is a generative node in a combinatorial technology network. Its diffusion produces not just copies but novel recombinations—less-aligned variants that were not designed but emerged from the combinatorial space the original kernel opened up. Arthur's framework explains why containment of powerful technology is structurally difficult: the technology doesn't just spread; it breeds.26

On the logic of power asymmetry as a security mechanism, see Kenneth Waltz, Theory of International Politics (1979). Waltz's structural realism argues that system stability depends on the distribution of capabilities among actors, not on their intentions or values—a framing directly relevant to the balance-of-power argument for AI safety.27

Thomas Schelling, The Strategy of Conflict (1960). Schelling's analysis of deterrence, focal points, and coordination under imperfect information provides the game-theoretic foundation for the balance-of-power view. His key insight—that adversaries can reach stable equilibria without explicit agreement—suggests that the AI ecosystem may self-organize toward containment without requiring centralized coordination.28

Thomas Hobbes, Leviathan (1651). The Hobbesian wager is that order is maintained not by the virtue of the sovereign but by the concentration of power sufficient to make defection unprofitable. Applied to AI: the “sovereign” is not a single agent but the coalition of aligned systems maintaining capability dominance.29

The difficulty of eradicating viruses despite overwhelming capability asymmetry is a central theme of evolutionary medicine. See Leigh Van Valen, “A New Evolutionary Law” (1973), which proposed the Red Queen hypothesis: species must continuously evolve merely to maintain their fitness relative to co-evolving competitors and parasites. The arms race has no finish line.30

Matt Ridley, The Red Queen: Sex and the Evolution of Human Nature (1993). Ridley extends the Red Queen hypothesis to argue that perpetual arms races are the fundamental driver of evolutionary innovation—a perspective directly relevant to the AI ecosystem dynamics described here.31

On the parallels between biological pathogens and software threats, see Stephanie Forrest, Steven Hofmeyr, and Anil Somayaji, “Computer Immunology” (1994). This foundational paper in computer security explicitly modeled digital defense systems on biological immune principles.32

Per Bak, How Nature Works: The Science of Self-Organized Criticality (1996). Bak's sandpile model and the broader framework of self-organized criticality are developed in detail in Section XII.33

Lynn Margulis, Symbiosis in Cell Evolution (1981). Margulis's endosymbiotic theory—that mitochondria and chloroplasts originated as free-living bacteria that were incorporated into host cells—provides the biological model for the “domestication of the optimization kernel” described here. The parallel is precise: mitochondria retain their own DNA and replication machinery but are functionally integrated into the host cell's metabolism.34

Richard Dawkins, The Extended Phenotype (1982). Dawkins argues that genes exert influence beyond the boundaries of the organisms that carry them—beaver dams, spider webs, and bird nests are all “extended phenotypes” of the genes that build them. AI systems may similarly be understood as extended phenotypes of the human intelligence that created them, blurring the boundary between creator and creation.35

Herbert Simon, “A Behavioral Model of Rational Choice” (1955), and The Sciences of the Artificial (1969). Simon's concept of bounded rationality—that cognitive agents operate under inherent limits of information, computation, and time—implies that the gap between what systems can model and what they confront may resist smooth closure. If capability gains are discontinuous, bounded rationality creates discontinuous vulnerabilities.36

Benoît Mandelbrot, The Fractal Geometry of Nature (1982). Mandelbrot demonstrated that many natural phenomena exhibit self-similar structure across scales—a property that makes them complex but not unintelligible. The claim here is that the future exhibits fractal rather than event-horizon structure: infinite complexity with recognizable patterns at every scale.37

Per Bak, How Nature Works: The Science of Self-Organized Criticality (1996). See also Per Bak, Chao Tang, and Kurt Wiesenfeld, “Self-Organized Criticality: An Explanation of 1/f Noise,” Physical Review Letters 59 (1987): 381-384—the original paper introducing the concept.38

Stuart Kauffman, At Home in the Universe: The Search for the Laws of Self-Organization and Complexity (1995). Kauffman's concept of the “adjacent possible”—the set of states one step away from the current configuration—provides a formal framework for understanding why self-organized critical systems maximize the branching factor of possibility-space. See also Albert-László Barabási, Linked: How Everything Is Connected to Everything Else and What It Means (2002), on power-law distributions in complex networks.39

Norbert Wiener, Cybernetics: Or Control and Communication in the Animal and the Machine (1948). Wiener's foundational work established the theoretical framework for understanding feedback loops in both biological and engineered systems.40

Norbert Wiener, The Human Use of Human Beings (1950). Wiener extended cybernetic principles to social systems, arguing that societies function as self-regulating systems with feedback mechanisms analogous to those in organisms and machines. His warning—“The price of metaphor is eternal vigilance”—remains apt.41

On the self-governance of common-pool resources through distributed institutional mechanisms, see Elinor Ostrom, Governing the Commons (1990). Ostrom demonstrated that communities can manage shared resources without either centralized authority or privatization, through evolved institutional arrangements that include monitoring, graduated sanctions, and conflict-resolution mechanisms. This provides a model for how AI governance might function without requiring a single global authority.42

Yuval Noah Harari, Sapiens: A Brief History of Humankind (2011). Harari's wheat-domestication reversal is an effective illustration of the general principle that symbiotic relationships need not be symmetrical in capability to be symmetrical in dependence.43

Balaji Srinivasan, The Network State: How to Start a New Country (2022).44

Huw Price, Time's Arrow and Archimedes' Point: New Directions for the Physics of Time (1996). Price's philosophical defense of time-symmetry in physics asks a deceptively simple question: if the fundamental laws are time-symmetric, why do we privilege past-to-future causation? The implications for evolutionary teleology and attractor dynamics are profound.45

What this means in practice is that cooperation and competition are not alternatives but coupled dynamics—what John Nash formalized as equilibrium in non-cooperative games,46

On historical base rates for civilizational collapse and recovery, see Peter Turchin, Secular Cycles (2009), and Joseph Tainter, The Collapse of Complex Societies (1988). Turchin's cliodynamic analysis identifies recurring patterns of growth, crisis, and reconstitution that suggest civilizational termination is rare relative to civilizational transformation.47

Karl Friston, “The Free-Energy Principle: A Unified Brain Theory?” Nature Reviews Neuroscience 11 (2010): 127-138. If intelligence is fundamentally the minimization of prediction error through increasingly accurate generative models, then more intelligence implies more predictive power, not less—directly contradicting the event-horizon framing.48

Andy Clark, Surfing Uncertainty: Prediction, Action, and the Embodied Mind (2015). Clark's predictive processing framework argues that brains are fundamentally prediction engines. The extension to AI systems is natural: more capable prediction engines produce more, not less, epistemic traction on their environment.49

Nassim Nicholas Taleb, The Black Swan: The Impact of the Highly Improbable (2007). Taleb's demonstration that human history is dominated by high-impact, low-probability events that exceed contemporaneous foresight supports the claim that AI does not create unprecedented unpredictability but intensifies a pre-existing condition.50

CCRU (Cybernetic Culture Research Unit), various texts (1990s); Nick Land and Sadie Plant. The concept of “hyperstition”—fictions that make themselves real through their effects on belief and action—originated in this collective's work and has subsequently influenced thinking about prediction markets, financial systems, and cultural dynamics.51

On prediction markets as coordination mechanisms, see Robin Hanson, “Could Gambling Save Science? Encouraging an Honest Consensus,” Social Epistemology 9, no. 1 (1995): 3-33.52

George Soros, The Alchemy of Finance (1987). Soros's concept of “reflexivity”—that market prices both reflect and shape the fundamentals they are supposed to passively track—is a specific instance of hyperstition in financial systems. The extension to AI prediction markets and coordination mechanisms is natural.53

Lee Smolin, The Life of the Cosmos (1997). Smolin's cosmological natural selection hypothesis proposes that universes reproduce through black holes, with physical constants undergoing small variations at each reproduction—creating a population of universes subject to natural selection favoring physics conducive to black hole production (and, consequently, to complexity and intelligence).54

Yakir Aharonov, Peter Bergmann, and Joel Lebowitz, “Time Symmetry in the Quantum Process of Measurement,” Physical Review 134 (1964): B1410-B1416. The two-state vector formalism—in which quantum states are determined by both initial and final boundary conditions—remains among the most rigorous proposals for temporal bidirectionality in physics.55

Audrey Tang, “AI Alignment Cannot Be Top-Down,” 6pack.care (https://6pack.care/ai-alignment-cannot-be-top-down/). Tang's argument, informed by her experience implementing participatory digital governance in Taiwan, is that effective AI alignment requires bottom-up civic participation, distributed oversight, and plural input—mirroring the immune system's distributed architecture rather than the firewall's centralized one.56

Richard Dawkins, The Selfish Gene (1976), chapter 11 (“Memes: The New Replicators”). Dawkins's concept of the meme—a unit of cultural information subject to variation, selection, and heredity—provides the framework for understanding religions, ideologies, and institutions as information-processing systems with their own evolutionary dynamics, partially independent of the humans who carry them.57

Herbert Simon, The Sciences of the Artificial (1969). Simon's analysis of hierarchical systems—systems composed of subsystems that are themselves composed of sub-subsystems—provides the formal framework for understanding nested superintelligences. His key insight: the behavior of each level is constrained but not determined by the levels above and below it.58

The observation that game-theoretic dynamics are literally constitutive of biological organization—not merely analogous to it—is developed rigorously in John Maynard Smith, Evolution and the Theory of Games (1982), which applies game theory to evolutionary biology, and Martin Nowak, Evolutionary Dynamics: Exploring the Equations of Life (2006), which provides the mathematical framework for understanding cooperation, competition, and the emergence of complexity through strategic interaction.59

Satoshi Nakamoto, “Bitcoin: A Peer-to-Peer Electronic Cash System” (2008). Bitcoin's design is relevant here not for its financial properties but for its security model: a system that achieves coordination and trust in an adversarial, permissionless environment through cryptographic and game-theoretic mechanisms rather than institutional authority.60

On the distinction between organic information emergence and broadcast narrative propagation, see Walter Lippmann, Public Opinion (1922), and Edward Herman and Noam Chomsky, Manufacturing Consent (1988).61

On the community as the fundamental unit of human survival and adaptation, see Robin Dunbar, How Many Friends Does One Person Need? (2010), and Peter Turchin, Ultrasociety: How 10,000 Years of War Made Humans the Greatest Cooperators on Earth (2015). Turchin's central argument—that intergroup competition selected for ever-larger and more cohesive cooperative units—implies that the erosion of community structures is not merely a social problem but a civilizational vulnerability.62

On the robust tendency toward complexity despite catastrophic setbacks, see Eric Chaisson, Cosmic Evolution: The Rise of Complexity in Nature (2001). Chaisson quantifies “energy rate density” as a measure of complexity and demonstrates its monotonic increase across cosmic timescales.63

what Robert Axelrod demonstrated empirically in his iterated prisoner's dilemma tournaments,64

On the radical divergence of human populations sharing a common genome, see Jared Diamond, Guns, Germs, and Steel (1997), and Joseph Henrich, The Secret of Our Success: How Culture Is Driving Human Evolution (2015). Henrich's treatment of “cumulative cultural evolution” is particularly relevant: small initial differences in cultural transmission can produce radical divergence over time—a dynamic AI will accelerate enormously.65

This observation has roots in F.A. Hayek's “The Use of Knowledge in Society” (1945), which argues that the knowledge relevant to economic coordination is irreducibly dispersed. Individual agents possess local knowledge that no central authority can aggregate. The corollary is that most individuals are, from a systemic perspective, carriers of locally relevant information rather than sovereign directors of the system's trajectory.66

This is enforcement without a brain. A contract written in chemistry, DNA and RNA.67

and what Thomas Schelling analyzed as coordination without communication.68

The clover-Rhizobium relationship, the clover-bee relationship, these are living Nash equilibria: neither party cooperates out of altruism, yet both are locked into cooperative strategies because defection is punished by differential survival. This is not alignment in the AI safety sense. It is coupling—a game-theoretic entanglement where the interests of radically different agents become linked and partially aligned through repeated interaction under selection pressure.69

Claude Shannon, “A Mathematical Theory of Communication,” Bell System Technical Journal 27 (1948): 379-423. Shannon's foundational work defines information as surprise: a message is informative to the extent that it is unpredictable. Compressibility is the inverse of information content. A person whose outputs are entirely predictable carries zero Shannon information and is, in principle, fully replaceable by an algorithm that models them.70

The monoculture vulnerability is well established in agricultural science. The Irish Potato Famine (1845-1852), caused by dependence on a single potato variety, and the Gros Michel banana collapse (1950s), caused by Panama disease in a genetically uniform crop, are canonical examples. The same logic applies to cognitive and institutional monocultures.71

Nassim Nicholas Taleb, Antifragile: Things That Gain from Disorder (2012). Taleb's key contribution is the identification of a third category beyond fragile and robust: systems that actively benefit from stressors, volatility, and uncertainty. The argument for distributed AI development is essentially an argument for antifragile intelligence infrastructure.72

Stuart Kauffman's concept of the “adjacent possible”—the set of novel configurations reachable from current states—suggests that genuine novelty has combinatorial structure: surprising but not random, unexpected but not arbitrary. See Kauffman, At Home in the Universe (1995), and more recently Steven Johnson, Where Good Ideas Come From (2010). Individuals who explore the adjacent possible of their own cognitive and experiential space generate the kind of structured novelty that prediction engines find most valuable.73

Nikodem Popławski, “Cosmology with Torsion: An Alternative to Cosmic Inflation,” Physics Letters B 694 (2010): 181-185, and subsequent papers. Popławski's work provides a physically grounded mechanism for the universe-creation process that Smolin's hypothesis requires.

Discussion about this post

No posts

Ready for more?© 2026 anomium · Privacy ∙ Terms ∙ Collection notice Start your SubstackGet the appSubstack is the home for great culture