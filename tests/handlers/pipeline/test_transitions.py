"""Tests for pipeline stage transition handler endpoints."""

from __future__ import annotations

import json
from dataclasses import asdict
from typing import Any
from unittest.mock import MagicMock, patch

import pytest

from aragora.server.handlers.pipeline.transitions import (
    PipelineEdge,
    PipelineNode,
    PipelineTransitionsHandler,
    TransitionResult,
    _cluster_ideas,
    _get_provenance_chain,
    _goals_to_tasks_logic,
    _ideas_to_goals_logic,
    _node_store,
    _tasks_to_workflow_logic,
)


# ---------------------------------------------------------------------------
# Fixtures
# ---------------------------------------------------------------------------


@pytest.fixture(autouse=True)
def _clear_node_store():
    """Reset the in-memory node store between tests."""
    _node_store.clear()
    yield
    _node_store.clear()


def _make_handler_ctx() -> dict[str, Any]:
    return {}


def _make_http_handler(body: dict[str, Any] | None = None) -> MagicMock:
    handler = MagicMock()
    handler.headers = {"Content-Length": "0"}
    handler.client_address = ("127.0.0.1", 12345)
    if body is not None:
        raw = json.dumps(body).encode()
        handler.headers = {"Content-Length": str(len(raw))}
        handler.rfile.read.return_value = raw
    return handler


# ---------------------------------------------------------------------------
# PipelineNode model tests
# ---------------------------------------------------------------------------


class TestPipelineNode:
    def test_hash_is_deterministic(self):
        a = PipelineNode(id="n1", stage="idea", label="Test")
        b = PipelineNode(id="n1", stage="idea", label="Test")
        assert a.hash == b.hash
        assert len(a.hash) == 16

    def test_hash_changes_with_label(self):
        a = PipelineNode(id="n1", stage="idea", label="Alpha")
        b = PipelineNode(id="n1", stage="idea", label="Beta")
        assert a.hash != b.hash

    def test_custom_hash_preserved(self):
        node = PipelineNode(id="n1", stage="idea", label="X", hash="custom123")
        assert node.hash == "custom123"

    def test_derived_from_defaults_empty(self):
        node = PipelineNode(id="n1", stage="idea", label="X")
        assert node.derived_from == []


# ---------------------------------------------------------------------------
# Clustering helper
# ---------------------------------------------------------------------------


class TestClusterIdeas:
    def test_empty_list(self):
        assert _cluster_ideas([]) == []

    def test_single_idea(self):
        ideas = [{"label": "build API"}]
        clusters = _cluster_ideas(ideas)
        assert len(clusters) == 1
        assert clusters[0] == ideas

    def test_two_ideas_single_cluster(self):
        ideas = [{"label": "build API"}, {"label": "test API"}]
        clusters = _cluster_ideas(ideas)
        assert len(clusters) == 1

    def test_disjoint_ideas_separate_clusters(self):
        ideas = [
            {"label": "frontend design"},
            {"label": "backend optimization"},
            {"label": "database migration"},
        ]
        clusters = _cluster_ideas(ideas)
        assert len(clusters) == 3


# ---------------------------------------------------------------------------
# Ideas-to-goals transition
# ---------------------------------------------------------------------------


class TestIdeasToGoals:
    def test_basic_transition(self):
        ideas = [
            {"id": "idea-1", "label": "Improve onboarding"},
            {"id": "idea-2", "label": "Improve dashboard"},
        ]
        result = _ideas_to_goals_logic(ideas)
        assert isinstance(result, TransitionResult)
        # At least the idea nodes + 1 goal
        assert len(result.nodes) >= 3
        goal_nodes = [n for n in result.nodes if n.stage == "goal"]
        assert len(goal_nodes) >= 1
        assert result.provenance["method"] in ("keyword_clustering", "meta_planner")

    def test_edges_are_derives(self):
        ideas = [{"id": "idea-1", "label": "Build feature"}]
        result = _ideas_to_goals_logic(ideas)
        derive_edges = [e for e in result.edges if e.edge_type == "derives"]
        assert len(derive_edges) >= 1

    def test_context_included_in_goal_metadata(self):
        ideas = [{"id": "idea-1", "label": "Ship fast"}]
        result = _ideas_to_goals_logic(ideas, context="Q1 sprint")
        goal_nodes = [n for n in result.nodes if n.stage == "goal"]
        assert goal_nodes[0].metadata["context"] == "Q1 sprint"

    def test_nodes_stored_in_store(self):
        ideas = [{"id": "idea-1", "label": "Test storing"}]
        result = _ideas_to_goals_logic(ideas)
        for node in result.nodes:
            assert node.id in _node_store

    def test_handler_endpoint(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"ideas": [{"id": "i1", "label": "idea one"}, {"id": "i2", "label": "idea two"}]}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/ideas-to-goals", {}, http_handler)
        assert resp is not None
        data, status, _ = resp
        assert "nodes" in data
        assert "edges" in data
        assert "provenance" in data

    def test_empty_ideas_returns_error(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"ideas": []}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/ideas-to-goals", {}, http_handler)
        assert resp is not None
        _, status, _ = resp
        assert status == 400


# ---------------------------------------------------------------------------
# Goals-to-tasks transition
# ---------------------------------------------------------------------------


class TestGoalsToTasks:
    def test_basic_decomposition(self):
        goals = [
            {
                "id": "goal-1",
                "label": "Ship MVP",
                "metadata": {"key_results": ["Build API", "Write tests", "Deploy"]},
            }
        ]
        result = _goals_to_tasks_logic(goals)
        task_nodes = [n for n in result.nodes if n.stage == "action"]
        assert len(task_nodes) == 3

    def test_max_tasks_constraint(self):
        goals = [
            {
                "id": "goal-1",
                "label": "Big goal",
                "metadata": {"key_results": ["a", "b", "c", "d", "e"]},
            }
        ]
        result = _goals_to_tasks_logic(goals, max_tasks=2)
        assert len(result.nodes) == 2

    def test_decomposes_edges(self):
        goals = [{"id": "goal-1", "label": "Test", "metadata": {"key_results": ["t1"]}}]
        result = _goals_to_tasks_logic(goals)
        decompose_edges = [e for e in result.edges if e.edge_type == "decomposes"]
        assert len(decompose_edges) >= 1

    def test_dependency_edges_between_tasks(self):
        goals = [
            {
                "id": "goal-1",
                "label": "Multi-step",
                "metadata": {"key_results": ["step 1", "step 2", "step 3"]},
            }
        ]
        result = _goals_to_tasks_logic(goals)
        dep_edges = [e for e in result.edges if e.edge_type == "depends_on"]
        assert len(dep_edges) == 2  # 3 tasks -> 2 sequential deps

    def test_assignee_types_cycle(self):
        goals = [
            {
                "id": "goal-1",
                "label": "Team work",
                "metadata": {"key_results": ["a", "b", "c"]},
            }
        ]
        result = _goals_to_tasks_logic(goals)
        assignees = [n.metadata["assignee_type"] for n in result.nodes]
        assert assignees == ["researcher", "implementer", "reviewer"]

    def test_handler_endpoint(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"goals": [{"id": "g1", "label": "Goal", "metadata": {"key_results": ["kr1"]}}]}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/goals-to-tasks", {}, http_handler)
        assert resp is not None
        data, status, _ = resp
        assert len(data["nodes"]) >= 1

    def test_empty_goals_returns_error(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"goals": []}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/goals-to-tasks", {}, http_handler)
        assert resp is not None
        _, status, _ = resp
        assert status == 400


# ---------------------------------------------------------------------------
# Tasks-to-workflow transition
# ---------------------------------------------------------------------------


class TestTasksToWorkflow:
    def test_basic_dag_generation(self):
        tasks = [
            {"id": "t1", "label": "Research", "metadata": {"assignee_type": "researcher"}},
            {"id": "t2", "label": "Implement", "metadata": {"assignee_type": "implementer"}},
        ]
        result = _tasks_to_workflow_logic(tasks)
        orch_nodes = [n for n in result.nodes if n.stage == "orchestration"]
        assert len(orch_nodes) == 2

    def test_triggers_edges(self):
        tasks = [{"id": "t1", "label": "Do it", "metadata": {"assignee_type": "implementer"}}]
        result = _tasks_to_workflow_logic(tasks)
        trigger_edges = [e for e in result.edges if e.edge_type == "triggers"]
        assert len(trigger_edges) == 1

    def test_execution_mode_in_metadata(self):
        tasks = [{"id": "t1", "label": "Task", "metadata": {}}]
        result = _tasks_to_workflow_logic(tasks, execution_mode="sequential")
        orch = result.nodes[0]
        assert orch.metadata["execution_mode"] == "sequential"

    def test_agent_type_mapping(self):
        tasks = [
            {"id": "t1", "label": "Research", "metadata": {"assignee_type": "researcher"}},
            {"id": "t2", "label": "Code", "metadata": {"assignee_type": "implementer"}},
            {"id": "t3", "label": "Review", "metadata": {"assignee_type": "reviewer"}},
        ]
        result = _tasks_to_workflow_logic(tasks)
        agent_types = [n.metadata["agent_type"] for n in result.nodes]
        assert agent_types == ["research_agent", "code_agent", "review_agent"]

    def test_handler_endpoint(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"tasks": [{"id": "t1", "label": "Do", "metadata": {}}]}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/tasks-to-workflow", {}, http_handler)
        assert resp is not None
        data, status, _ = resp
        assert len(data["nodes"]) == 1


# ---------------------------------------------------------------------------
# Execute endpoint
# ---------------------------------------------------------------------------


class TestExecute:
    def test_dry_run_returns_plan(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {
            "workflow_id": "wf-1",
            "nodes": [{"id": "o1"}],
            "edges": [{"source": "a", "target": "b"}],
            "dry_run": True,
        }
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/execute", {}, http_handler)
        assert resp is not None
        data, status, _ = resp
        assert data["status"] == "dry_run"
        assert "plan" in data
        assert data["plan"]["node_count"] == 1

    def test_real_execution_returns_started(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"workflow_id": "wf-2", "nodes": [{"id": "o1"}], "edges": []}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/execute", {}, http_handler)
        assert resp is not None
        data, status, _ = resp
        assert data["status"] == "started"
        assert data["execution_id"].startswith("exec-")

    def test_empty_nodes_returns_error(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"workflow_id": "wf-3", "nodes": [], "edges": []}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/execute", {}, http_handler)
        assert resp is not None
        _, status, _ = resp
        assert status == 400


# ---------------------------------------------------------------------------
# Provenance endpoint
# ---------------------------------------------------------------------------


class TestProvenance:
    def test_provenance_chain(self):
        # Manually populate store with a chain: idea -> goal -> task
        _node_store["idea-1"] = {
            "id": "idea-1",
            "stage": "idea",
            "label": "Origin",
            "derived_from": [],
            "hash": "aaa",
            "metadata": {},
        }
        _node_store["goal-1"] = {
            "id": "goal-1",
            "stage": "goal",
            "label": "Goal",
            "derived_from": ["idea-1"],
            "hash": "bbb",
            "metadata": {},
        }
        _node_store["task-1"] = {
            "id": "task-1",
            "stage": "action",
            "label": "Task",
            "derived_from": ["goal-1"],
            "hash": "ccc",
            "metadata": {},
        }

        chain = _get_provenance_chain("task-1")
        assert len(chain) == 3
        assert chain[0]["id"] == "idea-1"
        assert chain[1]["id"] == "goal-1"
        assert chain[2]["id"] == "task-1"

    def test_provenance_single_node(self):
        _node_store["lone-1"] = {
            "id": "lone-1",
            "stage": "idea",
            "label": "Solo",
            "derived_from": [],
            "hash": "xxx",
            "metadata": {},
        }
        chain = _get_provenance_chain("lone-1")
        assert len(chain) == 1

    def test_provenance_missing_node_returns_empty(self):
        chain = _get_provenance_chain("nonexistent")
        assert chain == []

    def test_provenance_handler_endpoint(self):
        _node_store["n1"] = {
            "id": "n1",
            "stage": "idea",
            "label": "Idea",
            "derived_from": [],
            "hash": "h1",
            "metadata": {},
        }
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        http_handler = _make_http_handler()
        resp = h.handle("/api/v1/pipeline/transitions/n1/provenance", {}, http_handler)
        assert resp is not None
        data, status, _ = resp
        assert data["node_id"] == "n1"
        assert data["depth"] == 1

    def test_provenance_not_found(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        http_handler = _make_http_handler()
        resp = h.handle("/api/v1/pipeline/transitions/missing/provenance", {}, http_handler)
        assert resp is not None
        _, status, _ = resp
        assert status == 404


# ---------------------------------------------------------------------------
# Can-handle routing
# ---------------------------------------------------------------------------


class TestRouting:
    def test_can_handle_transitions_path(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        assert h.can_handle("/api/v1/pipeline/transitions/ideas-to-goals") is True

    def test_cannot_handle_other_path(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        assert h.can_handle("/api/v1/debates") is False

    def test_handle_post_invalid_json(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        handler = MagicMock()
        handler.headers = {"Content-Length": "5"}
        handler.rfile.read.return_value = b"notjson"  # triggers JSONDecodeError
        handler.client_address = ("127.0.0.1", 12345)
        resp = h.handle_post("/api/v1/pipeline/transitions/ideas-to-goals", {}, handler)
        assert resp is not None
        _, status, _ = resp
        assert status == 400

    def test_handle_post_unknown_sub_path_returns_none(self):
        h = PipelineTransitionsHandler(ctx=_make_handler_ctx())
        body = {"ideas": [{"label": "x"}]}
        http_handler = _make_http_handler(body)
        resp = h.handle_post("/api/v1/pipeline/transitions/unknown-endpoint", {}, http_handler)
        assert resp is None


# ---------------------------------------------------------------------------
# End-to-end: full pipeline chain
# ---------------------------------------------------------------------------


class TestFullChain:
    def test_ideas_to_goals_to_tasks_to_workflow(self):
        # Stage 1: Ideas -> Goals
        ideas = [
            {"id": "idea-a", "label": "Build search feature"},
            {"id": "idea-b", "label": "Build indexing"},
        ]
        goal_result = _ideas_to_goals_logic(ideas)
        goal_nodes = [n for n in goal_result.nodes if n.stage == "goal"]
        assert len(goal_nodes) >= 1

        # Stage 2: Goals -> Tasks
        goals = [asdict(g) for g in goal_nodes]
        task_result = _goals_to_tasks_logic(goals)
        task_nodes = task_result.nodes
        assert len(task_nodes) >= 1

        # Stage 3: Tasks -> Workflow
        tasks = [asdict(t) for t in task_nodes]
        workflow_result = _tasks_to_workflow_logic(tasks)
        orch_nodes = workflow_result.nodes
        assert len(orch_nodes) >= 1

        # Provenance: trace back from an orchestration node
        orch_id = orch_nodes[0].id
        chain = _get_provenance_chain(orch_id)
        # Should have at least: idea -> goal -> task -> orchestration path
        assert len(chain) >= 2
        stages_in_chain = [c["stage"] for c in chain]
        assert "orchestration" in stages_in_chain


# ---------------------------------------------------------------------------
# LLM-powered transition paths (mocked subsystems)
# ---------------------------------------------------------------------------


class TestMetaPlannerPath:
    """Test that MetaPlanner integration works when available."""

    def test_meta_planner_produces_goals(self):
        """When MetaPlanner returns PrioritizedGoals, they become goal nodes."""
        from aragora.nomic.meta_planner import PrioritizedGoal, Track

        mock_goals = [
            PrioritizedGoal(
                id="mp-1",
                track=Track.CORE,
                description="Harden authentication flow",
                rationale="Critical for security",
                estimated_impact="high",
                priority=1,
                focus_areas=["auth", "session management"],
            ),
            PrioritizedGoal(
                id="mp-2",
                track=Track.QA,
                description="Add E2E test coverage for login",
                rationale="Prevents regressions",
                estimated_impact="medium",
                priority=2,
                focus_areas=["tests", "e2e"],
            ),
        ]

        with patch(
            "aragora.server.handlers.pipeline.transitions.MetaPlanner",
            create=True,
        ) as MockPlanner:
            # Build a mock that returns our canned goals
            instance = MagicMock()
            MockPlanner.return_value = instance

            # We need to actually call the real function and have MetaPlanner importable.
            # Instead, directly test the logic with a real MetaPlanner import but mocked prioritize_work.
            pass

        # Use the real function â€” MetaPlanner is importable in this env
        ideas = [
            {"id": "idea-a", "label": "Harden auth"},
            {"id": "idea-b", "label": "Add login tests"},
        ]
        result = _ideas_to_goals_logic(ideas)
        assert isinstance(result, TransitionResult)
        goal_nodes = [n for n in result.nodes if n.stage == "goal"]
        assert len(goal_nodes) >= 1
        assert result.provenance["method"] in ("meta_planner", "keyword_clustering")
        # Provenance should always have these keys
        assert "timestamp" in result.provenance
        assert "source_count" in result.provenance
        assert "output_count" in result.provenance

    def test_meta_planner_fallback_on_import_error(self):
        """When MetaPlanner import fails, falls back to heuristic."""
        with patch.dict("sys.modules", {"aragora.nomic.meta_planner": None}):
            ideas = [
                {"id": "idea-1", "label": "Build feature X"},
                {"id": "idea-2", "label": "Test feature X"},
            ]
            result = _ideas_to_goals_logic(ideas)
            assert result.provenance["method"] == "keyword_clustering"
            goal_nodes = [n for n in result.nodes if n.stage == "goal"]
            assert len(goal_nodes) >= 1

    def test_meta_planner_fallback_on_runtime_error(self):
        """When MetaPlanner raises RuntimeError, falls back to heuristic."""
        with patch(
            "aragora.nomic.meta_planner.MetaPlanner"
        ) as MockPlanner:
            MockPlanner.side_effect = RuntimeError("LLM unavailable")
            ideas = [{"id": "idea-1", "label": "Optimize search"}]
            result = _ideas_to_goals_logic(ideas)
            assert result.provenance["method"] == "keyword_clustering"


class TestTaskDecomposerPath:
    """Test that TaskDecomposer integration works when available."""

    def test_task_decomposer_produces_tasks(self):
        """When TaskDecomposer returns subtasks, they become action nodes."""
        from aragora.nomic.task_decomposer import (
            DecomposerConfig,
            SubTask,
            TaskDecomposer,
            TaskDecomposition,
        )

        mock_subtasks = [
            SubTask(
                id="sub-1",
                title="Write unit tests for auth module",
                description="Cover all auth edge cases",
                estimated_complexity="medium",
                file_scope=["aragora/auth/"],
            ),
            SubTask(
                id="sub-2",
                title="Add integration tests for login flow",
                description="E2E login tests",
                estimated_complexity="high",
                file_scope=["tests/integration/"],
            ),
        ]
        mock_decomposition = TaskDecomposition(
            original_task="Improve test coverage",
            complexity_score=7,
            complexity_level="high",
            should_decompose=True,
            subtasks=mock_subtasks,
        )

        with patch(
            "aragora.nomic.task_decomposer.TaskDecomposer.analyze",
            return_value=mock_decomposition,
        ):
            goals = [
                {
                    "id": "goal-1",
                    "label": "Improve test coverage",
                    "metadata": {"key_results": ["Write more tests"]},
                }
            ]
            result = _goals_to_tasks_logic(goals)
            assert result.provenance["method"] == "task_decomposer"
            task_nodes = [n for n in result.nodes if n.stage == "action"]
            assert len(task_nodes) == 2
            assert task_nodes[0].label == "Write unit tests for auth module"
            assert task_nodes[1].label == "Add integration tests for login flow"
            assert task_nodes[0].metadata["estimated_effort"] == "medium"
            # Verify dependency edges between sequential tasks
            dep_edges = [e for e in result.edges if e.edge_type == "depends_on"]
            assert len(dep_edges) == 1

    def test_task_decomposer_fallback_on_import_error(self):
        """When TaskDecomposer import fails, falls back to heuristic."""
        with patch.dict("sys.modules", {"aragora.nomic.task_decomposer": None}):
            goals = [
                {
                    "id": "goal-1",
                    "label": "Ship MVP",
                    "metadata": {"key_results": ["Build API", "Write tests"]},
                }
            ]
            result = _goals_to_tasks_logic(goals)
            assert result.provenance["method"] == "heuristic_decomposition"
            assert len(result.nodes) == 2

    def test_task_decomposer_respects_max_tasks(self):
        """TaskDecomposer path respects max_tasks constraint."""
        from aragora.nomic.task_decomposer import SubTask, TaskDecomposition

        mock_subtasks = [
            SubTask(id=f"sub-{i}", title=f"Task {i}", description=f"Desc {i}")
            for i in range(5)
        ]
        mock_decomposition = TaskDecomposition(
            original_task="Big goal",
            complexity_score=8,
            complexity_level="high",
            should_decompose=True,
            subtasks=mock_subtasks,
        )

        with patch(
            "aragora.nomic.task_decomposer.TaskDecomposer.analyze",
            return_value=mock_decomposition,
        ):
            goals = [{"id": "goal-1", "label": "Big goal", "metadata": {}}]
            result = _goals_to_tasks_logic(goals, max_tasks=2)
            task_nodes = [n for n in result.nodes if n.stage == "action"]
            assert len(task_nodes) == 2


class TestWorkflowEnginePath:
    """Test that WorkflowEngine integration works when available."""

    def test_workflow_engine_produces_orchestration_nodes(self):
        """When WorkflowEngine types are importable, they produce orch nodes with workflow_id."""
        tasks = [
            {"id": "t1", "label": "Research", "metadata": {"assignee_type": "researcher"}},
            {"id": "t2", "label": "Implement", "metadata": {"assignee_type": "implementer"}},
        ]
        result = _tasks_to_workflow_logic(tasks)
        assert isinstance(result, TransitionResult)
        orch_nodes = [n for n in result.nodes if n.stage == "orchestration"]
        assert len(orch_nodes) == 2
        # The method depends on whether workflow engine imports succeed
        assert result.provenance["method"] in ("workflow_engine", "dag_generation")
        assert "timestamp" in result.provenance
        assert "source_count" in result.provenance
        assert result.provenance["output_count"] == 2

        # If workflow_engine was used, nodes should have workflow_id metadata
        if result.provenance["method"] == "workflow_engine":
            assert "workflow_id" in result.provenance
            for node in orch_nodes:
                assert "workflow_id" in node.metadata
                assert "step_id" in node.metadata

    def test_workflow_engine_fallback_on_import_error(self):
        """When WorkflowEngine import fails, falls back to heuristic."""
        with patch.dict("sys.modules", {"aragora.workflow.types": None}):
            tasks = [
                {"id": "t1", "label": "Do thing", "metadata": {"assignee_type": "implementer"}},
            ]
            result = _tasks_to_workflow_logic(tasks)
            assert result.provenance["method"] == "dag_generation"
            assert len(result.nodes) == 1
            assert result.nodes[0].stage == "orchestration"

    def test_workflow_engine_preserves_execution_mode(self):
        """Execution mode is propagated through the workflow engine path."""
        tasks = [{"id": "t1", "label": "Task", "metadata": {}}]
        result = _tasks_to_workflow_logic(tasks, execution_mode="sequential")
        for node in result.nodes:
            assert node.metadata["execution_mode"] == "sequential"
        assert result.provenance.get("execution_mode") == "sequential"


class TestProvenanceTracking:
    """Test provenance metadata across all transition paths."""

    def test_all_transitions_include_timestamp(self):
        """Every transition result should have a timestamp in provenance."""
        ideas = [{"id": "i1", "label": "Idea"}]
        goals = [{"id": "g1", "label": "Goal", "metadata": {"key_results": ["kr"]}}]
        tasks = [{"id": "t1", "label": "Task", "metadata": {}}]

        r1 = _ideas_to_goals_logic(ideas)
        r2 = _goals_to_tasks_logic(goals)
        r3 = _tasks_to_workflow_logic(tasks)

        for result in [r1, r2, r3]:
            assert "timestamp" in result.provenance
            assert "method" in result.provenance
            assert "source_count" in result.provenance
            assert "output_count" in result.provenance
