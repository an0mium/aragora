"""
Tests for aragora.pipeline.pr_generator module.

Tests the DecisionMemo, PatchPlan, and PRGenerator classes for:
- Dataclass creation and serialization
- Markdown generation
- Decision/step extraction from debate content
- File change inference
"""

import pytest
from datetime import datetime
from unittest.mock import MagicMock, patch

from aragora.pipeline.pr_generator import (
    DecisionMemo,
    PatchPlan,
    PRGenerator,
)


class TestDecisionMemo:
    """Tests for the DecisionMemo dataclass."""

    def test_memo_creation_with_defaults(self):
        """Test creating a DecisionMemo with minimal required fields."""
        memo = DecisionMemo(
            debate_id="debate-001",
            title="Test Decision",
            summary="Summary of the debate",
            key_decisions=["Decision 1", "Decision 2"],
            rationale="Because it makes sense",
            supporting_evidence=[],
            dissenting_views=[],
            open_questions=[],
        )

        assert memo.debate_id == "debate-001"
        assert memo.title == "Test Decision"
        assert len(memo.key_decisions) == 2
        assert memo.consensus_confidence == 0.0  # default
        assert memo.rounds_used == 0  # default
        assert memo.agent_count == 0  # default
        assert memo.created_at  # Should be auto-populated

    def test_memo_creation_with_all_fields(self):
        """Test creating a DecisionMemo with all fields."""
        memo = DecisionMemo(
            debate_id="debate-002",
            title="Full Decision",
            summary="Complete summary",
            key_decisions=["Use async/await", "Add error handling"],
            rationale="Improves performance and reliability",
            supporting_evidence=[
                {"source": "profiler", "summary": "50% latency reduction"},
                {"source": "docs", "summary": "Best practice recommendation"},
            ],
            dissenting_views=["Some prefer callbacks"],
            open_questions=["What about legacy support?"],
            created_at="2025-01-01T12:00:00",
            consensus_confidence=0.85,
            rounds_used=3,
            agent_count=4,
        )

        assert memo.consensus_confidence == 0.85
        assert memo.rounds_used == 3
        assert memo.agent_count == 4
        assert len(memo.supporting_evidence) == 2
        assert len(memo.dissenting_views) == 1
        assert len(memo.open_questions) == 1

    def test_memo_to_markdown(self):
        """Test markdown generation."""
        memo = DecisionMemo(
            debate_id="md-test",
            title="Markdown Test",
            summary="Testing markdown output",
            key_decisions=["First decision", "Second decision"],
            rationale="Good reasons",
            supporting_evidence=[{"source": "test", "summary": "Evidence"}],
            dissenting_views=["Disagreement noted"],
            open_questions=["Question 1"],
            consensus_confidence=0.75,
            rounds_used=2,
            agent_count=3,
        )

        md = memo.to_markdown()

        assert "# Decision Memo: Markdown Test" in md
        assert "md-test" in md
        assert "75%" in md  # consensus_confidence
        assert "2 rounds" in md
        assert "First decision" in md
        assert "Second decision" in md
        assert "Good reasons" in md
        assert "Evidence" in md
        assert "Disagreement noted" in md
        assert "Question 1" in md
        assert "Generated by aragora" in md

    def test_memo_to_markdown_no_dissent(self):
        """Test markdown with empty dissenting views."""
        memo = DecisionMemo(
            debate_id="no-dissent",
            title="Agreement",
            summary="Everyone agreed",
            key_decisions=["Unanimous decision"],
            rationale="Clear choice",
            supporting_evidence=[],
            dissenting_views=[],
            open_questions=[],
        )

        md = memo.to_markdown()
        assert "*None recorded*" in md  # Default for empty dissenting views

    def test_memo_to_dict(self):
        """Test dictionary serialization."""
        memo = DecisionMemo(
            debate_id="dict-test",
            title="Dict Test",
            summary="Summary",
            key_decisions=["D1"],
            rationale="R",
            supporting_evidence=[{"source": "s", "summary": "e"}],
            dissenting_views=["DV"],
            open_questions=["OQ"],
            consensus_confidence=0.9,
            rounds_used=5,
            agent_count=6,
        )

        d = memo.to_dict()

        assert d["debate_id"] == "dict-test"
        assert d["title"] == "Dict Test"
        assert d["key_decisions"] == ["D1"]
        assert d["consensus_confidence"] == 0.9
        assert d["rounds_used"] == 5
        assert d["agent_count"] == 6
        assert "created_at" in d


class TestPatchPlan:
    """Tests for the PatchPlan dataclass."""

    def test_plan_creation_with_defaults(self):
        """Test creating a PatchPlan with minimal fields."""
        plan = PatchPlan(
            debate_id="patch-001",
            title="Add Feature",
            description="Add new authentication feature",
            steps=[],
            file_changes=[],
            dependencies=[],
            estimated_complexity="medium",
        )

        assert plan.debate_id == "patch-001"
        assert plan.title == "Add Feature"
        assert plan.estimated_complexity == "medium"
        assert plan.created_at  # Auto-populated

    def test_plan_with_steps_and_files(self):
        """Test PatchPlan with full content."""
        plan = PatchPlan(
            debate_id="patch-002",
            title="Refactor Auth",
            description="Refactor authentication module",
            steps=[
                {
                    "step_num": 1,
                    "action": "Create interface",
                    "target": "auth/interface.py",
                    "details": "Define AuthProvider interface",
                    "verification": "Type checking passes",
                },
                {
                    "step_num": 2,
                    "action": "Implement OAuth",
                    "target": "auth/oauth.py",
                    "details": "OAuth 2.0 implementation",
                    "verification": "Integration tests pass",
                },
            ],
            file_changes=[
                {"path": "auth/interface.py", "action": "create", "description": "New file"},
                {"path": "auth/oauth.py", "action": "create", "description": "OAuth impl"},
                {"path": "auth/__init__.py", "action": "modify", "description": "Add exports"},
            ],
            dependencies=["oauthlib", "requests"],
            estimated_complexity="high",
        )

        assert len(plan.steps) == 2
        assert len(plan.file_changes) == 3
        assert len(plan.dependencies) == 2

    def test_plan_to_markdown(self):
        """Test markdown generation."""
        plan = PatchPlan(
            debate_id="md-plan",
            title="Test Plan",
            description="Test description",
            steps=[
                {
                    "step_num": 1,
                    "action": "First step",
                    "target": "src/main.py",
                    "details": "Do the first thing",
                    "verification": "Test passes",
                },
            ],
            file_changes=[
                {"path": "src/main.py", "action": "modify", "description": "Update main"},
            ],
            dependencies=["pytest"],
            estimated_complexity="low",
        )

        md = plan.to_markdown()

        assert "# Patch Plan: Test Plan" in md
        assert "md-plan" in md
        assert "LOW" in md  # Complexity in uppercase
        assert "Step 1: First step" in md
        assert "src/main.py" in md
        assert "pytest" in md
        assert "Generated by aragora" in md

    def test_plan_to_markdown_no_dependencies(self):
        """Test markdown with no dependencies."""
        plan = PatchPlan(
            debate_id="no-deps",
            title="Simple Plan",
            description="No external deps",
            steps=[],
            file_changes=[],
            dependencies=[],
            estimated_complexity="low",
        )

        md = plan.to_markdown()
        assert "*None*" in md  # Default for empty dependencies

    def test_plan_to_dict(self):
        """Test dictionary serialization."""
        plan = PatchPlan(
            debate_id="dict-plan",
            title="Dict Plan",
            description="Desc",
            steps=[{"step_num": 1, "action": "A"}],
            file_changes=[{"path": "f.py", "action": "create"}],
            dependencies=["dep1"],
            estimated_complexity="medium",
        )

        d = plan.to_dict()

        assert d["debate_id"] == "dict-plan"
        assert d["title"] == "Dict Plan"
        assert len(d["steps"]) == 1
        assert len(d["file_changes"]) == 1
        assert d["estimated_complexity"] == "medium"


class TestPRGenerator:
    """Tests for the PRGenerator class."""

    @pytest.fixture
    def mock_artifact(self):
        """Create a mock DebateArtifact."""
        artifact = MagicMock()
        artifact.debate_id = "gen-debate-001"
        artifact.task = "Implement a rate limiter for the API endpoints."
        artifact.rounds = 3
        artifact.agents = ["claude", "gpt-4", "gemini"]

        # Mock consensus proof
        consensus = MagicMock()
        consensus.final_answer = """
To implement rate limiting, we should:
1. Use a token bucket algorithm for flexibility
2. Store rate limit state in Redis for scalability
3. Add middleware to check limits before processing

The key decisions are:
- Token bucket over sliding window (better burst handling)
- Per-user limits with IP fallback
- 100 requests per minute default
"""
        consensus.confidence = 0.88
        consensus.dissenting_views = ["Some prefer sliding window"]
        consensus.critiques = []
        artifact.consensus_proof = consensus

        # Mock trace data
        artifact.trace_data = {
            "events": [
                {"event_type": "agent_message", "content": "Discussion"},
                {
                    "event_type": "agent_synthesis",
                    "content": {"content": "Rate limiting improves API reliability"},
                },
            ]
        }

        # Mock provenance data
        artifact.provenance_data = {
            "chain": {
                "records": [
                    {
                        "source_type": "documentation",
                        "content": "Redis docs recommend token bucket",
                    },
                ]
            }
        }

        # Required for complexity estimation
        artifact.critique_count = 2

        return artifact

    def test_generator_creation(self, mock_artifact):
        """Test PRGenerator initialization."""
        gen = PRGenerator(mock_artifact)
        assert gen.artifact == mock_artifact

    def test_generate_decision_memo(self, mock_artifact):
        """Test decision memo generation."""
        gen = PRGenerator(mock_artifact)
        memo = gen.generate_decision_memo()

        assert isinstance(memo, DecisionMemo)
        assert memo.debate_id == "gen-debate-001"
        assert memo.rounds_used == 3
        assert memo.agent_count == 3
        assert memo.consensus_confidence == 0.88
        assert len(memo.key_decisions) > 0

    def test_generate_patch_plan(self, mock_artifact):
        """Test patch plan generation."""
        gen = PRGenerator(mock_artifact)
        plan = gen.generate_patch_plan()

        assert isinstance(plan, PatchPlan)
        assert plan.debate_id == "gen-debate-001"
        assert len(plan.steps) > 0
        assert plan.estimated_complexity in ["low", "medium", "high"]

    def test_extract_title_short(self, mock_artifact):
        """Test title extraction from short task."""
        mock_artifact.task = "Fix bug"
        gen = PRGenerator(mock_artifact)

        title = gen._extract_title("Fix bug")
        assert title == "Fix bug"

    def test_extract_title_with_period(self, mock_artifact):
        """Test title extraction with sentence."""
        gen = PRGenerator(mock_artifact)

        title = gen._extract_title("Implement feature. More details follow.")
        assert title == "Implement feature."

    def test_extract_title_long(self, mock_artifact):
        """Test title extraction from long text."""
        gen = PRGenerator(mock_artifact)
        long_text = "A" * 100

        title = gen._extract_title(long_text)
        assert len(title) <= 63  # 60 + "..."
        assert title.endswith("...")

    def test_extract_decisions_numbered(self, mock_artifact):
        """Test extracting numbered decisions."""
        gen = PRGenerator(mock_artifact)

        text = """
1. First important decision about the system
2. Second crucial choice we made
3. Third decision point
"""
        decisions = gen._extract_decisions(text)

        assert len(decisions) >= 3
        assert any("important" in d.lower() for d in decisions)

    def test_extract_decisions_bullets(self, mock_artifact):
        """Test extracting bullet point decisions."""
        gen = PRGenerator(mock_artifact)

        text = """
- Use async/await for better performance
- Implement caching layer
* Add error boundaries
"""
        decisions = gen._extract_decisions(text)

        assert len(decisions) >= 2

    def test_extract_decisions_fallback(self, mock_artifact):
        """Test fallback to sentence extraction."""
        gen = PRGenerator(mock_artifact)

        text = "This is a sentence with a decision. Another sentence follows. Third one here."
        decisions = gen._extract_decisions(text)

        # Should extract sentences as fallback
        assert len(decisions) > 0

    def test_extract_steps_numbered(self, mock_artifact):
        """Test step extraction from numbered text."""
        gen = PRGenerator(mock_artifact)

        text = """
1. Create the database schema
2. Implement the data access layer
3. Add unit tests for the new code
"""
        steps = gen._extract_steps(text)

        assert len(steps) >= 3
        assert steps[0]["step_num"] == 1
        assert "action" in steps[0]

    def test_extract_steps_fallback(self, mock_artifact):
        """Test step extraction fallback to defaults."""
        gen = PRGenerator(mock_artifact)

        text = "Just some text without steps."
        steps = gen._extract_steps(text)

        # Should return default steps
        assert len(steps) == 3
        assert steps[0]["action"] == "Review debate conclusions"

    def test_infer_file_changes(self, mock_artifact):
        """Test file path extraction from text."""
        gen = PRGenerator(mock_artifact)

        text = """
We need to modify `src/auth/handler.py` and create `src/auth/rate_limiter.py`.
Also update `tests/test_auth.py` for the new tests.
"""
        changes = gen._infer_file_changes(text)

        assert len(changes) == 3
        assert any("handler.py" in c["path"] for c in changes)
        assert any("rate_limiter.py" in c["path"] for c in changes)

    def test_infer_file_changes_limit(self, mock_artifact):
        """Test that file changes are limited to 10."""
        gen = PRGenerator(mock_artifact)

        # Create text with many file references
        text = " ".join([f"`file{i}.py`" for i in range(20)])
        changes = gen._infer_file_changes(text)

        assert len(changes) <= 10

    def test_build_rationale_with_synthesis(self, mock_artifact):
        """Test rationale building from synthesis events."""
        gen = PRGenerator(mock_artifact)
        rationale = gen._build_rationale()

        assert "reliability" in rationale or len(rationale) > 0

    def test_build_rationale_no_trace(self, mock_artifact):
        """Test rationale fallback when no trace data."""
        mock_artifact.trace_data = None
        gen = PRGenerator(mock_artifact)

        rationale = gen._build_rationale()
        assert "multi-agent debate" in rationale.lower()

    def test_extract_dissenting_views(self, mock_artifact):
        """Test dissenting views extraction."""
        gen = PRGenerator(mock_artifact)
        consensus = mock_artifact.consensus_proof

        dissenting = gen._extract_dissenting_views(consensus)

        # Should include explicit dissenting view
        assert any("sliding window" in d.lower() for d in dissenting)

    def test_extract_open_questions(self, mock_artifact):
        """Test open questions extraction from critiques."""
        # Add critiques to consensus
        critique1 = MagicMock()
        critique1.agent = "claude"
        critique1.issues = ["How to handle edge cases?", "Minor concern"]
        critique1.severity = 0.5

        critique2 = MagicMock()
        critique2.agent = "gpt-4"
        critique2.issues = ["How to handle edge cases?", "Other issue"]
        critique2.severity = 0.6

        mock_artifact.consensus_proof.critiques = [critique1, critique2]

        gen = PRGenerator(mock_artifact)
        questions = gen._extract_open_questions(mock_artifact.consensus_proof)

        # "How to handle edge cases?" appears twice, should be an open question
        assert len(questions) >= 1

    def test_no_consensus(self, mock_artifact):
        """Test handling when no consensus reached."""
        mock_artifact.consensus_proof = None
        mock_artifact.critique_count = 0

        gen = PRGenerator(mock_artifact)

        memo = gen.generate_decision_memo()
        assert memo.consensus_confidence == 0

        plan = gen.generate_patch_plan()
        assert len(plan.steps) > 0  # Should still generate default steps


class TestComplexityEstimation:
    """Tests for complexity estimation logic."""

    @pytest.fixture
    def mock_artifact(self):
        """Create artifact for complexity tests."""
        artifact = MagicMock()
        artifact.debate_id = "complexity-test"
        artifact.task = "Simple task"
        artifact.rounds = 3
        artifact.agents = ["a1", "a2"]
        artifact.consensus_proof = MagicMock()
        artifact.consensus_proof.final_answer = "Simple answer"
        artifact.consensus_proof.confidence = 0.9
        artifact.trace_data = None
        artifact.provenance_data = None
        artifact.critique_count = 2
        return artifact

    def test_estimate_complexity_low(self, mock_artifact):
        """Test low complexity estimation."""
        mock_artifact.rounds = 1
        mock_artifact.agents = ["a1"]
        mock_artifact.critique_count = 1

        gen = PRGenerator(mock_artifact)
        plan = gen.generate_patch_plan()

        # Few rounds and agents typically means lower complexity
        assert plan.estimated_complexity in ["low", "medium", "high"]

    def test_estimate_complexity_high(self, mock_artifact):
        """Test high complexity with many rounds."""
        mock_artifact.rounds = 10
        mock_artifact.agents = ["a1", "a2", "a3", "a4", "a5"]
        mock_artifact.critique_count = 10

        gen = PRGenerator(mock_artifact)
        plan = gen.generate_patch_plan()

        # Many rounds and agents may indicate higher complexity
        assert plan.estimated_complexity in ["low", "medium", "high"]


class TestIntegration:
    """Integration tests for PR generator workflow."""

    def test_full_workflow(self):
        """Test complete generation workflow."""
        # Create minimal artifact
        artifact = MagicMock()
        artifact.debate_id = "integration-001"
        artifact.task = "Add caching to improve performance."
        artifact.rounds = 4
        artifact.agents = ["claude", "gpt-4"]

        consensus = MagicMock()
        consensus.final_answer = """
1. Implement Redis-based caching
2. Add cache invalidation on write
3. Configure TTL per endpoint

Files to modify:
- `src/cache/redis_client.py`
- `src/api/middleware.py`
"""
        consensus.confidence = 0.92
        consensus.dissenting_views = []
        consensus.critiques = []
        artifact.consensus_proof = consensus
        artifact.trace_data = None
        artifact.provenance_data = None
        artifact.critique_count = 3

        # Generate artifacts
        gen = PRGenerator(artifact)

        memo = gen.generate_decision_memo()
        assert memo.debate_id == "integration-001"
        assert memo.consensus_confidence == 0.92
        assert "caching" in memo.title.lower() or "performance" in memo.title.lower()

        plan = gen.generate_patch_plan()
        assert plan.debate_id == "integration-001"
        assert len(plan.steps) >= 3  # Numbered items should be extracted
        assert len(plan.file_changes) >= 2  # File paths should be found

        # Test serialization
        memo_dict = memo.to_dict()
        plan_dict = plan.to_dict()
        assert "debate_id" in memo_dict
        assert "debate_id" in plan_dict

        # Test markdown
        memo_md = memo.to_markdown()
        plan_md = plan.to_markdown()
        assert "integration-001" in memo_md
        assert "integration-001" in plan_md


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
