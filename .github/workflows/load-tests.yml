name: Load Tests

on:
  push:
    branches: [main]
    paths:
      - 'aragora/**'
      - 'tests/load/**'
  pull_request:
    branches: [main]
    paths:
      - 'aragora/**'
      - 'tests/load/**'
  schedule:
    # Run nightly at 3am UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        default: '50'
      duration:
        description: 'Test duration (e.g., 60s, 5m)'
        required: false
        default: '60s'
      api_url:
        description: 'API URL to test'
        required: false
        default: 'http://localhost:8080'

env:
  PYTHON_VERSION: '3.11'

# Permissions needed for AWS OIDC
permissions:
  id-token: write
  contents: read

jobs:
  load-test:
    name: Load Test Suite
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,redis]"

      - name: Configure AWS credentials
        id: aws-creds
        continue-on-error: true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.AWS_DEPLOY_ROLE_NAME }}
          aws-region: us-east-2

      - name: Fetch AI provider credentials from AWS
        if: steps.aws-creds.outcome == 'success'
        run: |
          # Fetch secrets from AWS Secrets Manager
          SECRET_JSON=$(aws secretsmanager get-secret-value \
            --secret-id aragora/github-ci/api-keys \
            --query SecretString \
            --output text 2>/dev/null || echo '{}')

          # Export each key as environment variable (masked)
          if [ "$SECRET_JSON" != "{}" ]; then
            echo "ANTHROPIC_API_KEY=$(echo $SECRET_JSON | jq -r '.ANTHROPIC_API_KEY // empty')" >> $GITHUB_ENV
            echo "OPENAI_API_KEY=$(echo $SECRET_JSON | jq -r '.OPENAI_API_KEY // empty')" >> $GITHUB_ENV
            echo "OPENROUTER_API_KEY=$(echo $SECRET_JSON | jq -r '.OPENROUTER_API_KEY // empty')" >> $GITHUB_ENV
            echo "GEMINI_API_KEY=$(echo $SECRET_JSON | jq -r '.GEMINI_API_KEY // empty')" >> $GITHUB_ENV
            echo "::add-mask::$(echo $SECRET_JSON | jq -r '.ANTHROPIC_API_KEY // empty')"
            echo "::add-mask::$(echo $SECRET_JSON | jq -r '.OPENAI_API_KEY // empty')"
            echo "::add-mask::$(echo $SECRET_JSON | jq -r '.OPENROUTER_API_KEY // empty')"
            echo "::add-mask::$(echo $SECRET_JSON | jq -r '.GEMINI_API_KEY // empty')"
            echo "AI provider credentials loaded from AWS Secrets Manager"
          else
            echo "No AWS secrets available, running without AI providers"
          fi

      - name: Start Aragora server
        run: |
          # Create nomic directory for storage with absolute path
          export ARAGORA_DATA_DIR="${GITHUB_WORKSPACE}/.nomic"
          mkdir -p "$ARAGORA_DATA_DIR"
          echo "Using data directory: $ARAGORA_DATA_DIR"

          # Verify server can at least import
          echo "Verifying server imports..."
          python -c "from aragora.server.unified_server import run_unified_server; print('Server imports OK')"

          # Initialize demo data (agents, consensus) so server starts in normal mode
          echo "Initializing demo data..."
          python -c "
          from aragora.fixtures import ensure_demo_data
          ensure_demo_data()
          print('Demo data initialized')
          "

          # Show what was created
          echo "Database files created:"
          ls -la "$ARAGORA_DATA_DIR" || true

          # Start server with rate limiting disabled (all load test requests come from localhost)
          ARAGORA_DISABLE_ALL_RATE_LIMITS=1 ARAGORA_DATA_DIR="$ARAGORA_DATA_DIR" aragora serve --api-port 8080 --ws-port 8765 --host 127.0.0.1 > /tmp/aragora-server.log 2>&1 &
          SERVER_PID=$!
          echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV

          # Give server a moment to start
          sleep 3

          # Wait for server to be healthy (increased timeout for CI)
          for i in {1..60}; do
            if curl -fsS http://127.0.0.1:8080/api/health >/dev/null 2>&1; then
              echo "Server is healthy"
              break
            fi
            echo "Waiting for server... ($i/60)"
            # Show recent server logs to help debug
            if [ $((i % 10)) -eq 0 ]; then
              echo "--- Server logs (last 20 lines) ---"
              tail -20 /tmp/aragora-server.log || true
              echo "--- End server logs ---"
            fi
            sleep 2
          done

          # Final health check with better error output
          if ! curl -fsS http://127.0.0.1:8080/api/health; then
            echo "Health check failed. Server logs:"
            cat /tmp/aragora-server.log
            exit 1
          fi

          # Wait for critical endpoints to be ready (not just health)
          # These endpoints are used by the load test scripts
          echo "Waiting for critical endpoints to be ready..."
          for i in {1..30}; do
            READY=true

            # Check leaderboard-view - accept 2xx or 4xx (endpoint exists but may need auth)
            CODE=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:8080/api/v1/leaderboard-view?limit=10)
            if [ "$CODE" -ge 500 ] || [ "$CODE" -eq 000 ]; then
              READY=false
            fi

            # Check agents list - accept 2xx or 4xx
            CODE=$(curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:8080/api/v1/agents)
            if [ "$CODE" -ge 500 ] || [ "$CODE" -eq 000 ]; then
              READY=false
            fi

            if [ "$READY" = "true" ]; then
              echo "All critical endpoints ready (responding with non-5xx)"
              break
            fi

            echo "Waiting for endpoints... ($i/30)"
            sleep 2
          done

          # Log endpoint status for debugging
          echo "=== Endpoint status ==="
          echo "Leaderboard: $(curl -s -o /dev/null -w '%{http_code}' http://127.0.0.1:8080/api/v1/leaderboard-view?limit=10)"
          echo "Agents: $(curl -s -o /dev/null -w '%{http_code}' http://127.0.0.1:8080/api/v1/agents)"
          echo "Debates: $(curl -s -o /dev/null -w '%{http_code}' http://127.0.0.1:8080/api/v1/debates?limit=10)"
          echo "=== End status ==="
        env:
          ARAGORA_REDIS_URL: redis://localhost:6379/0
          # Disable rate limiting for load tests - all requests come from same IP (localhost)
          ARAGORA_ENABLE_REDIS_RATE_LIMIT: "0"
          # Disable ALL rate limiters (both Redis and local in-memory) for load tests
          ARAGORA_DISABLE_ALL_RATE_LIMITS: "1"
          # AI provider keys are set via GITHUB_ENV in "Fetch AI provider credentials" step

      - name: Run API load tests
        run: |
          k6 run tests/load/api_load.js \
            --vus ${{ github.event.inputs.concurrent_users || '50' }} \
            --duration ${{ github.event.inputs.duration || '60s' }} \
            --out json=load-results.json \
            --summary-export=load-summary.json
        env:
          API_URL: ${{ github.event.inputs.api_url || 'http://localhost:8080' }}

      - name: Run WebSocket burst test
        run: |
          k6 run tests/load/websocket_burst.js \
            --vus 100 \
            --duration 30s \
            --out json=ws-results.json
        env:
          WS_URL: ws://localhost:8765

      - name: Run rate limit validation
        run: |
          k6 run tests/load/rate_limit_test.js \
            --vus 10 \
            --duration 30s
        env:
          API_URL: http://localhost:8080

      - name: Check SLO thresholds
        id: slo_check
        run: |
          python -c "
          import json
          import sys
          import os

          # SLO Definitions (documented in docs/SCALING.md)
          SLO = {
              'p50_ms': 200,      # Median latency
              'p95_ms': 500,      # 95th percentile
              'p99_ms': 2000,     # 99th percentile (warning threshold)
              'error_rate_pct': 1.0,  # Max error rate
              'min_rps': 50,      # Minimum requests per second
          }

          with open('load-summary.json') as f:
              summary = json.load(f)

          metrics = summary.get('metrics', {})
          duration_values = metrics.get('http_req_duration', {}).get('values', {})

          # Extract latency percentiles
          p50 = duration_values.get('p(50)', duration_values.get('med', 0))
          p95 = duration_values.get('p(95)', 0)
          p99 = duration_values.get('p(99)', 0)
          avg = duration_values.get('avg', 0)

          # Calculate error rate and RPS
          reqs = metrics.get('http_reqs', {}).get('values', {}).get('count', 0)
          rate = metrics.get('http_reqs', {}).get('values', {}).get('rate', 0)
          failed_values = metrics.get('http_req_failed', {}).get('values', {})
          fail_rate = failed_values.get('rate')
          if fail_rate is None:
              fails = failed_values.get('fails')
              if fails is None:
                  passes = failed_values.get('passes')
                  if passes is None:
                      fails = 0
                  else:
                      fails = max(0, reqs - passes)
              fail_rate = (fails / reqs) if reqs > 0 else 0
          error_rate = fail_rate * 100

          # Print metrics summary
          print('=' * 60)
          print('SLO COMPLIANCE REPORT')
          print('=' * 60)
          print()

          violations = []

          # Check p50 (median)
          status = 'PASS' if p50 <= SLO['p50_ms'] else 'FAIL'
          print(f'{status}: p50 latency {p50:.1f}ms (SLO: <{SLO[\"p50_ms\"]}ms)')
          if p50 > SLO['p50_ms']:
              violations.append(f'p50 {p50:.1f}ms > {SLO[\"p50_ms\"]}ms')

          # Check p95
          status = 'PASS' if p95 <= SLO['p95_ms'] else 'FAIL'
          print(f'{status}: p95 latency {p95:.1f}ms (SLO: <{SLO[\"p95_ms\"]}ms)')
          if p95 > SLO['p95_ms']:
              violations.append(f'p95 {p95:.1f}ms > {SLO[\"p95_ms\"]}ms')

          # Check p99
          status = 'PASS' if p99 <= SLO['p99_ms'] else 'FAIL'
          print(f'{status}: p99 latency {p99:.1f}ms (SLO: <{SLO[\"p99_ms\"]}ms)')
          if p99 > SLO['p99_ms']:
              violations.append(f'p99 {p99:.1f}ms > {SLO[\"p99_ms\"]}ms')

          # Check error rate
          status = 'PASS' if error_rate <= SLO['error_rate_pct'] else 'FAIL'
          print(f'{status}: error rate {error_rate:.2f}% (SLO: <{SLO[\"error_rate_pct\"]}%)')
          if error_rate > SLO['error_rate_pct']:
              violations.append(f'error rate {error_rate:.2f}% > {SLO[\"error_rate_pct\"]}%')

          # Check throughput
          status = 'PASS' if rate >= SLO['min_rps'] else 'WARN'
          print(f'{status}: throughput {rate:.1f} RPS (SLO: >{SLO[\"min_rps\"]} RPS)')

          print()
          print('=' * 60)

          # Write metrics for artifact and GitHub output
          metrics_json = json.dumps({
              'p50_ms': round(p50, 1),
              'p95_ms': round(p95, 1),
              'p99_ms': round(p99, 1),
              'avg_ms': round(avg, 1),
              'error_rate_pct': round(error_rate, 2),
              'rps': round(rate, 1),
              'total_requests': reqs,
              'slo_violations': violations,
          })

          with open('slo-report.json', 'w') as f:
              f.write(metrics_json)

          # Export to GitHub output
          with open(os.environ.get('GITHUB_OUTPUT', '/dev/null'), 'a') as f:
              f.write(f'p50={p50:.1f}\n')
              f.write(f'p95={p95:.1f}\n')
              f.write(f'p99={p99:.1f}\n')
              f.write(f'error_rate={error_rate:.2f}\n')
              f.write(f'rps={rate:.1f}\n')
              f.write(f'slo_pass={\"true\" if not violations else \"false\"}\n')

          if violations:
              print(f'FAILED: {len(violations)} SLO violation(s)')
              for v in violations:
                  print(f'  - {v}')
              sys.exit(1)

          print('All SLO thresholds passed!')
          "

      - name: Add SLO summary to job
        if: always()
        run: |
          echo "## Load Test SLO Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f slo-report.json ]; then
            echo "| Metric | Value | SLO | Status |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|-----|--------|" >> $GITHUB_STEP_SUMMARY
            python -c "
          import json
          with open('slo-report.json') as f:
              data = json.load(f)
          slo = {'p50_ms': 200, 'p95_ms': 500, 'p99_ms': 2000, 'error_rate_pct': 1.0}
          print(f'| p50 latency | {data[\"p50_ms\"]}ms | <{slo[\"p50_ms\"]}ms | {\"PASS\" if data[\"p50_ms\"] <= slo[\"p50_ms\"] else \"FAIL\"} |')
          print(f'| p95 latency | {data[\"p95_ms\"]}ms | <{slo[\"p95_ms\"]}ms | {\"PASS\" if data[\"p95_ms\"] <= slo[\"p95_ms\"] else \"FAIL\"} |')
          print(f'| p99 latency | {data[\"p99_ms\"]}ms | <{slo[\"p99_ms\"]}ms | {\"PASS\" if data[\"p99_ms\"] <= slo[\"p99_ms\"] else \"FAIL\"} |')
          print(f'| Error rate | {data[\"error_rate_pct\"]}% | <{slo[\"error_rate_pct\"]}% | {\"PASS\" if data[\"error_rate_pct\"] <= slo[\"error_rate_pct\"] else \"FAIL\"} |')
          print(f'| Throughput | {data[\"rps\"]} RPS | >50 RPS | {\"PASS\" if data[\"rps\"] >= 50 else \"WARN\"} |')
            " >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load-results.json
            load-summary.json
            ws-results.json
            slo-report.json
            /tmp/aragora-server.log
          retention-days: 14

      - name: Stop server
        if: always()
        run: |
          if [ -n "$SERVER_PID" ]; then
            kill $SERVER_PID || true
          fi

  memory-stress:
    name: Memory Stress Test
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: load-test

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,redis]"
          pip install memory_profiler psutil

      - name: Start server with memory monitoring
        run: |
          python -c "
          import subprocess
          import time
          import psutil
          import sys

          # Start server
          proc = subprocess.Popen(
              ['aragora', 'serve', '--api-port', '8080', '--host', '127.0.0.1'],
              stdout=subprocess.PIPE,
              stderr=subprocess.PIPE
          )

          # Wait for startup
          time.sleep(5)

          # Monitor memory for 60 seconds under simulated load
          max_memory = 0
          for _ in range(60):
              try:
                  p = psutil.Process(proc.pid)
                  mem = p.memory_info().rss / (1024 * 1024)  # MB
                  max_memory = max(max_memory, mem)
                  print(f'Memory: {mem:.1f}MB')
              except psutil.NoSuchProcess:
                  break
              time.sleep(1)

          proc.terminate()

          # Check memory threshold (2GB)
          if max_memory > 2048:
              print(f'FAIL: Max memory {max_memory:.1f}MB > 2048MB')
              sys.exit(1)
          print(f'PASS: Max memory {max_memory:.1f}MB')
          "

      - name: Check for memory leaks
        run: |
          python -c "
          import subprocess
          import time
          import psutil

          # Start server
          proc = subprocess.Popen(
              ['aragora', 'serve', '--api-port', '8080', '--host', '127.0.0.1'],
              stdout=subprocess.PIPE,
              stderr=subprocess.PIPE
          )

          time.sleep(3)

          # Get initial memory
          p = psutil.Process(proc.pid)
          initial_mem = p.memory_info().rss / (1024 * 1024)

          # Make 1000 requests
          import urllib.request
          for _ in range(1000):
              try:
                  urllib.request.urlopen('http://localhost:8080/api/health', timeout=1)
              except:
                  pass

          # Check memory growth
          final_mem = p.memory_info().rss / (1024 * 1024)
          growth = final_mem - initial_mem

          proc.terminate()

          print(f'Initial memory: {initial_mem:.1f}MB')
          print(f'Final memory: {final_mem:.1f}MB')
          print(f'Growth: {growth:.1f}MB')

          # Allow up to 100MB growth for caches
          if growth > 100:
              print(f'WARNING: Memory grew by {growth:.1f}MB (possible leak)')
          else:
              print('PASS: No significant memory leak detected')
          "
