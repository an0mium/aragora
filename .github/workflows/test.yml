name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ["3.10", "3.11", "3.12"]
        exclude:
          # Reduce matrix size - only test 3.11 on Windows/macOS
          - os: windows-latest
            python-version: "3.10"
          - os: windows-latest
            python-version: "3.12"
          - os: macos-latest
            python-version: "3.10"
          - os: macos-latest
            python-version: "3.12"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,research]"
          pip install pytest-cov pytest-timeout

      - name: Run tests
        run: |
          pytest tests/ -v \
            --timeout=60 \
            --cov=aragora \
            --cov-report=xml \
            --cov-report=term-missing \
            -x --tb=short
        env:
          PYTHONPATH: .

      - name: Upload coverage to Codecov
        if: matrix.python-version == '3.11'
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage.xml
          fail_ci_if_error: false
          verbose: true
        continue-on-error: true

  smoke:
    name: CLI Smoke
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .

      - name: Smoke - demo ask
        run: |
          aragora ask "Smoke test a demo debate" --demo --rounds 1

      - name: Smoke - demo gauntlet
        run: |
          cat > /tmp/aragora_demo_spec.md <<'EOF'
          # Demo Spec
          Build a simple HTTP service that returns "ok" to GET /health.
          EOF
          if ! aragora gauntlet /tmp/aragora_demo_spec.md \
            --input-type spec \
            --agents demo,demo \
            --profile quick \
            --no-probing \
            --no-audit \
            --timeout 30 \
            --output /tmp/aragora_gauntlet_demo.json; then
            echo "Gauntlet exited non-zero (possible needs-review/reject verdict)."
          fi
          test -s /tmp/aragora_gauntlet_demo.json

      - name: Smoke - boot server
        run: |
          set -e
          aragora serve --api-port 8090 --ws-port 8766 --host 127.0.0.1 >/tmp/aragora-server.log 2>&1 &
          SERVER_PID=$!
          trap "kill $SERVER_PID" EXIT
          for i in {1..10}; do
            if curl -fsS http://127.0.0.1:8090/api/health >/dev/null; then
              echo "Server is healthy."
              break
            fi
            sleep 1
          done
          curl -fsS http://127.0.0.1:8090/api/health >/dev/null

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,research]"
          pip install pytest-benchmark

      - name: Download previous benchmark data
        uses: actions/cache@v4
        with:
          path: .benchmarks
          key: benchmark-${{ runner.os }}-${{ hashFiles('tests/benchmarks/**') }}
          restore-keys: |
            benchmark-${{ runner.os }}-

      - name: Run benchmarks
        run: |
          pytest tests/benchmarks/ -v \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-autosave \
            || true  # Don't fail on benchmark tests
        env:
          PYTHONPATH: .

      - name: Store benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            .benchmarks/
          retention-days: 30

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: benchmark-results.json
          comment-on-alert: true
          alert-threshold: '150%'
          fail-on-alert: false
          github-token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true
