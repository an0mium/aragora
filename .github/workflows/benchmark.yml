name: Benchmarks
# --------------------------------------------------------------------------
# Required Status Checks (add these in Settings > Branches > main > Require
# status checks to pass before merging):
#
#   - "Orchestration Speed Smoke"    -- guards fast-first routing latency
#   - "Performance Regression Check" -- catches >20% benchmark regressions
#
# These two jobs MUST pass on every PR that touches aragora/** or tests/**.
# --------------------------------------------------------------------------

on:
  push:
    branches: [main]
    paths:
      - 'aragora/**'
      - 'tests/benchmark/**'
      - 'tests/benchmarks/**'
      - 'tests/memory/test_benchmark.py'
      - 'tests/rlm/test_benchmark.py'
      - 'scripts/check_benchmark_regression.py'
      - 'pyproject.toml'
      - '.github/workflows/benchmark.yml'
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [main]
    paths:
      - 'aragora/**'
      - 'tests/benchmark/**'
      - 'tests/benchmarks/**'
      - 'tests/memory/test_benchmark.py'
      - 'tests/rlm/test_benchmark.py'
      - 'scripts/check_benchmark_regression.py'
      - 'pyproject.toml'
  schedule:
    # Run daily at 2 AM UTC to track performance trends
    - cron: '0 2 * * *'
  workflow_dispatch:

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    if: github.event_name != 'pull_request' || !github.event.pull_request.draft
    name: Performance Benchmarks
    runs-on: aragora
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for benchmark comparison

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            ${{ runner.os }}-pip-benchmark-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"
          pip install pytest-benchmark

      - name: Run benchmark tests
        run: |
          pytest tests/benchmarks/ tests/memory/test_benchmark.py tests/rlm/test_benchmark.py \
            -v \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=min,max,mean,stddev,median,rounds \
            --benchmark-sort=mean \
            --benchmark-warmup=on \
            --benchmark-disable-gc \
            -x
        env:
          PYTHONPATH: .

      - name: Store benchmark result
        if: github.ref == 'refs/heads/main'
        continue-on-error: true  # Don't fail the workflow if benchmark storage fails
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Aragora Performance Benchmarks
          tool: 'pytest'
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '115%'
          comment-on-alert: true
          fail-on-alert: true
          alert-comment-cc-users: '@an0mium'

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: benchmark-results.json
          retention-days: 30

  orchestration-speed-smoke:
    # REQUIRED STATUS CHECK: This job must pass on all PRs.
    # Validates that fast-first routing reduces critique latency by >=25%.
    name: Orchestration Speed Smoke
    runs-on: aragora
    timeout-minutes: 10
    if: (github.event_name != 'pull_request' || !github.event.pull_request.draft) && (github.ref == 'refs/heads/main' || github.event_name == 'pull_request')

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Run orchestration speed smoke benchmark
        run: |
          pytest -q tests/benchmarks/test_orchestration_speed_policy.py -x
        env:
          PYTHONPATH: .

  performance-regression:
    # REQUIRED STATUS CHECK: This job must pass on all PRs.
    # Fails if any benchmark regresses by >20% vs main branch.
    name: Performance Regression Check
    runs-on: aragora
    timeout-minutes: 20
    if: (github.event_name != 'pull_request' || !github.event.pull_request.draft) && (github.event_name == 'pull_request')

    steps:
      - name: Checkout PR
        uses: actions/checkout@v4

      - name: Checkout main for comparison
        uses: actions/checkout@v4
        with:
          ref: main
          path: main-branch

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"
          pip install pytest-benchmark

      - name: Run PR benchmarks
        run: |
          pytest tests/benchmarks/test_performance.py \
            --benchmark-only \
            --benchmark-json=pr-benchmark.json \
            --benchmark-warmup=on \
            -x
        env:
          PYTHONPATH: .

      - name: Run main benchmarks
        run: |
          cd main-branch
          pip install -e ".[test]"
          pytest tests/benchmarks/test_performance.py \
            --benchmark-only \
            --benchmark-json=../main-benchmark.json \
            --benchmark-warmup=on \
            -x
        env:
          PYTHONPATH: .

      - name: Check for regressions (>20% threshold)
        run: |
          python scripts/check_benchmark_regression.py compare \
            --current  pr-benchmark.json \
            --baseline main-benchmark.json \
            --threshold 20

      - name: Run fast-first routing smoke test
        run: |
          pytest -q tests/benchmarks/test_orchestration_speed_policy.py -x
        env:
          PYTHONPATH: .

      - name: Upload regression report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: regression-report
          path: |
            pr-benchmark.json
            main-benchmark.json
          retention-days: 14

  latency-tests:
    name: Latency Distribution
    runs-on: aragora
    timeout-minutes: 15
    if: (github.event_name != 'pull_request' || github.event.pull_request.draft == false) && (github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch')

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Run latency tests
        run: |
          pytest tests/benchmarks/test_api_rate_limits.py \
            tests/benchmarks/test_memory_tier_throughput.py \
            -v \
            --tb=short \
            -x
        env:
          PYTHONPATH: .

      - name: Summary
        if: always()
        run: |
          echo "## Latency Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Latency distribution tests completed." >> $GITHUB_STEP_SUMMARY

  baseline-comparison:
    name: Baseline Comparison
    runs-on: aragora
    timeout-minutes: 15
    if: (github.event_name != 'pull_request' || !github.event.pull_request.draft) && (github.ref == 'refs/heads/main' || github.event_name == 'pull_request')

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[test]"

      - name: Run baseline comparison
        id: baseline
        run: |
          python -m benchmarks.baseline_compare --run --format json > baseline_results.json 2>&1 || true
          python -m benchmarks.baseline_compare --run
        env:
          PYTHONPATH: .

      - name: Post baseline results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let results = '';
            try {
              const output = fs.readFileSync('baseline_results.json', 'utf8');
              const data = JSON.parse(output);
              const failures = data.filter(m => m.status === 'fail');
              const warnings = data.filter(m => m.status === 'warning');

              if (failures.length > 0 || warnings.length > 0) {
                results = '### Baseline Comparison Results\n\n';
                if (failures.length > 0) {
                  results += '**Failures:**\n';
                  failures.forEach(f => {
                    results += `- ${f.metric}: ${f.message}\n`;
                  });
                }
                if (warnings.length > 0) {
                  results += '\n**Warnings:**\n';
                  warnings.forEach(w => {
                    results += `- ${w.metric}: ${w.message}\n`;
                  });
                }
              } else {
                results = '### Baseline Comparison: All metrics within tolerance';
              }
            } catch (e) {
              results = '### Baseline Comparison: Unable to parse results';
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: results
            });

      - name: Upload baseline results
        uses: actions/upload-artifact@v4
        with:
          name: baseline-comparison
          path: baseline_results.json
          retention-days: 30
