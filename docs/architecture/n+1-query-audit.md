# N+1 Query Audit Report

**Date:** 2026-01-29
**Scope:** Email connectors, export handlers, knowledge management adapters
**Status:** Active remediation required

## Executive Summary

This audit identified **6 N+1 query patterns** across the Aragora codebase, primarily concentrated in email processing and export functionality. These patterns cause performance degradation that scales linearly (or worse) with data volume, impacting response times and API quota consumption.

**Severity Distribution:**
- HIGH: 2 patterns (immediate action required)
- MEDIUM: 3 patterns (address within current sprint)
- LOW: 1 pattern (backlog candidate)

**Estimated Impact:** Fixing these patterns will reduce Gmail API calls by 60-80% in typical workflows and improve export throughput by 3-5x for large datasets.

---

## Identified Patterns

### 1. Gmail Message Fetch (HIGH)

**Location:** `aragora/connectors/gmail_query.py:181-204`

**Description:** Individual API calls made for each message ID when fetching message details, rather than using Gmail's batch API.

**Current Pattern:**
```python
for msg_id in message_ids:
    message = service.users().messages().get(userId='me', id=msg_id).execute()
    results.append(message)
```

**Impact:**
- 100 messages = 100 API calls (vs 2 batch calls)
- Rate limit exhaustion under moderate load
- Latency compounds: ~200ms/call × N messages

**Remediation:**
```python
from googleapiclient.http import BatchHttpRequest

def fetch_messages_batch(service, message_ids, batch_size=100):
    results = []

    def callback(request_id, response, exception):
        if exception is None:
            results.append(response)

    for chunk in chunked(message_ids, batch_size):
        batch = service.new_batch_http_request(callback=callback)
        for msg_id in chunk:
            batch.add(service.users().messages().get(userId='me', id=msg_id))
        batch.execute()

    return results
```

---

### 2. Email Scoring Loop (HIGH)

**Location:** `aragora/connectors/gmail_query.py:433-446`

**Description:** Each email is scored individually with separate database lookups for sender history, thread context, and priority signals.

**Current Pattern:**
```python
for email in emails:
    sender_score = await get_sender_score(email.sender)  # DB query
    thread_score = await get_thread_priority(email.thread_id)  # DB query
    label_score = await get_label_weights(email.labels)  # DB query
    email.score = compute_score(sender_score, thread_score, label_score)
```

**Impact:**
- 50 emails × 3 queries = 150 database roundtrips
- Database connection pool exhaustion
- P95 latency spikes during inbox sync

**Remediation:**
```python
async def score_emails_batch(emails: list[Email]) -> list[Email]:
    # Collect all unique keys
    senders = {e.sender for e in emails}
    thread_ids = {e.thread_id for e in emails}
    all_labels = set().union(*(e.labels for e in emails))

    # Batch fetch all scores
    sender_scores = await get_sender_scores_batch(list(senders))
    thread_scores = await get_thread_priorities_batch(list(thread_ids))
    label_weights = await get_label_weights_batch(list(all_labels))

    # Compute scores using pre-fetched data
    for email in emails:
        email.score = compute_score(
            sender_scores[email.sender],
            thread_scores[email.thread_id],
            label_weights,
            email.labels
        )

    return emails
```

---

### 3. Attachment Metadata Fetch (MEDIUM)

**Location:** `aragora/connectors/gmail_threads.py:344-351`

**Description:** Attachment metadata fetched individually for each attachment in a thread, even when multiple attachments exist in the same message.

**Current Pattern:**
```python
for message in thread.messages:
    for attachment in message.attachments:
        metadata = service.users().messages().attachments().get(
            userId='me', messageId=message.id, id=attachment.id
        ).execute()
```

**Impact:**
- Threads with 10 messages × 2 attachments = 20 API calls
- Unnecessary for metadata-only operations (size, mime type already in message)

**Remediation:**
- Extract metadata from message payload (already present)
- Only fetch attachment data when content is actually needed
- Batch fetch when downloading multiple attachments

---

### 4. Email Prioritizer Handler (MEDIUM)

**Location:** `aragora/server/handlers/email.py:291`

**Description:** The `/api/email/prioritize` endpoint processes emails sequentially, making individual calls to the scoring service.

**Current Pattern:**
```python
@app.post("/api/email/prioritize")
async def prioritize_emails(request: PrioritizeRequest):
    results = []
    for email_id in request.email_ids:
        email = await fetch_email(email_id)  # N+1 here
        score = await score_email(email)      # N+1 here
        results.append({"id": email_id, "score": score})
    return results
```

**Impact:**
- API response time scales linearly with batch size
- Timeout risk for large prioritization requests
- Inefficient use of async capabilities

**Remediation:**
```python
@app.post("/api/email/prioritize")
async def prioritize_emails(request: PrioritizeRequest):
    # Batch fetch
    emails = await fetch_emails_batch(request.email_ids)

    # Batch score
    scored_emails = await score_emails_batch(emails)

    return [{"id": e.id, "score": e.score} for e in scored_emails]
```

---

### 5. Batch Export Handler (MEDIUM)

**Location:** `aragora/server/handlers/export.py:196+`

**Description:** Export operations fetch related entities (debates, decisions, evidence) individually when building export packages.

**Current Pattern:**
```python
for debate_id in export_request.debate_ids:
    debate = await get_debate(debate_id)
    decisions = await get_decisions_for_debate(debate_id)  # N+1
    evidence = await get_evidence_for_debate(debate_id)    # N+1
    export_data.append(build_export_entry(debate, decisions, evidence))
```

**Impact:**
- Large exports (100+ debates) cause severe slowdown
- Memory pressure from uncoordinated fetches
- Export timeouts reported in production logs

**Remediation:**
```python
async def build_export_batch(debate_ids: list[str]) -> list[ExportEntry]:
    # Single query with JOINs or batch fetch
    debates = await get_debates_batch(debate_ids)
    decisions = await get_decisions_for_debates(debate_ids)  # GROUP BY debate_id
    evidence = await get_evidence_for_debates(debate_ids)    # GROUP BY debate_id

    # Index by debate_id for O(1) lookup
    decisions_by_debate = group_by(decisions, 'debate_id')
    evidence_by_debate = group_by(evidence, 'debate_id')

    return [
        build_export_entry(
            debate,
            decisions_by_debate.get(debate.id, []),
            evidence_by_debate.get(debate.id, [])
        )
        for debate in debates
    ]
```

---

### 6. KM Adapter Sync (LOW)

**Location:** `aragora/knowledge/mound/adapters/consensus_adapter.py`

**Description:** Consensus adapter syncs individual debate outcomes to Knowledge Mound sequentially during periodic sync operations.

**Current Pattern:**
```python
async def sync_recent_outcomes(self, since: datetime):
    outcomes = await self.consensus_store.get_outcomes_since(since)
    for outcome in outcomes:
        await self.mound.ingest(self._to_knowledge_item(outcome))
```

**Impact:**
- Low frequency operation (hourly sync)
- Volume typically small (10-50 items)
- Acceptable latency for background task

**Remediation (backlog):**
- Implement `mound.ingest_batch()` for bulk operations
- Add to Knowledge Mound Phase B roadmap

---

## Prioritized Remediation Roadmap

### Phase 1: Critical Path (Week 1-2)

| Pattern | Location | Effort | Impact |
|---------|----------|--------|--------|
| Gmail Message Fetch | gmail_query.py:181-204 | 2 days | HIGH |
| Email Scoring Loop | gmail_query.py:433-446 | 3 days | HIGH |

**Deliverables:**
- Batch Gmail API wrapper
- Bulk score computation service
- Updated integration tests

### Phase 2: Handler Optimization (Week 3)

| Pattern | Location | Effort | Impact |
|---------|----------|--------|--------|
| Email Prioritizer | email.py:291 | 1 day | MEDIUM |
| Batch Export | export.py:196+ | 2 days | MEDIUM |
| Attachment Metadata | gmail_threads.py:344-351 | 1 day | MEDIUM |

**Deliverables:**
- Refactored endpoint handlers
- Batch query utilities
- Performance benchmarks

### Phase 3: Background Optimization (Backlog)

| Pattern | Location | Effort | Impact |
|---------|----------|--------|--------|
| KM Adapter Sync | consensus_adapter.py | 1 day | LOW |

**Deliverables:**
- Batch ingest for Knowledge Mound
- Align with Phase B KM roadmap

---

## Testing Recommendations

### Unit Tests

```python
# tests/connectors/test_gmail_query_batch.py

@pytest.mark.asyncio
async def test_fetch_messages_batch_reduces_api_calls():
    """Verify batch fetch uses minimal API calls."""
    mock_service = create_mock_gmail_service()
    message_ids = [f"msg_{i}" for i in range(250)]

    results = await fetch_messages_batch(mock_service, message_ids)

    # 250 messages should require 3 batch calls (100 per batch)
    assert mock_service.new_batch_http_request.call_count == 3
    assert len(results) == 250


@pytest.mark.asyncio
async def test_score_emails_batch_single_db_roundtrip():
    """Verify batch scoring minimizes database queries."""
    emails = [create_test_email() for _ in range(50)]

    with patch('aragora.connectors.gmail_query.get_sender_scores_batch') as mock:
        await score_emails_batch(emails)

        # Should be called once, not 50 times
        assert mock.call_count == 1
```

### Integration Tests

```python
# tests/integration/test_email_prioritization_perf.py

@pytest.mark.integration
@pytest.mark.asyncio
async def test_prioritize_100_emails_under_2_seconds():
    """Ensure batch prioritization meets SLO."""
    client = TestClient(app)
    email_ids = await seed_test_emails(100)

    start = time.monotonic()
    response = client.post("/api/email/prioritize", json={"email_ids": email_ids})
    elapsed = time.monotonic() - start

    assert response.status_code == 200
    assert elapsed < 2.0, f"Prioritization took {elapsed:.2f}s, expected <2s"
```

### Load Tests

```python
# tests/load/test_gmail_connector_load.py

@pytest.mark.load
def test_gmail_sync_under_rate_limit():
    """Verify Gmail sync stays under API quota."""
    with RateLimitTracker() as tracker:
        sync_inbox(message_count=1000)

        # Gmail allows 250 quota units per second
        assert tracker.max_qps < 250
        assert tracker.total_calls < 50  # Should use batching
```

### Monitoring Recommendations

Add these metrics to track remediation effectiveness:

```python
# aragora/connectors/metrics.py

gmail_api_calls = Counter(
    'aragora_gmail_api_calls_total',
    'Total Gmail API calls',
    ['operation', 'batch_size']
)

email_scoring_duration = Histogram(
    'aragora_email_scoring_seconds',
    'Email scoring batch duration',
    ['batch_size_bucket']
)

export_query_count = Counter(
    'aragora_export_queries_total',
    'Database queries during export',
    ['export_type']
)
```

---

## Estimated Impact

### Before Remediation

| Metric | Current | With 100 Emails |
|--------|---------|-----------------|
| Gmail API calls | 1 per message | 100 calls |
| DB queries (scoring) | 3 per email | 300 queries |
| Export latency | O(n) | ~10 seconds |
| P95 response time | Variable | 5-15 seconds |

### After Remediation

| Metric | Optimized | With 100 Emails |
|--------|-----------|-----------------|
| Gmail API calls | 1 per 100 messages | 1 call |
| DB queries (scoring) | 3 total | 3 queries |
| Export latency | O(1) with batch | ~0.5 seconds |
| P95 response time | Consistent | <1 second |

### Resource Savings

- **Gmail API Quota:** 60-80% reduction in quota consumption
- **Database Load:** 95%+ reduction in query count for batch operations
- **Response Time:** 3-10x improvement for bulk operations
- **Connection Pool:** Reduced contention, fewer timeout errors

---

## References

- [Gmail API Batch Requests](https://developers.google.com/gmail/api/guides/batch)
- [SQLAlchemy Eager Loading](https://docs.sqlalchemy.org/en/14/orm/loading_relationships.html)
- `docs/TECHNICAL_DEBT.md` - Related debt items
- `docs/architecture/primitive-consolidation.md` - Query consolidation patterns
